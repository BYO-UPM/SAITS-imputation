nohup: ignoring input
[34m
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó
‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë
   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

2025-05-02 13:54:10 [WARNING]: ‚ÄºÔ∏è `pypots.utils.metrics` is deprecated. Please import from `pypots.nn.functional` instead.
TRAIN
{'0': 'HF008', '1': 'HF008_V2', '2': 'HF009', '3': 'HF009_V2', '4': 'HF010', '5': 'HF010_V2', '6': 'HF011', '7': 'HF011_V2', '8': 'HF012', '9': 'HF012_V2', '10': 'HF013', '11': 'HF014', '12': 'HF015', '13': 'HF015_V2', '14': 'HF016', '15': 'HF016_V2', '16': 'HF017', '17': 'HF018', '18': 'HF019', '19': 'HF020', '20': 'HF021', '21': 'HF022', '22': 'HF023', '23': 'HF024', '24': 'HF026', '25': 'HF027', '26': 'HF028', '27': 'HF029', '28': 'HF030_V2', '29': 'HF031', '30': 'HF031_V2', '31': 'HF032', '32': 'HF033_V2', '33': 'HG001_V2', '34': 'HG002', '35': 'HG002_V2', '36': 'HG003', '37': 'HG004', '38': 'HG005', '39': 'HG005_V2', '40': 'HG005_V3', '41': 'HG006_V2', '42': 'HG008', '43': 'HG008_V2', '44': 'HG009', '45': 'HG009_V2', '46': 'HG010', '47': 'HG010_V2', '48': 'HG011', '49': 'HG011_V2', '50': 'HG012_V2', '51': 'HG014_V2', '52': 'HG015_V2', '53': 'HG016', '54': 'HG016_V2', '55': 'HG017', '56': 'HG017_V2', '57': 'HG018', '58': 'HG018_V2', '59': 'HG019_V2', '60': 'HG021_V2', '61': 'HG022', '62': 'HG022_V2', '63': 'HG023', '64': 'HG023_V2', '65': 'HG024', '66': 'HG024_V2', '67': 'HG025_V2', '68': 'HG027', '69': 'HG027_V2', '70': 'HG029', '71': 'HG029_V2', '72': 'HG030_V2', '73': 'HG031', '74': 'HG031_V2', '75': 'HG032', '76': 'HG033', '77': 'HG034', '78': 'HG034_V2', '79': 'HG035', '80': 'HG035_V2', '81': 'HG036_V2', '82': 'HG037', '83': 'HG037_V2', '84': 'HG038_V2', '85': 'HG039', '86': 'HG039_V2', '87': 'HG041', '88': 'HG042', '89': 'HG043', '90': 'HG044', '91': 'HG044_V2', '92': 'HG045', '93': 'HG047', '94': 'HG047_V2', '95': 'HG049', '96': 'HG049_V2', '97': 'HG050', '98': 'HG050_V2', '99': 'HG051', '100': 'HG051_V2', '101': 'HG052', '102': 'HG052_V2', '103': 'HG053_V2', '104': 'HG054', '105': 'HG054_V2', '106': 'HG055', '107': 'HG055_V2', '108': 'HG056', '109': 'HG057', '110': 'HG059', '111': 'HG060', '112': 'HG061_V2', '113': 'HG062', '114': 'HG063', '115': 'HG064', '116': 'HG064_V2', '117': 'HG065', '118': 'HG065_V2', '119': 'HG066', '120': 'HG066_V2', '121': 'HG067', '122': 'HG067_V2', '123': 'HG071', '124': 'HG072', '125': 'HG072_V2', '126': 'HG073', '127': 'HG073_V2', '128': 'HG074', '129': 'HGJ002', '130': 'HGJ003', '131': 'HGJ004', '132': 'HGJ006', '133': 'HGJ008', '134': 'HGJ009', '135': 'HGJ011', '136': 'HGJ012'}
length of instances_dict :  137
----------------------------------------------------------------------------------------------------
length of smoothpur_1_4 :  137
length of smoothpur_5_8 :  137
length of smoothpur_9_10 :  137
length of smoothpur_11_12 :  137
----------------------------------------------------------------------------------------------------
type of smoothpur_1_4 :  <class 'numpy.ndarray'>
type of smoothpur_5_8 :  <class 'numpy.ndarray'>
type of smoothpur_9_10 :  <class 'numpy.ndarray'>
type of smoothpur_11_12 :  <class 'numpy.ndarray'>
----------------------------------------------------------------------------------------------------
shape of smoothpur_1_4 :  (137, 4, 3, 15000)
shape of smoothpur_5_8 :  (137, 4, 3, 15000)
shape of smoothpur_9_10 :  (137, 2, 3, 2, 15000)
shape of smoothpur_11_12 :  (137, 2, 3, 2, 15000)
TEST
{'0': 'HF026_V2', '1': 'HF027_V2', '2': 'HF032_V2', '3': 'HF033', '4': 'HG001', '5': 'HG003_V2', '6': 'HG004_V2', '7': 'HG015', '8': 'HG019', '9': 'HG021', '10': 'HG025', '11': 'HG026', '12': 'HG028', '13': 'HG028_V2', '14': 'HG030', '15': 'HG032_V2', '16': 'HG038', '17': 'HG041_V2', '18': 'HG043_V2', '19': 'HG048', '20': 'HG048_V2', '21': 'HG053', '22': 'HG058', '23': 'HG058_V2', '24': 'HG060_V2', '25': 'HG061', '26': 'HG063_V2', '27': 'HG069', '28': 'HG069_V2', '29': 'HG070', '30': 'HG074_V2', '31': 'HGJ001', '32': 'HGJ005', '33': 'HGJ007', '34': 'HGJ010'}
length of instances_dict :  35
----------------------------------------------------------------------------------------------------
length of smoothpur_1_4 :  35
length of smoothpur_5_8 :  35
length of smoothpur_9_10 :  35
length of smoothpur_11_12 :  35
----------------------------------------------------------------------------------------------------
type of smoothpur_1_4 :  <class 'numpy.ndarray'>
type of smoothpur_5_8 :  <class 'numpy.ndarray'>
type of smoothpur_9_10 :  <class 'numpy.ndarray'>
type of smoothpur_11_12 :  <class 'numpy.ndarray'>
----------------------------------------------------------------------------------------------------
shape of smoothpur_1_4 :  (35, 4, 3, 15000)
shape of smoothpur_5_8 :  (35, 4, 3, 15000)
shape of smoothpur_9_10 :  (35, 2, 3, 2, 15000)
shape of smoothpur_11_12 :  (35, 2, 3, 2, 15000)
TRAIN
shape of flattened smoothpur_1_4 : (1096, 15000)
shape of flattened smoothpur_5_8 : (1096, 15000)
shape of flattened smoothpur_x_9_10 : (548, 15000)
shape of flattened smoothpur_y_9_10 : (548, 15000)
shape of flattened smoothpur_x_11_12 : (548, 15000)
shape of flattened smoothpur_y_11_12 : (548, 15000)
TEST
shape of flattened smoothpur_1_4 : (280, 15000)
shape of flattened smoothpur_5_8 : (280, 15000)
shape of flattened smoothpur_x_9_10 : (140, 15000)
shape of flattened smoothpur_y_9_10 : (140, 15000)
shape of flattened smoothpur_x_11_12 : (140, 15000)
shape of flattened smoothpur_y_11_12 : (140, 15000)
TRAIN
shape of flattened smoothpur_1_4 : (1096, 15000)
shape of flattened smoothpur_5_8 : (1096, 15000)
shape of flattened smoothpur_x_9_10 : (548, 15000)
shape of flattened smoothpur_y_9_10 : (548, 15000)
shape of flattened smoothpur_x_11_12 : (548, 15000)
shape of flattened smoothpur_y_11_12 : (548, 15000)
TEST
shape of flattened smoothpur_1_4 : (280, 15000)
shape of flattened smoothpur_5_8 : (280, 15000)
shape of flattened smoothpur_x_9_10 : (140, 15000)
shape of flattened smoothpur_y_9_10 : (140, 15000)
shape of flattened smoothpur_x_11_12 : (140, 15000)
shape of flattened smoothpur_y_11_12 : (140, 15000)
TRAIN
shape of downsampled smoothpur_1_4 : (1096, 500)
shape of downsampled smoothpur_5_8 : (1096, 500)
shape of downsampled smoothpur_x_9_10 : (548, 500)
shape of downsampled smoothpur_y_9_10 : (548, 500)
shape of downsampled smoothpur_x_11_12 : (548, 500)
shape of downsampled smoothpur_y_11_12 : (548, 500)
shape of downsampled All smoothpurs : (4384, 500)
TEST
shape of downsampled smoothpur_1_4 : (280, 500)
shape of downsampled smoothpur_5_8 : (280, 500)
shape of downsampled smoothpur_x_9_10 : (140, 500)
shape of downsampled smoothpur_y_9_10 : (140, 500)
shape of downsampled smoothpur_x_11_12 : (140, 500)
shape of downsampled smoothpur_y_11_12 : (140, 500)
shape of downsampled All smoothpurs : (1120, 500)
TRAIN
TRAIN
shape of expanded smoothpur_1_4 : (1096, 500, 1)
shape of expanded smoothpur_5_8 : (1096, 500, 1)
shape of expanded smoothpur_x_9_10 : (548, 500, 1)
shape of expanded smoothpur_y_9_10 : (548, 500, 1)
shape of expanded smoothpur_x_11_12 : (548, 500, 1)
shape of expanded smoothpur_y_11_12 : (548, 500, 1)
shape of expanded All smoothpur : (4384, 500, 1)
TEST
shape of expanded smoothpur_1_4 : (280, 500, 1)
shape of expanded smoothpur_5_8 : (280, 500, 1)
shape of expanded smoothpur_x_9_10 : (140, 500, 1)
shape of expanded smoothpur_y_9_10 : (140, 500, 1)
shape of expanded smoothpur_x_11_12 : (140, 500, 1)
shape of expanded smoothpur_y_11_12 : (140, 500, 1)
shape of expanded All smoothpur : (1120, 500, 1)
2025-05-02 13:54:16 [INFO]: No given device, using default device: cuda
2025-05-02 13:54:16 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-02 13:54:16 [INFO]: Using customized MAE as the training loss function.
2025-05-02 13:54:16 [INFO]: Using customized MSE as the validation metric function.
2025-05-02 13:54:16 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-02 13:54:23 [INFO]: Epoch 001 - training loss (MAE): 0.8634
2025-05-02 13:54:29 [INFO]: Epoch 002 - training loss (MAE): 0.3739
2025-05-02 13:54:35 [INFO]: Epoch 003 - training loss (MAE): 0.3170
2025-05-02 13:54:42 [INFO]: Epoch 004 - training loss (MAE): 0.2840
2025-05-02 13:54:48 [INFO]: Epoch 005 - training loss (MAE): 0.2557
2025-05-02 13:54:55 [INFO]: Epoch 006 - training loss (MAE): 0.2282
2025-05-02 13:55:01 [INFO]: Epoch 007 - training loss (MAE): 0.2063
2025-05-02 13:55:08 [INFO]: Epoch 008 - training loss (MAE): 0.1909
2025-05-02 13:55:14 [INFO]: Epoch 009 - training loss (MAE): 0.1747
2025-05-02 13:55:20 [INFO]: Epoch 010 - training loss (MAE): 0.1633
2025-05-02 13:55:27 [INFO]: Epoch 011 - training loss (MAE): 0.1529
2025-05-02 13:55:33 [INFO]: Epoch 012 - training loss (MAE): 0.1450
2025-05-02 13:55:39 [INFO]: Epoch 013 - training loss (MAE): 0.1400
2025-05-02 13:55:46 [INFO]: Epoch 014 - training loss (MAE): 0.1340
2025-05-02 13:55:52 [INFO]: Epoch 015 - training loss (MAE): 0.1300
2025-05-02 13:55:59 [INFO]: Epoch 016 - training loss (MAE): 0.1256
2025-05-02 13:56:05 [INFO]: Epoch 017 - training loss (MAE): 0.1214
2025-05-02 13:56:11 [INFO]: Epoch 018 - training loss (MAE): 0.1193
2025-05-02 13:56:18 [INFO]: Epoch 019 - training loss (MAE): 0.1162
2025-05-02 13:56:25 [INFO]: Epoch 020 - training loss (MAE): 0.1136
2025-05-02 13:56:31 [INFO]: Epoch 021 - training loss (MAE): 0.1120
2025-05-02 13:56:38 [INFO]: Epoch 022 - training loss (MAE): 0.1087
2025-05-02 13:56:44 [INFO]: Epoch 023 - training loss (MAE): 0.1066
2025-05-02 13:56:51 [INFO]: Epoch 024 - training loss (MAE): 0.1043
2025-05-02 13:56:57 [INFO]: Epoch 025 - training loss (MAE): 0.1026
2025-05-02 13:57:06 [INFO]: Epoch 026 - training loss (MAE): 0.1009
2025-05-02 13:57:23 [INFO]: Epoch 027 - training loss (MAE): 0.0988
2025-05-02 13:57:39 [INFO]: Epoch 028 - training loss (MAE): 0.0973
2025-05-02 13:57:55 [INFO]: Epoch 029 - training loss (MAE): 0.0952
2025-05-02 13:58:11 [INFO]: Epoch 030 - training loss (MAE): 0.0939
2025-05-02 13:58:27 [INFO]: Epoch 031 - training loss (MAE): 0.0936
2025-05-02 13:58:44 [INFO]: Epoch 032 - training loss (MAE): 0.0908
2025-05-02 13:59:00 [INFO]: Epoch 033 - training loss (MAE): 0.0910
2025-05-02 13:59:16 [INFO]: Epoch 034 - training loss (MAE): 0.0902
2025-05-02 13:59:32 [INFO]: Epoch 035 - training loss (MAE): 0.0881
2025-05-02 13:59:48 [INFO]: Epoch 036 - training loss (MAE): 0.0874
2025-05-02 14:00:04 [INFO]: Epoch 037 - training loss (MAE): 0.0862
2025-05-02 14:00:20 [INFO]: Epoch 038 - training loss (MAE): 0.0851
2025-05-02 14:00:36 [INFO]: Epoch 039 - training loss (MAE): 0.0847
2025-05-02 14:00:53 [INFO]: Epoch 040 - training loss (MAE): 0.0844
2025-05-02 14:01:09 [INFO]: Epoch 041 - training loss (MAE): 0.0830
2025-05-02 14:01:25 [INFO]: Epoch 042 - training loss (MAE): 0.0827
2025-05-02 14:01:41 [INFO]: Epoch 043 - training loss (MAE): 0.0824
2025-05-02 14:01:57 [INFO]: Epoch 044 - training loss (MAE): 0.0814
2025-05-02 14:02:13 [INFO]: Epoch 045 - training loss (MAE): 0.0809
2025-05-02 14:02:30 [INFO]: Epoch 046 - training loss (MAE): 0.0800
2025-05-02 14:02:46 [INFO]: Epoch 047 - training loss (MAE): 0.0812
2025-05-02 14:03:02 [INFO]: Epoch 048 - training loss (MAE): 0.0795
2025-05-02 14:03:18 [INFO]: Epoch 049 - training loss (MAE): 0.0792
2025-05-02 14:03:34 [INFO]: Epoch 050 - training loss (MAE): 0.0782
2025-05-02 14:03:51 [INFO]: Epoch 051 - training loss (MAE): 0.0787
2025-05-02 14:04:07 [INFO]: Epoch 052 - training loss (MAE): 0.0776
2025-05-02 14:04:23 [INFO]: Epoch 053 - training loss (MAE): 0.0775
2025-05-02 14:04:39 [INFO]: Epoch 054 - training loss (MAE): 0.0770
2025-05-02 14:04:55 [INFO]: Epoch 055 - training loss (MAE): 0.0774
2025-05-02 14:05:12 [INFO]: Epoch 056 - training loss (MAE): 0.0764
2025-05-02 14:05:28 [INFO]: Epoch 057 - training loss (MAE): 0.0753
2025-05-02 14:05:44 [INFO]: Epoch 058 - training loss (MAE): 0.0752
2025-05-02 14:06:00 [INFO]: Epoch 059 - training loss (MAE): 0.0752
2025-05-02 14:06:16 [INFO]: Epoch 060 - training loss (MAE): 0.0747
2025-05-02 14:06:33 [INFO]: Epoch 061 - training loss (MAE): 0.0745
2025-05-02 14:06:49 [INFO]: Epoch 062 - training loss (MAE): 0.0745
2025-05-02 14:07:05 [INFO]: Epoch 063 - training loss (MAE): 0.0743
2025-05-02 14:07:21 [INFO]: Epoch 064 - training loss (MAE): 0.0731
2025-05-02 14:07:38 [INFO]: Epoch 065 - training loss (MAE): 0.0736
2025-05-02 14:07:54 [INFO]: Epoch 066 - training loss (MAE): 0.0734
2025-05-02 14:08:10 [INFO]: Epoch 067 - training loss (MAE): 0.0726
2025-05-02 14:08:26 [INFO]: Epoch 068 - training loss (MAE): 0.0730
2025-05-02 14:08:42 [INFO]: Epoch 069 - training loss (MAE): 0.0719
2025-05-02 14:08:58 [INFO]: Epoch 070 - training loss (MAE): 0.0721
2025-05-02 14:09:15 [INFO]: Epoch 071 - training loss (MAE): 0.0721
2025-05-02 14:09:31 [INFO]: Epoch 072 - training loss (MAE): 0.0714
2025-05-02 14:09:47 [INFO]: Epoch 073 - training loss (MAE): 0.0715
2025-05-02 14:10:03 [INFO]: Epoch 074 - training loss (MAE): 0.0727
2025-05-02 14:10:19 [INFO]: Epoch 075 - training loss (MAE): 0.0712
2025-05-02 14:10:35 [INFO]: Epoch 076 - training loss (MAE): 0.0709
2025-05-02 14:10:52 [INFO]: Epoch 077 - training loss (MAE): 0.0710
2025-05-02 14:11:08 [INFO]: Epoch 078 - training loss (MAE): 0.0708
2025-05-02 14:11:24 [INFO]: Epoch 079 - training loss (MAE): 0.0699
2025-05-02 14:11:40 [INFO]: Epoch 080 - training loss (MAE): 0.0706
2025-05-02 14:11:57 [INFO]: Epoch 081 - training loss (MAE): 0.0701
2025-05-02 14:12:13 [INFO]: Epoch 082 - training loss (MAE): 0.0700
2025-05-02 14:12:29 [INFO]: Epoch 083 - training loss (MAE): 0.0699
2025-05-02 14:12:45 [INFO]: Epoch 084 - training loss (MAE): 0.0698
2025-05-02 14:13:01 [INFO]: Epoch 085 - training loss (MAE): 0.0694
2025-05-02 14:13:17 [INFO]: Epoch 086 - training loss (MAE): 0.0694
2025-05-02 14:13:34 [INFO]: Epoch 087 - training loss (MAE): 0.0689
2025-05-02 14:13:50 [INFO]: Epoch 088 - training loss (MAE): 0.0691
2025-05-02 14:14:06 [INFO]: Epoch 089 - training loss (MAE): 0.0683
2025-05-02 14:14:22 [INFO]: Epoch 090 - training loss (MAE): 0.0683
2025-05-02 14:14:38 [INFO]: Epoch 091 - training loss (MAE): 0.0683
2025-05-02 14:14:54 [INFO]: Epoch 092 - training loss (MAE): 0.0680
2025-05-02 14:15:11 [INFO]: Epoch 093 - training loss (MAE): 0.0687
2025-05-02 14:15:27 [INFO]: Epoch 094 - training loss (MAE): 0.0679
2025-05-02 14:15:43 [INFO]: Epoch 095 - training loss (MAE): 0.0675
2025-05-02 14:15:59 [INFO]: Epoch 096 - training loss (MAE): 0.0677
2025-05-02 14:16:15 [INFO]: Epoch 097 - training loss (MAE): 0.0666
2025-05-02 14:16:32 [INFO]: Epoch 098 - training loss (MAE): 0.0678
2025-05-02 14:16:48 [INFO]: Epoch 099 - training loss (MAE): 0.0669
2025-05-02 14:17:04 [INFO]: Epoch 100 - training loss (MAE): 0.0666
2025-05-02 14:17:20 [INFO]: Epoch 101 - training loss (MAE): 0.0677
2025-05-02 14:17:36 [INFO]: Epoch 102 - training loss (MAE): 0.0670
2025-05-02 14:17:53 [INFO]: Epoch 103 - training loss (MAE): 0.0662
2025-05-02 14:18:09 [INFO]: Epoch 104 - training loss (MAE): 0.0665
2025-05-02 14:18:25 [INFO]: Epoch 105 - training loss (MAE): 0.0671
2025-05-02 14:18:41 [INFO]: Epoch 106 - training loss (MAE): 0.0662
2025-05-02 14:18:57 [INFO]: Epoch 107 - training loss (MAE): 0.0661
2025-05-02 14:19:14 [INFO]: Epoch 108 - training loss (MAE): 0.0671
2025-05-02 14:19:30 [INFO]: Epoch 109 - training loss (MAE): 0.0657
2025-05-02 14:19:46 [INFO]: Epoch 110 - training loss (MAE): 0.0664
2025-05-02 14:20:02 [INFO]: Epoch 111 - training loss (MAE): 0.0659
2025-05-02 14:20:18 [INFO]: Epoch 112 - training loss (MAE): 0.0667
2025-05-02 14:20:35 [INFO]: Epoch 113 - training loss (MAE): 0.0653
2025-05-02 14:20:51 [INFO]: Epoch 114 - training loss (MAE): 0.0658
2025-05-02 14:21:07 [INFO]: Epoch 115 - training loss (MAE): 0.0650
2025-05-02 14:21:23 [INFO]: Epoch 116 - training loss (MAE): 0.0656
2025-05-02 14:21:39 [INFO]: Epoch 117 - training loss (MAE): 0.0655
2025-05-02 14:21:55 [INFO]: Epoch 118 - training loss (MAE): 0.0644
2025-05-02 14:22:12 [INFO]: Epoch 119 - training loss (MAE): 0.0650
2025-05-02 14:22:28 [INFO]: Epoch 120 - training loss (MAE): 0.0647
2025-05-02 14:22:44 [INFO]: Epoch 121 - training loss (MAE): 0.0654
2025-05-02 14:23:00 [INFO]: Epoch 122 - training loss (MAE): 0.0645
2025-05-02 14:23:16 [INFO]: Epoch 123 - training loss (MAE): 0.0643
2025-05-02 14:23:32 [INFO]: Epoch 124 - training loss (MAE): 0.0645
2025-05-02 14:23:49 [INFO]: Epoch 125 - training loss (MAE): 0.0646
2025-05-02 14:24:05 [INFO]: Epoch 126 - training loss (MAE): 0.0643
2025-05-02 14:24:21 [INFO]: Epoch 127 - training loss (MAE): 0.0646
2025-05-02 14:24:37 [INFO]: Epoch 128 - training loss (MAE): 0.0640
2025-05-02 14:24:53 [INFO]: Epoch 129 - training loss (MAE): 0.0643
2025-05-02 14:25:09 [INFO]: Epoch 130 - training loss (MAE): 0.0643
2025-05-02 14:25:25 [INFO]: Epoch 131 - training loss (MAE): 0.0633
2025-05-02 14:25:42 [INFO]: Epoch 132 - training loss (MAE): 0.0631
2025-05-02 14:25:58 [INFO]: Epoch 133 - training loss (MAE): 0.0636
2025-05-02 14:26:14 [INFO]: Epoch 134 - training loss (MAE): 0.0634
2025-05-02 14:26:30 [INFO]: Epoch 135 - training loss (MAE): 0.0634
2025-05-02 14:26:47 [INFO]: Epoch 136 - training loss (MAE): 0.0633
2025-05-02 14:27:03 [INFO]: Epoch 137 - training loss (MAE): 0.0640
2025-05-02 14:27:19 [INFO]: Epoch 138 - training loss (MAE): 0.0638
2025-05-02 14:27:35 [INFO]: Epoch 139 - training loss (MAE): 0.0629
2025-05-02 14:27:51 [INFO]: Epoch 140 - training loss (MAE): 0.0633
2025-05-02 14:28:08 [INFO]: Epoch 141 - training loss (MAE): 0.0635
2025-05-02 14:28:24 [INFO]: Epoch 142 - training loss (MAE): 0.0630
2025-05-02 14:28:40 [INFO]: Epoch 143 - training loss (MAE): 0.0629
2025-05-02 14:28:56 [INFO]: Epoch 144 - training loss (MAE): 0.0624
2025-05-02 14:29:13 [INFO]: Epoch 145 - training loss (MAE): 0.0627
2025-05-02 14:29:29 [INFO]: Epoch 146 - training loss (MAE): 0.0626
2025-05-02 14:29:45 [INFO]: Epoch 147 - training loss (MAE): 0.0628
2025-05-02 14:30:01 [INFO]: Epoch 148 - training loss (MAE): 0.0617
2025-05-02 14:30:17 [INFO]: Epoch 149 - training loss (MAE): 0.0622
2025-05-02 14:30:33 [INFO]: Epoch 150 - training loss (MAE): 0.0627
2025-05-02 14:30:50 [INFO]: Epoch 151 - training loss (MAE): 0.0620
2025-05-02 14:31:06 [INFO]: Epoch 152 - training loss (MAE): 0.0620
2025-05-02 14:31:22 [INFO]: Epoch 153 - training loss (MAE): 0.0624
2025-05-02 14:31:38 [INFO]: Epoch 154 - training loss (MAE): 0.0616
2025-05-02 14:31:55 [INFO]: Epoch 155 - training loss (MAE): 0.0621
2025-05-02 14:32:11 [INFO]: Epoch 156 - training loss (MAE): 0.0618
2025-05-02 14:32:27 [INFO]: Epoch 157 - training loss (MAE): 0.0619
2025-05-02 14:32:43 [INFO]: Epoch 158 - training loss (MAE): 0.0623
2025-05-02 14:32:59 [INFO]: Epoch 159 - training loss (MAE): 0.0618
2025-05-02 14:33:15 [INFO]: Epoch 160 - training loss (MAE): 0.0621
2025-05-02 14:33:32 [INFO]: Epoch 161 - training loss (MAE): 0.0619
2025-05-02 14:33:48 [INFO]: Epoch 162 - training loss (MAE): 0.0617
2025-05-02 14:34:04 [INFO]: Epoch 163 - training loss (MAE): 0.0610
2025-05-02 14:34:20 [INFO]: Epoch 164 - training loss (MAE): 0.0613
2025-05-02 14:34:36 [INFO]: Epoch 165 - training loss (MAE): 0.0611
2025-05-02 14:34:53 [INFO]: Epoch 166 - training loss (MAE): 0.0609
2025-05-02 14:35:09 [INFO]: Epoch 167 - training loss (MAE): 0.0613
2025-05-02 14:35:25 [INFO]: Epoch 168 - training loss (MAE): 0.0608
2025-05-02 14:35:41 [INFO]: Epoch 169 - training loss (MAE): 0.0611
2025-05-02 14:35:57 [INFO]: Epoch 170 - training loss (MAE): 0.0614
2025-05-02 14:36:13 [INFO]: Epoch 171 - training loss (MAE): 0.0608
2025-05-02 14:36:30 [INFO]: Epoch 172 - training loss (MAE): 0.0614
2025-05-02 14:36:46 [INFO]: Epoch 173 - training loss (MAE): 0.0606
2025-05-02 14:37:02 [INFO]: Epoch 174 - training loss (MAE): 0.0608
2025-05-02 14:37:18 [INFO]: Epoch 175 - training loss (MAE): 0.0609
2025-05-02 14:37:35 [INFO]: Epoch 176 - training loss (MAE): 0.0604
2025-05-02 14:37:51 [INFO]: Epoch 177 - training loss (MAE): 0.0607
2025-05-02 14:38:07 [INFO]: Epoch 178 - training loss (MAE): 0.0608
2025-05-02 14:38:23 [INFO]: Epoch 179 - training loss (MAE): 0.0606
2025-05-02 14:38:39 [INFO]: Epoch 180 - training loss (MAE): 0.0608
2025-05-02 14:38:56 [INFO]: Epoch 181 - training loss (MAE): 0.0607
2025-05-02 14:39:12 [INFO]: Epoch 182 - training loss (MAE): 0.0606
2025-05-02 14:39:28 [INFO]: Epoch 183 - training loss (MAE): 0.0603
2025-05-02 14:39:44 [INFO]: Epoch 184 - training loss (MAE): 0.0604
2025-05-02 14:40:01 [INFO]: Epoch 185 - training loss (MAE): 0.0606
2025-05-02 14:40:17 [INFO]: Epoch 186 - training loss (MAE): 0.0604
2025-05-02 14:40:33 [INFO]: Epoch 187 - training loss (MAE): 0.0602
2025-05-02 14:40:49 [INFO]: Epoch 188 - training loss (MAE): 0.0598
2025-05-02 14:41:05 [INFO]: Epoch 189 - training loss (MAE): 0.0606
2025-05-02 14:41:22 [INFO]: Epoch 190 - training loss (MAE): 0.0604
2025-05-02 14:41:38 [INFO]: Epoch 191 - training loss (MAE): 0.0601
2025-05-02 14:41:54 [INFO]: Epoch 192 - training loss (MAE): 0.0605
2025-05-02 14:42:10 [INFO]: Epoch 193 - training loss (MAE): 0.0601
2025-05-02 14:42:26 [INFO]: Epoch 194 - training loss (MAE): 0.0597
2025-05-02 14:42:43 [INFO]: Epoch 195 - training loss (MAE): 0.0601
2025-05-02 14:42:59 [INFO]: Epoch 196 - training loss (MAE): 0.0599
2025-05-02 14:43:15 [INFO]: Epoch 197 - training loss (MAE): 0.0595
2025-05-02 14:43:31 [INFO]: Epoch 198 - training loss (MAE): 0.0599
2025-05-02 14:43:47 [INFO]: Epoch 199 - training loss (MAE): 0.0597
2025-05-02 14:44:04 [INFO]: Epoch 200 - training loss (MAE): 0.0595
2025-05-02 14:44:20 [INFO]: Epoch 201 - training loss (MAE): 0.0601
2025-05-02 14:44:36 [INFO]: Epoch 202 - training loss (MAE): 0.0596
2025-05-02 14:44:53 [INFO]: Epoch 203 - training loss (MAE): 0.0595
2025-05-02 14:45:09 [INFO]: Epoch 204 - training loss (MAE): 0.0598
2025-05-02 14:45:25 [INFO]: Epoch 205 - training loss (MAE): 0.0596
2025-05-02 14:45:41 [INFO]: Epoch 206 - training loss (MAE): 0.0592
2025-05-02 14:45:57 [INFO]: Epoch 207 - training loss (MAE): 0.0594
2025-05-02 14:46:13 [INFO]: Epoch 208 - training loss (MAE): 0.0591
2025-05-02 14:46:30 [INFO]: Epoch 209 - training loss (MAE): 0.0592
2025-05-02 14:46:46 [INFO]: Epoch 210 - training loss (MAE): 0.0592
2025-05-02 14:47:02 [INFO]: Epoch 211 - training loss (MAE): 0.0592
2025-05-02 14:47:18 [INFO]: Epoch 212 - training loss (MAE): 0.0592
2025-05-02 14:47:35 [INFO]: Epoch 213 - training loss (MAE): 0.0590
2025-05-02 14:47:51 [INFO]: Epoch 214 - training loss (MAE): 0.0591
2025-05-02 14:48:07 [INFO]: Epoch 215 - training loss (MAE): 0.0589
2025-05-02 14:48:23 [INFO]: Epoch 216 - training loss (MAE): 0.0586
2025-05-02 14:48:39 [INFO]: Epoch 217 - training loss (MAE): 0.0589
2025-05-02 14:48:56 [INFO]: Epoch 218 - training loss (MAE): 0.0591
2025-05-02 14:49:12 [INFO]: Epoch 219 - training loss (MAE): 0.0584
2025-05-02 14:49:28 [INFO]: Epoch 220 - training loss (MAE): 0.0590
2025-05-02 14:49:44 [INFO]: Epoch 221 - training loss (MAE): 0.0587
2025-05-02 14:50:00 [INFO]: Epoch 222 - training loss (MAE): 0.0590
2025-05-02 14:50:17 [INFO]: Epoch 223 - training loss (MAE): 0.0585
2025-05-02 14:50:33 [INFO]: Epoch 224 - training loss (MAE): 0.0589
2025-05-02 14:50:49 [INFO]: Epoch 225 - training loss (MAE): 0.0585
2025-05-02 14:51:05 [INFO]: Epoch 226 - training loss (MAE): 0.0585
2025-05-02 14:51:21 [INFO]: Epoch 227 - training loss (MAE): 0.0582
2025-05-02 14:51:38 [INFO]: Epoch 228 - training loss (MAE): 0.0589
2025-05-02 14:51:54 [INFO]: Epoch 229 - training loss (MAE): 0.0588
2025-05-02 14:52:10 [INFO]: Epoch 230 - training loss (MAE): 0.0583
2025-05-02 14:52:26 [INFO]: Epoch 231 - training loss (MAE): 0.0585
2025-05-02 14:52:43 [INFO]: Epoch 232 - training loss (MAE): 0.0586
2025-05-02 14:52:59 [INFO]: Epoch 233 - training loss (MAE): 0.0585
2025-05-02 14:53:15 [INFO]: Epoch 234 - training loss (MAE): 0.0584
2025-05-02 14:53:31 [INFO]: Epoch 235 - training loss (MAE): 0.0585
2025-05-02 14:53:47 [INFO]: Epoch 236 - training loss (MAE): 0.0588
2025-05-02 14:54:03 [INFO]: Epoch 237 - training loss (MAE): 0.0580
2025-05-02 14:54:20 [INFO]: Epoch 238 - training loss (MAE): 0.0584
2025-05-02 14:54:36 [INFO]: Epoch 239 - training loss (MAE): 0.0579
2025-05-02 14:54:52 [INFO]: Epoch 240 - training loss (MAE): 0.0585
2025-05-02 14:55:08 [INFO]: Epoch 241 - training loss (MAE): 0.0581
2025-05-02 14:55:24 [INFO]: Epoch 242 - training loss (MAE): 0.0581
2025-05-02 14:55:41 [INFO]: Epoch 243 - training loss (MAE): 0.0576
2025-05-02 14:55:57 [INFO]: Epoch 244 - training loss (MAE): 0.0579
2025-05-02 14:56:13 [INFO]: Epoch 245 - training loss (MAE): 0.0584
2025-05-02 14:56:29 [INFO]: Epoch 246 - training loss (MAE): 0.0581
2025-05-02 14:56:45 [INFO]: Epoch 247 - training loss (MAE): 0.0580
2025-05-02 14:57:02 [INFO]: Epoch 248 - training loss (MAE): 0.0584
2025-05-02 14:57:18 [INFO]: Epoch 249 - training loss (MAE): 0.0578
2025-05-02 14:57:34 [INFO]: Epoch 250 - training loss (MAE): 0.0581
2025-05-02 14:57:50 [INFO]: Epoch 251 - training loss (MAE): 0.0574
2025-05-02 14:58:06 [INFO]: Epoch 252 - training loss (MAE): 0.0574
2025-05-02 14:58:23 [INFO]: Epoch 253 - training loss (MAE): 0.0578
2025-05-02 14:58:39 [INFO]: Epoch 254 - training loss (MAE): 0.0584
2025-05-02 14:58:55 [INFO]: Epoch 255 - training loss (MAE): 0.0577
2025-05-02 14:59:11 [INFO]: Epoch 256 - training loss (MAE): 0.0573
2025-05-02 14:59:27 [INFO]: Epoch 257 - training loss (MAE): 0.0582
2025-05-02 14:59:44 [INFO]: Epoch 258 - training loss (MAE): 0.0576
2025-05-02 15:00:00 [INFO]: Epoch 259 - training loss (MAE): 0.0574
2025-05-02 15:00:16 [INFO]: Epoch 260 - training loss (MAE): 0.0577
2025-05-02 15:00:32 [INFO]: Epoch 261 - training loss (MAE): 0.0573
2025-05-02 15:00:48 [INFO]: Epoch 262 - training loss (MAE): 0.0575
2025-05-02 15:01:04 [INFO]: Epoch 263 - training loss (MAE): 0.0572
2025-05-02 15:01:21 [INFO]: Epoch 264 - training loss (MAE): 0.0572
2025-05-02 15:01:37 [INFO]: Epoch 265 - training loss (MAE): 0.0575
2025-05-02 15:01:53 [INFO]: Epoch 266 - training loss (MAE): 0.0573
2025-05-02 15:02:09 [INFO]: Epoch 267 - training loss (MAE): 0.0569
2025-05-02 15:02:26 [INFO]: Epoch 268 - training loss (MAE): 0.0575
2025-05-02 15:02:42 [INFO]: Epoch 269 - training loss (MAE): 0.0569
2025-05-02 15:02:58 [INFO]: Epoch 270 - training loss (MAE): 0.0573
2025-05-02 15:03:14 [INFO]: Epoch 271 - training loss (MAE): 0.0571
2025-05-02 15:03:30 [INFO]: Epoch 272 - training loss (MAE): 0.0574
2025-05-02 15:03:47 [INFO]: Epoch 273 - training loss (MAE): 0.0571
2025-05-02 15:04:03 [INFO]: Epoch 274 - training loss (MAE): 0.0564
2025-05-02 15:04:19 [INFO]: Epoch 275 - training loss (MAE): 0.0573
2025-05-02 15:04:36 [INFO]: Epoch 276 - training loss (MAE): 0.0573
2025-05-02 15:04:52 [INFO]: Epoch 277 - training loss (MAE): 0.0570
2025-05-02 15:05:08 [INFO]: Epoch 278 - training loss (MAE): 0.0571
2025-05-02 15:05:24 [INFO]: Epoch 279 - training loss (MAE): 0.0567
2025-05-02 15:05:40 [INFO]: Epoch 280 - training loss (MAE): 0.0567
2025-05-02 15:05:56 [INFO]: Epoch 281 - training loss (MAE): 0.0564
2025-05-02 15:06:12 [INFO]: Epoch 282 - training loss (MAE): 0.0566
2025-05-02 15:06:29 [INFO]: Epoch 283 - training loss (MAE): 0.0570
2025-05-02 15:06:45 [INFO]: Epoch 284 - training loss (MAE): 0.0575
2025-05-02 15:07:02 [INFO]: Epoch 285 - training loss (MAE): 0.0572
2025-05-02 15:07:18 [INFO]: Epoch 286 - training loss (MAE): 0.0571
2025-05-02 15:07:34 [INFO]: Epoch 287 - training loss (MAE): 0.0569
2025-05-02 15:07:50 [INFO]: Epoch 288 - training loss (MAE): 0.0563
2025-05-02 15:08:06 [INFO]: Epoch 289 - training loss (MAE): 0.0564
2025-05-02 15:08:22 [INFO]: Epoch 290 - training loss (MAE): 0.0570
2025-05-02 15:08:39 [INFO]: Epoch 291 - training loss (MAE): 0.0562
2025-05-02 15:08:55 [INFO]: Epoch 292 - training loss (MAE): 0.0563
2025-05-02 15:09:11 [INFO]: Epoch 293 - training loss (MAE): 0.0561
2025-05-02 15:09:27 [INFO]: Epoch 294 - training loss (MAE): 0.0566
2025-05-02 15:09:43 [INFO]: Epoch 295 - training loss (MAE): 0.0560
2025-05-02 15:10:00 [INFO]: Epoch 296 - training loss (MAE): 0.0567
2025-05-02 15:10:16 [INFO]: Epoch 297 - training loss (MAE): 0.0558
2025-05-02 15:10:32 [INFO]: Epoch 298 - training loss (MAE): 0.0563
2025-05-02 15:10:48 [INFO]: Epoch 299 - training loss (MAE): 0.0556
2025-05-02 15:11:04 [INFO]: Epoch 300 - training loss (MAE): 0.0563
2025-05-02 15:11:20 [INFO]: Epoch 301 - training loss (MAE): 0.0562
2025-05-02 15:11:36 [INFO]: Epoch 302 - training loss (MAE): 0.0559
2025-05-02 15:11:53 [INFO]: Epoch 303 - training loss (MAE): 0.0558
2025-05-02 15:12:09 [INFO]: Epoch 304 - training loss (MAE): 0.0563
2025-05-02 15:12:25 [INFO]: Epoch 305 - training loss (MAE): 0.0558
2025-05-02 15:12:42 [INFO]: Epoch 306 - training loss (MAE): 0.0556
2025-05-02 15:12:58 [INFO]: Epoch 307 - training loss (MAE): 0.0557
2025-05-02 15:13:14 [INFO]: Epoch 308 - training loss (MAE): 0.0572
2025-05-02 15:13:30 [INFO]: Epoch 309 - training loss (MAE): 0.0558
2025-05-02 15:13:47 [INFO]: Epoch 310 - training loss (MAE): 0.0559
2025-05-02 15:14:03 [INFO]: Epoch 311 - training loss (MAE): 0.0556
2025-05-02 15:14:19 [INFO]: Epoch 312 - training loss (MAE): 0.0557
2025-05-02 15:14:36 [INFO]: Epoch 313 - training loss (MAE): 0.0552
2025-05-02 15:14:52 [INFO]: Epoch 314 - training loss (MAE): 0.0551
2025-05-02 15:15:08 [INFO]: Epoch 315 - training loss (MAE): 0.0564
2025-05-02 15:15:24 [INFO]: Epoch 316 - training loss (MAE): 0.0558
2025-05-02 15:15:40 [INFO]: Epoch 317 - training loss (MAE): 0.0552
2025-05-02 15:15:56 [INFO]: Epoch 318 - training loss (MAE): 0.0557
2025-05-02 15:16:13 [INFO]: Epoch 319 - training loss (MAE): 0.0552
2025-05-02 15:16:29 [INFO]: Epoch 320 - training loss (MAE): 0.0551
2025-05-02 15:16:45 [INFO]: Epoch 321 - training loss (MAE): 0.0552
2025-05-02 15:17:01 [INFO]: Epoch 322 - training loss (MAE): 0.0551
2025-05-02 15:17:17 [INFO]: Epoch 323 - training loss (MAE): 0.0560
2025-05-02 15:17:33 [INFO]: Epoch 324 - training loss (MAE): 0.0550
2025-05-02 15:17:50 [INFO]: Epoch 325 - training loss (MAE): 0.0553
2025-05-02 15:18:06 [INFO]: Epoch 326 - training loss (MAE): 0.0553
2025-05-02 15:18:22 [INFO]: Epoch 327 - training loss (MAE): 0.0551
2025-05-02 15:18:38 [INFO]: Epoch 328 - training loss (MAE): 0.0551
2025-05-02 15:18:55 [INFO]: Epoch 329 - training loss (MAE): 0.0550
2025-05-02 15:19:11 [INFO]: Epoch 330 - training loss (MAE): 0.0547
2025-05-02 15:19:27 [INFO]: Epoch 331 - training loss (MAE): 0.0548
2025-05-02 15:19:43 [INFO]: Epoch 332 - training loss (MAE): 0.0546
2025-05-02 15:20:00 [INFO]: Epoch 333 - training loss (MAE): 0.0546
2025-05-02 15:20:16 [INFO]: Epoch 334 - training loss (MAE): 0.0546
2025-05-02 15:20:32 [INFO]: Epoch 335 - training loss (MAE): 0.0547
2025-05-02 15:20:48 [INFO]: Epoch 336 - training loss (MAE): 0.0546
2025-05-02 15:21:04 [INFO]: Epoch 337 - training loss (MAE): 0.0546
2025-05-02 15:21:20 [INFO]: Epoch 338 - training loss (MAE): 0.0550
2025-05-02 15:21:37 [INFO]: Epoch 339 - training loss (MAE): 0.0549
2025-05-02 15:21:53 [INFO]: Epoch 340 - training loss (MAE): 0.0549
2025-05-02 15:22:09 [INFO]: Epoch 341 - training loss (MAE): 0.0540
2025-05-02 15:22:25 [INFO]: Epoch 342 - training loss (MAE): 0.0541
2025-05-02 15:22:41 [INFO]: Epoch 343 - training loss (MAE): 0.0541
2025-05-02 15:22:57 [INFO]: Epoch 344 - training loss (MAE): 0.0539
2025-05-02 15:23:13 [INFO]: Epoch 345 - training loss (MAE): 0.0544
2025-05-02 15:23:30 [INFO]: Epoch 346 - training loss (MAE): 0.0540
2025-05-02 15:23:46 [INFO]: Epoch 347 - training loss (MAE): 0.0544
2025-05-02 15:24:02 [INFO]: Epoch 348 - training loss (MAE): 0.0539
2025-05-02 15:24:18 [INFO]: Epoch 349 - training loss (MAE): 0.0539
2025-05-02 15:24:34 [INFO]: Epoch 350 - training loss (MAE): 0.0540
2025-05-02 15:24:51 [INFO]: Epoch 351 - training loss (MAE): 0.0538
2025-05-02 15:25:07 [INFO]: Epoch 352 - training loss (MAE): 0.0537
2025-05-02 15:25:23 [INFO]: Epoch 353 - training loss (MAE): 0.0539
2025-05-02 15:25:40 [INFO]: Epoch 354 - training loss (MAE): 0.0536
2025-05-02 15:25:56 [INFO]: Epoch 355 - training loss (MAE): 0.0539
2025-05-02 15:26:12 [INFO]: Epoch 356 - training loss (MAE): 0.0537
2025-05-02 15:26:28 [INFO]: Epoch 357 - training loss (MAE): 0.0538
2025-05-02 15:26:45 [INFO]: Epoch 358 - training loss (MAE): 0.0549
2025-05-02 15:27:01 [INFO]: Epoch 359 - training loss (MAE): 0.0532
2025-05-02 15:27:17 [INFO]: Epoch 360 - training loss (MAE): 0.0536
2025-05-02 15:27:33 [INFO]: Epoch 361 - training loss (MAE): 0.0530
2025-05-02 15:27:49 [INFO]: Epoch 362 - training loss (MAE): 0.0533
2025-05-02 15:28:05 [INFO]: Epoch 363 - training loss (MAE): 0.0535
2025-05-02 15:28:22 [INFO]: Epoch 364 - training loss (MAE): 0.0535
2025-05-02 15:28:38 [INFO]: Epoch 365 - training loss (MAE): 0.0530
2025-05-02 15:28:54 [INFO]: Epoch 366 - training loss (MAE): 0.0532
2025-05-02 15:29:11 [INFO]: Epoch 367 - training loss (MAE): 0.0539
2025-05-02 15:29:27 [INFO]: Epoch 368 - training loss (MAE): 0.0530
2025-05-02 15:29:43 [INFO]: Epoch 369 - training loss (MAE): 0.0529
2025-05-02 15:29:59 [INFO]: Epoch 370 - training loss (MAE): 0.0528
2025-05-02 15:30:15 [INFO]: Epoch 371 - training loss (MAE): 0.0531
2025-05-02 15:30:32 [INFO]: Epoch 372 - training loss (MAE): 0.0527
2025-05-02 15:30:48 [INFO]: Epoch 373 - training loss (MAE): 0.0527
2025-05-02 15:31:04 [INFO]: Epoch 374 - training loss (MAE): 0.0522
2025-05-02 15:31:20 [INFO]: Epoch 375 - training loss (MAE): 0.0532
2025-05-02 15:31:37 [INFO]: Epoch 376 - training loss (MAE): 0.0528
2025-05-02 15:31:53 [INFO]: Epoch 377 - training loss (MAE): 0.0526
2025-05-02 15:32:09 [INFO]: Epoch 378 - training loss (MAE): 0.0526
2025-05-02 15:32:25 [INFO]: Epoch 379 - training loss (MAE): 0.0529
2025-05-02 15:32:41 [INFO]: Epoch 380 - training loss (MAE): 0.0530
2025-05-02 15:32:57 [INFO]: Epoch 381 - training loss (MAE): 0.0529
2025-05-02 15:33:14 [INFO]: Epoch 382 - training loss (MAE): 0.0527
2025-05-02 15:33:30 [INFO]: Epoch 383 - training loss (MAE): 0.0522
2025-05-02 15:33:46 [INFO]: Epoch 384 - training loss (MAE): 0.0522
2025-05-02 15:34:02 [INFO]: Epoch 385 - training loss (MAE): 0.0524
2025-05-02 15:34:18 [INFO]: Epoch 386 - training loss (MAE): 0.0521
2025-05-02 15:34:35 [INFO]: Epoch 387 - training loss (MAE): 0.0526
2025-05-02 15:34:51 [INFO]: Epoch 388 - training loss (MAE): 0.0516
2025-05-02 15:35:07 [INFO]: Epoch 389 - training loss (MAE): 0.0521
2025-05-02 15:35:23 [INFO]: Epoch 390 - training loss (MAE): 0.0525
2025-05-02 15:35:39 [INFO]: Epoch 391 - training loss (MAE): 0.0514
2025-05-02 15:35:56 [INFO]: Epoch 392 - training loss (MAE): 0.0518
2025-05-02 15:36:12 [INFO]: Epoch 393 - training loss (MAE): 0.0517
2025-05-02 15:36:28 [INFO]: Epoch 394 - training loss (MAE): 0.0515
2025-05-02 15:36:44 [INFO]: Epoch 395 - training loss (MAE): 0.0517
2025-05-02 15:37:01 [INFO]: Epoch 396 - training loss (MAE): 0.0517
2025-05-02 15:37:17 [INFO]: Epoch 397 - training loss (MAE): 0.0516
2025-05-02 15:37:33 [INFO]: Epoch 398 - training loss (MAE): 0.0520
2025-05-02 15:37:50 [INFO]: Epoch 399 - training loss (MAE): 0.0520
2025-05-02 15:38:06 [INFO]: Epoch 400 - training loss (MAE): 0.0519
2025-05-02 15:38:22 [INFO]: Epoch 401 - training loss (MAE): 0.0509
2025-05-02 15:38:38 [INFO]: Epoch 402 - training loss (MAE): 0.0513
2025-05-02 15:38:55 [INFO]: Epoch 403 - training loss (MAE): 0.0514
2025-05-02 15:39:11 [INFO]: Epoch 404 - training loss (MAE): 0.0513
2025-05-02 15:39:27 [INFO]: Epoch 405 - training loss (MAE): 0.0513
2025-05-02 15:39:43 [INFO]: Epoch 406 - training loss (MAE): 0.0516
2025-05-02 15:39:59 [INFO]: Epoch 407 - training loss (MAE): 0.0520
2025-05-02 15:40:15 [INFO]: Epoch 408 - training loss (MAE): 0.0516
2025-05-02 15:40:31 [INFO]: Epoch 409 - training loss (MAE): 0.0508
2025-05-02 15:40:48 [INFO]: Epoch 410 - training loss (MAE): 0.0512
2025-05-02 15:41:04 [INFO]: Epoch 411 - training loss (MAE): 0.0512
2025-05-02 15:41:20 [INFO]: Epoch 412 - training loss (MAE): 0.0507
2025-05-02 15:41:36 [INFO]: Epoch 413 - training loss (MAE): 0.0511
2025-05-02 15:41:52 [INFO]: Epoch 414 - training loss (MAE): 0.0507
2025-05-02 15:42:09 [INFO]: Epoch 415 - training loss (MAE): 0.0511
2025-05-02 15:42:25 [INFO]: Epoch 416 - training loss (MAE): 0.0511
2025-05-02 15:42:41 [INFO]: Epoch 417 - training loss (MAE): 0.0505
2025-05-02 15:42:57 [INFO]: Epoch 418 - training loss (MAE): 0.0508
2025-05-02 15:43:13 [INFO]: Epoch 419 - training loss (MAE): 0.0506
2025-05-02 15:43:29 [INFO]: Epoch 420 - training loss (MAE): 0.0510
2025-05-02 15:43:45 [INFO]: Epoch 421 - training loss (MAE): 0.0504
2025-05-02 15:44:02 [INFO]: Epoch 422 - training loss (MAE): 0.0503
2025-05-02 15:44:18 [INFO]: Epoch 423 - training loss (MAE): 0.0507
2025-05-02 15:44:34 [INFO]: Epoch 424 - training loss (MAE): 0.0506
2025-05-02 15:44:50 [INFO]: Epoch 425 - training loss (MAE): 0.0502
2025-05-02 15:45:06 [INFO]: Epoch 426 - training loss (MAE): 0.0500
2025-05-02 15:45:23 [INFO]: Epoch 427 - training loss (MAE): 0.0505
2025-05-02 15:45:39 [INFO]: Epoch 428 - training loss (MAE): 0.0501
2025-05-02 15:45:55 [INFO]: Epoch 429 - training loss (MAE): 0.0503
2025-05-02 15:46:11 [INFO]: Epoch 430 - training loss (MAE): 0.0499
2025-05-02 15:46:27 [INFO]: Epoch 431 - training loss (MAE): 0.0502
2025-05-02 15:46:44 [INFO]: Epoch 432 - training loss (MAE): 0.0505
2025-05-02 15:47:00 [INFO]: Epoch 433 - training loss (MAE): 0.0500
2025-05-02 15:47:16 [INFO]: Epoch 434 - training loss (MAE): 0.0502
2025-05-02 15:47:32 [INFO]: Epoch 435 - training loss (MAE): 0.0497
2025-05-02 15:47:48 [INFO]: Epoch 436 - training loss (MAE): 0.0500
2025-05-02 15:48:05 [INFO]: Epoch 437 - training loss (MAE): 0.0497
2025-05-02 15:48:21 [INFO]: Epoch 438 - training loss (MAE): 0.0506
2025-05-02 15:48:37 [INFO]: Epoch 439 - training loss (MAE): 0.0497
2025-05-02 15:48:54 [INFO]: Epoch 440 - training loss (MAE): 0.0500
2025-05-02 15:49:10 [INFO]: Epoch 441 - training loss (MAE): 0.0494
2025-05-02 15:49:26 [INFO]: Epoch 442 - training loss (MAE): 0.0497
2025-05-02 15:49:42 [INFO]: Epoch 443 - training loss (MAE): 0.0494
2025-05-02 15:49:58 [INFO]: Epoch 444 - training loss (MAE): 0.0494
2025-05-02 15:50:15 [INFO]: Epoch 445 - training loss (MAE): 0.0507
2025-05-02 15:50:31 [INFO]: Epoch 446 - training loss (MAE): 0.0498
2025-05-02 15:50:47 [INFO]: Epoch 447 - training loss (MAE): 0.0494
2025-05-02 15:51:03 [INFO]: Epoch 448 - training loss (MAE): 0.0502
2025-05-02 15:51:19 [INFO]: Epoch 449 - training loss (MAE): 0.0498
2025-05-02 15:51:36 [INFO]: Epoch 450 - training loss (MAE): 0.0498
2025-05-02 15:51:52 [INFO]: Epoch 451 - training loss (MAE): 0.0505
2025-05-02 15:52:08 [INFO]: Epoch 452 - training loss (MAE): 0.0491
2025-05-02 15:52:24 [INFO]: Epoch 453 - training loss (MAE): 0.0505
2025-05-02 15:52:40 [INFO]: Epoch 454 - training loss (MAE): 0.0491
2025-05-02 15:52:57 [INFO]: Epoch 455 - training loss (MAE): 0.0496
2025-05-02 15:53:13 [INFO]: Epoch 456 - training loss (MAE): 0.0494
2025-05-02 15:53:29 [INFO]: Epoch 457 - training loss (MAE): 0.0488
2025-05-02 15:53:45 [INFO]: Epoch 458 - training loss (MAE): 0.0489
2025-05-02 15:54:01 [INFO]: Epoch 459 - training loss (MAE): 0.0493
2025-05-02 15:54:17 [INFO]: Epoch 460 - training loss (MAE): 0.0486
2025-05-02 15:54:34 [INFO]: Epoch 461 - training loss (MAE): 0.0494
2025-05-02 15:54:50 [INFO]: Epoch 462 - training loss (MAE): 0.0490
2025-05-02 15:55:06 [INFO]: Epoch 463 - training loss (MAE): 0.0489
2025-05-02 15:55:23 [INFO]: Epoch 464 - training loss (MAE): 0.0489
2025-05-02 15:55:39 [INFO]: Epoch 465 - training loss (MAE): 0.0487
2025-05-02 15:55:55 [INFO]: Epoch 466 - training loss (MAE): 0.0486
2025-05-02 15:56:11 [INFO]: Epoch 467 - training loss (MAE): 0.0495
2025-05-02 15:56:27 [INFO]: Epoch 468 - training loss (MAE): 0.0483
2025-05-02 15:56:44 [INFO]: Epoch 469 - training loss (MAE): 0.0493
2025-05-02 15:57:00 [INFO]: Epoch 470 - training loss (MAE): 0.0492
2025-05-02 15:57:16 [INFO]: Epoch 471 - training loss (MAE): 0.0481
2025-05-02 15:57:32 [INFO]: Epoch 472 - training loss (MAE): 0.0487
2025-05-02 15:57:48 [INFO]: Epoch 473 - training loss (MAE): 0.0484
2025-05-02 15:58:05 [INFO]: Epoch 474 - training loss (MAE): 0.0492
2025-05-02 15:58:21 [INFO]: Epoch 475 - training loss (MAE): 0.0483
2025-05-02 15:58:37 [INFO]: Epoch 476 - training loss (MAE): 0.0486
2025-05-02 15:58:53 [INFO]: Epoch 477 - training loss (MAE): 0.0482
2025-05-02 15:59:10 [INFO]: Epoch 478 - training loss (MAE): 0.0482
2025-05-02 15:59:26 [INFO]: Epoch 479 - training loss (MAE): 0.0482
2025-05-02 15:59:42 [INFO]: Epoch 480 - training loss (MAE): 0.0481
2025-05-02 15:59:58 [INFO]: Epoch 481 - training loss (MAE): 0.0484
2025-05-02 16:00:14 [INFO]: Epoch 482 - training loss (MAE): 0.0487
2025-05-02 16:00:31 [INFO]: Epoch 483 - training loss (MAE): 0.0480
2025-05-02 16:00:47 [INFO]: Epoch 484 - training loss (MAE): 0.0489
2025-05-02 16:01:03 [INFO]: Epoch 485 - training loss (MAE): 0.0485
2025-05-02 16:01:19 [INFO]: Epoch 486 - training loss (MAE): 0.0478
2025-05-02 16:01:35 [INFO]: Epoch 487 - training loss (MAE): 0.0483
2025-05-02 16:01:52 [INFO]: Epoch 488 - training loss (MAE): 0.0483
2025-05-02 16:02:08 [INFO]: Epoch 489 - training loss (MAE): 0.0480
2025-05-02 16:02:24 [INFO]: Epoch 490 - training loss (MAE): 0.0493
2025-05-02 16:02:41 [INFO]: Epoch 491 - training loss (MAE): 0.0478
2025-05-02 16:02:57 [INFO]: Epoch 492 - training loss (MAE): 0.0488
2025-05-02 16:03:13 [INFO]: Epoch 493 - training loss (MAE): 0.0481
2025-05-02 16:03:30 [INFO]: Epoch 494 - training loss (MAE): 0.0479
2025-05-02 16:03:46 [INFO]: Epoch 495 - training loss (MAE): 0.0481
2025-05-02 16:04:02 [INFO]: Epoch 496 - training loss (MAE): 0.0479
2025-05-02 16:04:19 [INFO]: Epoch 497 - training loss (MAE): 0.0474
2025-05-02 16:04:35 [INFO]: Epoch 498 - training loss (MAE): 0.0481
2025-05-02 16:04:51 [INFO]: Epoch 499 - training loss (MAE): 0.0482
2025-05-02 16:05:07 [INFO]: Epoch 500 - training loss (MAE): 0.0486
2025-05-02 16:05:07 [INFO]: Finished training. The best model is from epoch#497.
2025-05-02 16:05:12 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-02 16:05:12 [INFO]: Saved the model to saits_weights/saits_all_artificial.pypots
2025-05-02 16:05:12 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial.pypots
Trainning metrics
MAE: 0.290, MSE: 0.355, MRE: 0.408
Testing Results:
MAE: 0.255, MSE: 0.268, MRE: 0.347
Training fold 1/10
2025-05-02 16:05:13 [INFO]: No given device, using default device: cuda
2025-05-02 16:05:13 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-02 16:05:13 [INFO]: Using customized MAE as the training loss function.
2025-05-02 16:05:13 [INFO]: Using customized MSE as the validation metric function.
2025-05-02 16:05:13 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-02 16:05:28 [INFO]: Epoch 001 - training loss (MAE): 0.8804, validation MSE: -0.1734
2025-05-02 16:05:44 [INFO]: Epoch 002 - training loss (MAE): 0.3847, validation MSE: -0.2173
2025-05-02 16:05:59 [INFO]: Epoch 003 - training loss (MAE): 0.3284, validation MSE: -0.2260
2025-05-02 16:06:14 [INFO]: Epoch 004 - training loss (MAE): 0.2989, validation MSE: -0.2763
2025-05-02 16:06:29 [INFO]: Epoch 005 - training loss (MAE): 0.2740, validation MSE: -0.3002
2025-05-02 16:06:44 [INFO]: Epoch 006 - training loss (MAE): 0.2474, validation MSE: -0.3220
2025-05-02 16:06:59 [INFO]: Epoch 007 - training loss (MAE): 0.2238, validation MSE: -0.3366
2025-05-02 16:07:15 [INFO]: Epoch 008 - training loss (MAE): 0.2034, validation MSE: -0.3653
2025-05-02 16:07:30 [INFO]: Epoch 009 - training loss (MAE): 0.1859, validation MSE: -0.3518
2025-05-02 16:07:45 [INFO]: Epoch 010 - training loss (MAE): 0.1728, validation MSE: -0.3738
2025-05-02 16:08:00 [INFO]: Epoch 011 - training loss (MAE): 0.1613, validation MSE: -0.3808
2025-05-02 16:08:15 [INFO]: Epoch 012 - training loss (MAE): 0.1519, validation MSE: -0.3680
2025-05-02 16:08:31 [INFO]: Epoch 013 - training loss (MAE): 0.1437, validation MSE: -0.3670
2025-05-02 16:08:46 [INFO]: Epoch 014 - training loss (MAE): 0.1377, validation MSE: -0.3722
2025-05-02 16:09:01 [INFO]: Epoch 015 - training loss (MAE): 0.1320, validation MSE: -0.3802
2025-05-02 16:09:16 [INFO]: Epoch 016 - training loss (MAE): 0.1273, validation MSE: -0.3722
2025-05-02 16:09:31 [INFO]: Epoch 017 - training loss (MAE): 0.1234, validation MSE: -0.3728
2025-05-02 16:09:46 [INFO]: Epoch 018 - training loss (MAE): 0.1197, validation MSE: -0.3987
2025-05-02 16:10:02 [INFO]: Epoch 019 - training loss (MAE): 0.1182, validation MSE: -0.3799
2025-05-02 16:10:17 [INFO]: Epoch 020 - training loss (MAE): 0.1131, validation MSE: -0.3952
2025-05-02 16:10:32 [INFO]: Epoch 021 - training loss (MAE): 0.1125, validation MSE: -0.3987
2025-05-02 16:10:47 [INFO]: Epoch 022 - training loss (MAE): 0.1093, validation MSE: -0.3931
2025-05-02 16:11:02 [INFO]: Epoch 023 - training loss (MAE): 0.1071, validation MSE: -0.3918
2025-05-02 16:11:17 [INFO]: Epoch 024 - training loss (MAE): 0.1054, validation MSE: -0.3984
2025-05-02 16:11:32 [INFO]: Epoch 025 - training loss (MAE): 0.1026, validation MSE: -0.3991
2025-05-02 16:11:48 [INFO]: Epoch 026 - training loss (MAE): 0.1015, validation MSE: -0.4043
2025-05-02 16:12:03 [INFO]: Epoch 027 - training loss (MAE): 0.0999, validation MSE: -0.3968
2025-05-02 16:12:18 [INFO]: Epoch 028 - training loss (MAE): 0.0985, validation MSE: -0.4045
2025-05-02 16:12:33 [INFO]: Epoch 029 - training loss (MAE): 0.0982, validation MSE: -0.3812
2025-05-02 16:12:48 [INFO]: Epoch 030 - training loss (MAE): 0.0964, validation MSE: -0.4029
2025-05-02 16:13:03 [INFO]: Epoch 031 - training loss (MAE): 0.0949, validation MSE: -0.4070
2025-05-02 16:13:18 [INFO]: Epoch 032 - training loss (MAE): 0.0937, validation MSE: -0.4143
2025-05-02 16:13:34 [INFO]: Epoch 033 - training loss (MAE): 0.0929, validation MSE: -0.4024
2025-05-02 16:13:49 [INFO]: Epoch 034 - training loss (MAE): 0.0914, validation MSE: -0.4056
2025-05-02 16:14:05 [INFO]: Epoch 035 - training loss (MAE): 0.0908, validation MSE: -0.4169
2025-05-02 16:14:20 [INFO]: Epoch 036 - training loss (MAE): 0.0903, validation MSE: -0.3989
2025-05-02 16:14:35 [INFO]: Epoch 037 - training loss (MAE): 0.0888, validation MSE: -0.4033
2025-05-02 16:14:50 [INFO]: Epoch 038 - training loss (MAE): 0.0880, validation MSE: -0.3874
2025-05-02 16:15:05 [INFO]: Epoch 039 - training loss (MAE): 0.0878, validation MSE: -0.4097
2025-05-02 16:15:21 [INFO]: Epoch 040 - training loss (MAE): 0.0869, validation MSE: -0.3982
2025-05-02 16:15:36 [INFO]: Epoch 041 - training loss (MAE): 0.0863, validation MSE: -0.4104
2025-05-02 16:15:51 [INFO]: Epoch 042 - training loss (MAE): 0.0856, validation MSE: -0.3864
2025-05-02 16:16:07 [INFO]: Epoch 043 - training loss (MAE): 0.0859, validation MSE: -0.4160
2025-05-02 16:16:22 [INFO]: Epoch 044 - training loss (MAE): 0.0845, validation MSE: -0.4099
2025-05-02 16:16:37 [INFO]: Epoch 045 - training loss (MAE): 0.0842, validation MSE: -0.4045
2025-05-02 16:16:53 [INFO]: Epoch 046 - training loss (MAE): 0.0834, validation MSE: -0.4062
2025-05-02 16:17:08 [INFO]: Epoch 047 - training loss (MAE): 0.0822, validation MSE: -0.4096
2025-05-02 16:17:23 [INFO]: Epoch 048 - training loss (MAE): 0.0814, validation MSE: -0.3963
2025-05-02 16:17:38 [INFO]: Epoch 049 - training loss (MAE): 0.0835, validation MSE: -0.4097
2025-05-02 16:17:53 [INFO]: Epoch 050 - training loss (MAE): 0.0811, validation MSE: -0.4092
2025-05-02 16:18:08 [INFO]: Epoch 051 - training loss (MAE): 0.0808, validation MSE: -0.3905
2025-05-02 16:18:24 [INFO]: Epoch 052 - training loss (MAE): 0.0811, validation MSE: -0.4039
2025-05-02 16:18:39 [INFO]: Epoch 053 - training loss (MAE): 0.0795, validation MSE: -0.3950
2025-05-02 16:18:54 [INFO]: Epoch 054 - training loss (MAE): 0.0786, validation MSE: -0.4043
2025-05-02 16:19:09 [INFO]: Epoch 055 - training loss (MAE): 0.0787, validation MSE: -0.3949
2025-05-02 16:19:24 [INFO]: Epoch 056 - training loss (MAE): 0.0795, validation MSE: -0.3772
2025-05-02 16:19:39 [INFO]: Epoch 057 - training loss (MAE): 0.0783, validation MSE: -0.4067
2025-05-02 16:19:55 [INFO]: Epoch 058 - training loss (MAE): 0.0772, validation MSE: -0.4094
2025-05-02 16:20:10 [INFO]: Epoch 059 - training loss (MAE): 0.0778, validation MSE: -0.4082
2025-05-02 16:20:25 [INFO]: Epoch 060 - training loss (MAE): 0.0776, validation MSE: -0.3780
2025-05-02 16:20:40 [INFO]: Epoch 061 - training loss (MAE): 0.0771, validation MSE: -0.4098
2025-05-02 16:20:55 [INFO]: Epoch 062 - training loss (MAE): 0.0773, validation MSE: -0.4092
2025-05-02 16:21:11 [INFO]: Epoch 063 - training loss (MAE): 0.0760, validation MSE: -0.4104
2025-05-02 16:21:26 [INFO]: Epoch 064 - training loss (MAE): 0.0767, validation MSE: -0.4196
2025-05-02 16:21:41 [INFO]: Epoch 065 - training loss (MAE): 0.0772, validation MSE: -0.4076
2025-05-02 16:21:57 [INFO]: Epoch 066 - training loss (MAE): 0.0750, validation MSE: -0.3753
2025-05-02 16:22:12 [INFO]: Epoch 067 - training loss (MAE): 0.0751, validation MSE: -0.3906
2025-05-02 16:22:27 [INFO]: Epoch 068 - training loss (MAE): 0.0756, validation MSE: -0.4008
2025-05-02 16:22:42 [INFO]: Epoch 069 - training loss (MAE): 0.0743, validation MSE: -0.4025
2025-05-02 16:22:58 [INFO]: Epoch 070 - training loss (MAE): 0.0744, validation MSE: -0.4032
2025-05-02 16:23:13 [INFO]: Epoch 071 - training loss (MAE): 0.0737, validation MSE: -0.3993
2025-05-02 16:23:28 [INFO]: Epoch 072 - training loss (MAE): 0.0742, validation MSE: -0.4209
2025-05-02 16:23:43 [INFO]: Epoch 073 - training loss (MAE): 0.0741, validation MSE: -0.4105
2025-05-02 16:23:59 [INFO]: Epoch 074 - training loss (MAE): 0.0738, validation MSE: -0.4186
2025-05-02 16:24:14 [INFO]: Epoch 075 - training loss (MAE): 0.0733, validation MSE: -0.4081
2025-05-02 16:24:29 [INFO]: Epoch 076 - training loss (MAE): 0.0728, validation MSE: -0.4023
2025-05-02 16:24:44 [INFO]: Epoch 077 - training loss (MAE): 0.0722, validation MSE: -0.4081
2025-05-02 16:24:59 [INFO]: Epoch 078 - training loss (MAE): 0.0725, validation MSE: -0.4138
2025-05-02 16:25:15 [INFO]: Epoch 079 - training loss (MAE): 0.0723, validation MSE: -0.3863
2025-05-02 16:25:30 [INFO]: Epoch 080 - training loss (MAE): 0.0709, validation MSE: -0.4235
2025-05-02 16:25:45 [INFO]: Epoch 081 - training loss (MAE): 0.0722, validation MSE: -0.4132
2025-05-02 16:26:00 [INFO]: Epoch 082 - training loss (MAE): 0.0717, validation MSE: -0.4081
2025-05-02 16:26:15 [INFO]: Epoch 083 - training loss (MAE): 0.0716, validation MSE: -0.4071
2025-05-02 16:26:30 [INFO]: Epoch 084 - training loss (MAE): 0.0703, validation MSE: -0.4087
2025-05-02 16:26:46 [INFO]: Epoch 085 - training loss (MAE): 0.0712, validation MSE: -0.3849
2025-05-02 16:27:01 [INFO]: Epoch 086 - training loss (MAE): 0.0703, validation MSE: -0.4099
2025-05-02 16:27:16 [INFO]: Epoch 087 - training loss (MAE): 0.0710, validation MSE: -0.3763
2025-05-02 16:27:31 [INFO]: Epoch 088 - training loss (MAE): 0.0711, validation MSE: -0.4093
2025-05-02 16:27:46 [INFO]: Epoch 089 - training loss (MAE): 0.0699, validation MSE: -0.3944
2025-05-02 16:28:02 [INFO]: Epoch 090 - training loss (MAE): 0.0705, validation MSE: -0.4252
2025-05-02 16:28:17 [INFO]: Epoch 091 - training loss (MAE): 0.0696, validation MSE: -0.4117
2025-05-02 16:28:32 [INFO]: Epoch 092 - training loss (MAE): 0.0702, validation MSE: -0.3930
2025-05-02 16:28:47 [INFO]: Epoch 093 - training loss (MAE): 0.0699, validation MSE: -0.4063
2025-05-02 16:29:02 [INFO]: Epoch 094 - training loss (MAE): 0.0698, validation MSE: -0.4121
2025-05-02 16:29:18 [INFO]: Epoch 095 - training loss (MAE): 0.0693, validation MSE: -0.4074
2025-05-02 16:29:33 [INFO]: Epoch 096 - training loss (MAE): 0.0694, validation MSE: -0.3629
2025-05-02 16:29:48 [INFO]: Epoch 097 - training loss (MAE): 0.0711, validation MSE: -0.3449
2025-05-02 16:30:04 [INFO]: Epoch 098 - training loss (MAE): 0.0681, validation MSE: -0.3935
2025-05-02 16:30:19 [INFO]: Epoch 099 - training loss (MAE): 0.0689, validation MSE: -0.4119
2025-05-02 16:30:34 [INFO]: Epoch 100 - training loss (MAE): 0.0682, validation MSE: -0.3504
2025-05-02 16:30:49 [INFO]: Epoch 101 - training loss (MAE): 0.0686, validation MSE: -0.3879
2025-05-02 16:31:04 [INFO]: Epoch 102 - training loss (MAE): 0.0678, validation MSE: -0.4163
2025-05-02 16:31:20 [INFO]: Epoch 103 - training loss (MAE): 0.0679, validation MSE: -0.4032
2025-05-02 16:31:35 [INFO]: Epoch 104 - training loss (MAE): 0.0680, validation MSE: -0.3999
2025-05-02 16:31:50 [INFO]: Epoch 105 - training loss (MAE): 0.0681, validation MSE: -0.4163
2025-05-02 16:32:05 [INFO]: Epoch 106 - training loss (MAE): 0.0675, validation MSE: -0.3940
2025-05-02 16:32:20 [INFO]: Epoch 107 - training loss (MAE): 0.0676, validation MSE: -0.4233
2025-05-02 16:32:36 [INFO]: Epoch 108 - training loss (MAE): 0.0677, validation MSE: -0.4046
2025-05-02 16:32:51 [INFO]: Epoch 109 - training loss (MAE): 0.0673, validation MSE: -0.4099
2025-05-02 16:33:06 [INFO]: Epoch 110 - training loss (MAE): 0.0675, validation MSE: -0.3780
2025-05-02 16:33:21 [INFO]: Epoch 111 - training loss (MAE): 0.0677, validation MSE: -0.3747
2025-05-02 16:33:36 [INFO]: Epoch 112 - training loss (MAE): 0.0670, validation MSE: -0.3958
2025-05-02 16:33:52 [INFO]: Epoch 113 - training loss (MAE): 0.0664, validation MSE: -0.4150
2025-05-02 16:34:07 [INFO]: Epoch 114 - training loss (MAE): 0.0670, validation MSE: -0.4103
2025-05-02 16:34:22 [INFO]: Epoch 115 - training loss (MAE): 0.0667, validation MSE: -0.4062
2025-05-02 16:34:37 [INFO]: Epoch 116 - training loss (MAE): 0.0664, validation MSE: -0.3872
2025-05-02 16:34:53 [INFO]: Epoch 117 - training loss (MAE): 0.0661, validation MSE: -0.3869
2025-05-02 16:35:08 [INFO]: Epoch 118 - training loss (MAE): 0.0659, validation MSE: -0.4024
2025-05-02 16:35:23 [INFO]: Epoch 119 - training loss (MAE): 0.0666, validation MSE: -0.3904
2025-05-02 16:35:38 [INFO]: Epoch 120 - training loss (MAE): 0.0665, validation MSE: -0.4199
2025-05-02 16:35:53 [INFO]: Epoch 121 - training loss (MAE): 0.0669, validation MSE: -0.4170
2025-05-02 16:36:09 [INFO]: Epoch 122 - training loss (MAE): 0.0663, validation MSE: -0.3707
2025-05-02 16:36:24 [INFO]: Epoch 123 - training loss (MAE): 0.0653, validation MSE: -0.3490
2025-05-02 16:36:39 [INFO]: Epoch 124 - training loss (MAE): 0.0657, validation MSE: -0.4095
2025-05-02 16:36:54 [INFO]: Epoch 125 - training loss (MAE): 0.0657, validation MSE: -0.3959
2025-05-02 16:37:09 [INFO]: Epoch 126 - training loss (MAE): 0.0656, validation MSE: -0.3643
2025-05-02 16:37:25 [INFO]: Epoch 127 - training loss (MAE): 0.0657, validation MSE: -0.3999
2025-05-02 16:37:40 [INFO]: Epoch 128 - training loss (MAE): 0.0651, validation MSE: -0.4031
2025-05-02 16:37:56 [INFO]: Epoch 129 - training loss (MAE): 0.0654, validation MSE: -0.4251
2025-05-02 16:38:11 [INFO]: Epoch 130 - training loss (MAE): 0.0660, validation MSE: -0.4150
2025-05-02 16:38:26 [INFO]: Epoch 131 - training loss (MAE): 0.0661, validation MSE: -0.3687
2025-05-02 16:38:42 [INFO]: Epoch 132 - training loss (MAE): 0.0651, validation MSE: -0.4054
2025-05-02 16:38:57 [INFO]: Epoch 133 - training loss (MAE): 0.0649, validation MSE: -0.4084
2025-05-02 16:39:12 [INFO]: Epoch 134 - training loss (MAE): 0.0646, validation MSE: -0.4135
2025-05-02 16:39:27 [INFO]: Epoch 135 - training loss (MAE): 0.0647, validation MSE: -0.4243
2025-05-02 16:39:43 [INFO]: Epoch 136 - training loss (MAE): 0.0651, validation MSE: -0.3958
2025-05-02 16:39:58 [INFO]: Epoch 137 - training loss (MAE): 0.0642, validation MSE: -0.3911
2025-05-02 16:40:13 [INFO]: Epoch 138 - training loss (MAE): 0.0650, validation MSE: -0.3819
2025-05-02 16:40:29 [INFO]: Epoch 139 - training loss (MAE): 0.0644, validation MSE: -0.4127
2025-05-02 16:40:44 [INFO]: Epoch 140 - training loss (MAE): 0.0648, validation MSE: -0.4175
2025-05-02 16:40:59 [INFO]: Epoch 141 - training loss (MAE): 0.0641, validation MSE: -0.4067
2025-05-02 16:41:14 [INFO]: Epoch 142 - training loss (MAE): 0.0643, validation MSE: -0.3876
2025-05-02 16:41:30 [INFO]: Epoch 143 - training loss (MAE): 0.0647, validation MSE: -0.3936
2025-05-02 16:41:45 [INFO]: Epoch 144 - training loss (MAE): 0.0637, validation MSE: -0.4113
2025-05-02 16:42:01 [INFO]: Epoch 145 - training loss (MAE): 0.0643, validation MSE: -0.4213
2025-05-02 16:42:16 [INFO]: Epoch 146 - training loss (MAE): 0.0649, validation MSE: -0.4284
2025-05-02 16:42:31 [INFO]: Epoch 147 - training loss (MAE): 0.0639, validation MSE: -0.4101
2025-05-02 16:42:46 [INFO]: Epoch 148 - training loss (MAE): 0.0639, validation MSE: -0.3799
2025-05-02 16:43:01 [INFO]: Epoch 149 - training loss (MAE): 0.0645, validation MSE: -0.4208
2025-05-02 16:43:17 [INFO]: Epoch 150 - training loss (MAE): 0.0644, validation MSE: -0.4082
2025-05-02 16:43:32 [INFO]: Epoch 151 - training loss (MAE): 0.0635, validation MSE: -0.3646
2025-05-02 16:43:47 [INFO]: Epoch 152 - training loss (MAE): 0.0647, validation MSE: -0.4227
2025-05-02 16:44:02 [INFO]: Epoch 153 - training loss (MAE): 0.0630, validation MSE: -0.4139
2025-05-02 16:44:18 [INFO]: Epoch 154 - training loss (MAE): 0.0635, validation MSE: -0.2964
2025-05-02 16:44:33 [INFO]: Epoch 155 - training loss (MAE): 0.0631, validation MSE: -0.4083
2025-05-02 16:44:49 [INFO]: Epoch 156 - training loss (MAE): 0.0632, validation MSE: -0.2881
2025-05-02 16:45:04 [INFO]: Epoch 157 - training loss (MAE): 0.0634, validation MSE: -0.3894
2025-05-02 16:45:19 [INFO]: Epoch 158 - training loss (MAE): 0.0630, validation MSE: -0.4142
2025-05-02 16:45:34 [INFO]: Epoch 159 - training loss (MAE): 0.0634, validation MSE: -0.4171
2025-05-02 16:45:49 [INFO]: Epoch 160 - training loss (MAE): 0.0630, validation MSE: -0.3905
2025-05-02 16:46:05 [INFO]: Epoch 161 - training loss (MAE): 0.0637, validation MSE: -0.3757
2025-05-02 16:46:20 [INFO]: Epoch 162 - training loss (MAE): 0.0625, validation MSE: -0.4152
2025-05-02 16:46:36 [INFO]: Epoch 163 - training loss (MAE): 0.0634, validation MSE: -0.3681
2025-05-02 16:46:51 [INFO]: Epoch 164 - training loss (MAE): 0.0630, validation MSE: -0.3883
2025-05-02 16:47:06 [INFO]: Epoch 165 - training loss (MAE): 0.0628, validation MSE: -0.3751
2025-05-02 16:47:22 [INFO]: Epoch 166 - training loss (MAE): 0.0628, validation MSE: -0.3811
2025-05-02 16:47:37 [INFO]: Epoch 167 - training loss (MAE): 0.0629, validation MSE: -0.4083
2025-05-02 16:47:52 [INFO]: Epoch 168 - training loss (MAE): 0.0634, validation MSE: -0.3447
2025-05-02 16:48:07 [INFO]: Epoch 169 - training loss (MAE): 0.0629, validation MSE: -0.3456
2025-05-02 16:48:22 [INFO]: Epoch 170 - training loss (MAE): 0.0626, validation MSE: -0.3935
2025-05-02 16:48:38 [INFO]: Epoch 171 - training loss (MAE): 0.0626, validation MSE: -0.3930
2025-05-02 16:48:53 [INFO]: Epoch 172 - training loss (MAE): 0.0624, validation MSE: -0.3890
2025-05-02 16:49:08 [INFO]: Epoch 173 - training loss (MAE): 0.0620, validation MSE: -0.3995
2025-05-02 16:49:24 [INFO]: Epoch 174 - training loss (MAE): 0.0620, validation MSE: -0.3766
2025-05-02 16:49:39 [INFO]: Epoch 175 - training loss (MAE): 0.0635, validation MSE: -0.3166
2025-05-02 16:49:54 [INFO]: Epoch 176 - training loss (MAE): 0.0619, validation MSE: -0.4263
2025-05-02 16:50:10 [INFO]: Epoch 177 - training loss (MAE): 0.0612, validation MSE: -0.3840
2025-05-02 16:50:25 [INFO]: Epoch 178 - training loss (MAE): 0.0618, validation MSE: -0.4208
2025-05-02 16:50:40 [INFO]: Epoch 179 - training loss (MAE): 0.0628, validation MSE: -0.4081
2025-05-02 16:50:55 [INFO]: Epoch 180 - training loss (MAE): 0.0626, validation MSE: -0.3839
2025-05-02 16:51:11 [INFO]: Epoch 181 - training loss (MAE): 0.0625, validation MSE: -0.3457
2025-05-02 16:51:26 [INFO]: Epoch 182 - training loss (MAE): 0.0616, validation MSE: -0.4214
2025-05-02 16:51:41 [INFO]: Epoch 183 - training loss (MAE): 0.0617, validation MSE: -0.3625
2025-05-02 16:51:57 [INFO]: Epoch 184 - training loss (MAE): 0.0618, validation MSE: -0.4082
2025-05-02 16:52:12 [INFO]: Epoch 185 - training loss (MAE): 0.0617, validation MSE: -0.4046
2025-05-02 16:52:27 [INFO]: Epoch 186 - training loss (MAE): 0.0616, validation MSE: -0.3899
2025-05-02 16:52:43 [INFO]: Epoch 187 - training loss (MAE): 0.0613, validation MSE: -0.3140
2025-05-02 16:52:58 [INFO]: Epoch 188 - training loss (MAE): 0.0616, validation MSE: -0.3579
2025-05-02 16:53:13 [INFO]: Epoch 189 - training loss (MAE): 0.0619, validation MSE: -0.3741
2025-05-02 16:53:29 [INFO]: Epoch 190 - training loss (MAE): 0.0614, validation MSE: -0.3730
2025-05-02 16:53:44 [INFO]: Epoch 191 - training loss (MAE): 0.0617, validation MSE: -0.3763
2025-05-02 16:53:59 [INFO]: Epoch 192 - training loss (MAE): 0.0616, validation MSE: -0.3884
2025-05-02 16:54:14 [INFO]: Epoch 193 - training loss (MAE): 0.0617, validation MSE: -0.3675
2025-05-02 16:54:29 [INFO]: Epoch 194 - training loss (MAE): 0.0611, validation MSE: -0.3381
2025-05-02 16:54:45 [INFO]: Epoch 195 - training loss (MAE): 0.0610, validation MSE: -0.4033
2025-05-02 16:55:00 [INFO]: Epoch 196 - training loss (MAE): 0.0616, validation MSE: -0.3410
2025-05-02 16:55:15 [INFO]: Epoch 197 - training loss (MAE): 0.0611, validation MSE: -0.3039
2025-05-02 16:55:31 [INFO]: Epoch 198 - training loss (MAE): 0.0607, validation MSE: -0.3944
2025-05-02 16:55:46 [INFO]: Epoch 199 - training loss (MAE): 0.0607, validation MSE: -0.4018
2025-05-02 16:56:01 [INFO]: Epoch 200 - training loss (MAE): 0.0607, validation MSE: -0.3642
2025-05-02 16:56:16 [INFO]: Epoch 201 - training loss (MAE): 0.0627, validation MSE: -0.3436
2025-05-02 16:56:32 [INFO]: Epoch 202 - training loss (MAE): 0.0607, validation MSE: -0.3241
2025-05-02 16:56:47 [INFO]: Epoch 203 - training loss (MAE): 0.0610, validation MSE: -0.3653
2025-05-02 16:57:02 [INFO]: Epoch 204 - training loss (MAE): 0.0610, validation MSE: -0.3279
2025-05-02 16:57:17 [INFO]: Epoch 205 - training loss (MAE): 0.0608, validation MSE: -0.4185
2025-05-02 16:57:33 [INFO]: Epoch 206 - training loss (MAE): 0.0605, validation MSE: -0.4340
2025-05-02 16:57:48 [INFO]: Epoch 207 - training loss (MAE): 0.0605, validation MSE: -0.3687
2025-05-02 16:58:03 [INFO]: Epoch 208 - training loss (MAE): 0.0605, validation MSE: -0.3796
2025-05-02 16:58:19 [INFO]: Epoch 209 - training loss (MAE): 0.0604, validation MSE: -0.3759
2025-05-02 16:58:34 [INFO]: Epoch 210 - training loss (MAE): 0.0608, validation MSE: -0.3127
2025-05-02 16:58:50 [INFO]: Epoch 211 - training loss (MAE): 0.0603, validation MSE: -0.3798
2025-05-02 16:59:05 [INFO]: Epoch 212 - training loss (MAE): 0.0603, validation MSE: -0.4042
2025-05-02 16:59:20 [INFO]: Epoch 213 - training loss (MAE): 0.0604, validation MSE: -0.3645
2025-05-02 16:59:36 [INFO]: Epoch 214 - training loss (MAE): 0.0603, validation MSE: -0.3289
2025-05-02 16:59:51 [INFO]: Epoch 215 - training loss (MAE): 0.0599, validation MSE: -0.3852
2025-05-02 17:00:06 [INFO]: Epoch 216 - training loss (MAE): 0.0599, validation MSE: -0.3636
2025-05-02 17:00:21 [INFO]: Epoch 217 - training loss (MAE): 0.0607, validation MSE: -0.4312
2025-05-02 17:00:36 [INFO]: Epoch 218 - training loss (MAE): 0.0606, validation MSE: -0.4256
2025-05-02 17:00:52 [INFO]: Epoch 219 - training loss (MAE): 0.0609, validation MSE: -0.3557
2025-05-02 17:01:07 [INFO]: Epoch 220 - training loss (MAE): 0.0598, validation MSE: -0.3478
2025-05-02 17:01:22 [INFO]: Epoch 221 - training loss (MAE): 0.0602, validation MSE: -0.3824
2025-05-02 17:01:37 [INFO]: Epoch 222 - training loss (MAE): 0.0596, validation MSE: -0.4059
2025-05-02 17:01:53 [INFO]: Epoch 223 - training loss (MAE): 0.0605, validation MSE: -0.3889
2025-05-02 17:02:08 [INFO]: Epoch 224 - training loss (MAE): 0.0599, validation MSE: -0.4047
2025-05-02 17:02:23 [INFO]: Epoch 225 - training loss (MAE): 0.0598, validation MSE: -0.4044
2025-05-02 17:02:38 [INFO]: Epoch 226 - training loss (MAE): 0.0606, validation MSE: -0.3920
2025-05-02 17:02:54 [INFO]: Epoch 227 - training loss (MAE): 0.0599, validation MSE: -0.3803
2025-05-02 17:03:09 [INFO]: Epoch 228 - training loss (MAE): 0.0596, validation MSE: -0.3772
2025-05-02 17:03:24 [INFO]: Epoch 229 - training loss (MAE): 0.0592, validation MSE: -0.3379
2025-05-02 17:03:39 [INFO]: Epoch 230 - training loss (MAE): 0.0602, validation MSE: -0.3679
2025-05-02 17:03:55 [INFO]: Epoch 231 - training loss (MAE): 0.0595, validation MSE: -0.4089
2025-05-02 17:04:10 [INFO]: Epoch 232 - training loss (MAE): 0.0599, validation MSE: -0.4113
2025-05-02 17:04:25 [INFO]: Epoch 233 - training loss (MAE): 0.0597, validation MSE: -0.3344
2025-05-02 17:04:41 [INFO]: Epoch 234 - training loss (MAE): 0.0593, validation MSE: -0.4291
2025-05-02 17:04:56 [INFO]: Epoch 235 - training loss (MAE): 0.0598, validation MSE: -0.3602
2025-05-02 17:05:11 [INFO]: Epoch 236 - training loss (MAE): 0.0595, validation MSE: -0.3332
2025-05-02 17:05:26 [INFO]: Epoch 237 - training loss (MAE): 0.0591, validation MSE: -0.4145
2025-05-02 17:05:42 [INFO]: Epoch 238 - training loss (MAE): 0.0588, validation MSE: -0.3206
2025-05-02 17:05:57 [INFO]: Epoch 239 - training loss (MAE): 0.0596, validation MSE: -0.3717
2025-05-02 17:06:12 [INFO]: Epoch 240 - training loss (MAE): 0.0597, validation MSE: -0.3555
2025-05-02 17:06:27 [INFO]: Epoch 241 - training loss (MAE): 0.0591, validation MSE: -0.4165
2025-05-02 17:06:42 [INFO]: Epoch 242 - training loss (MAE): 0.0592, validation MSE: -0.4293
2025-05-02 17:06:58 [INFO]: Epoch 243 - training loss (MAE): 0.0592, validation MSE: -0.4029
2025-05-02 17:07:13 [INFO]: Epoch 244 - training loss (MAE): 0.0590, validation MSE: -0.3703
2025-05-02 17:07:28 [INFO]: Epoch 245 - training loss (MAE): 0.0592, validation MSE: -0.3962
2025-05-02 17:07:43 [INFO]: Epoch 246 - training loss (MAE): 0.0599, validation MSE: -0.4154
2025-05-02 17:07:59 [INFO]: Epoch 247 - training loss (MAE): 0.0592, validation MSE: -0.4075
2025-05-02 17:08:14 [INFO]: Epoch 248 - training loss (MAE): 0.0597, validation MSE: -0.3515
2025-05-02 17:08:29 [INFO]: Epoch 249 - training loss (MAE): 0.0595, validation MSE: -0.3381
2025-05-02 17:08:44 [INFO]: Epoch 250 - training loss (MAE): 0.0590, validation MSE: -0.4019
2025-05-02 17:08:59 [INFO]: Epoch 251 - training loss (MAE): 0.0590, validation MSE: -0.4007
2025-05-02 17:09:15 [INFO]: Epoch 252 - training loss (MAE): 0.0584, validation MSE: -0.4289
2025-05-02 17:09:30 [INFO]: Epoch 253 - training loss (MAE): 0.0589, validation MSE: -0.3881
2025-05-02 17:09:45 [INFO]: Epoch 254 - training loss (MAE): 0.0585, validation MSE: -0.3531
2025-05-02 17:10:00 [INFO]: Epoch 255 - training loss (MAE): 0.0591, validation MSE: -0.3947
2025-05-02 17:10:15 [INFO]: Epoch 256 - training loss (MAE): 0.0596, validation MSE: -0.3939
2025-05-02 17:10:31 [INFO]: Epoch 257 - training loss (MAE): 0.0581, validation MSE: -0.3854
2025-05-02 17:10:46 [INFO]: Epoch 258 - training loss (MAE): 0.0591, validation MSE: -0.3715
2025-05-02 17:11:01 [INFO]: Epoch 259 - training loss (MAE): 0.0588, validation MSE: -0.4214
2025-05-02 17:11:16 [INFO]: Epoch 260 - training loss (MAE): 0.0589, validation MSE: -0.2996
2025-05-02 17:11:31 [INFO]: Epoch 261 - training loss (MAE): 0.0592, validation MSE: -0.4043
2025-05-02 17:11:47 [INFO]: Epoch 262 - training loss (MAE): 0.0593, validation MSE: -0.4278
2025-05-02 17:12:02 [INFO]: Epoch 263 - training loss (MAE): 0.0586, validation MSE: -0.3556
2025-05-02 17:12:17 [INFO]: Epoch 264 - training loss (MAE): 0.0600, validation MSE: -0.3683
2025-05-02 17:12:32 [INFO]: Epoch 265 - training loss (MAE): 0.0585, validation MSE: -0.4326
2025-05-02 17:12:47 [INFO]: Epoch 266 - training loss (MAE): 0.0586, validation MSE: -0.2969
2025-05-02 17:13:02 [INFO]: Epoch 267 - training loss (MAE): 0.0583, validation MSE: -0.4313
2025-05-02 17:13:18 [INFO]: Epoch 268 - training loss (MAE): 0.0582, validation MSE: -0.3944
2025-05-02 17:13:33 [INFO]: Epoch 269 - training loss (MAE): 0.0580, validation MSE: -0.4124
2025-05-02 17:13:48 [INFO]: Epoch 270 - training loss (MAE): 0.0578, validation MSE: -0.3827
2025-05-02 17:14:04 [INFO]: Epoch 271 - training loss (MAE): 0.0579, validation MSE: -0.4174
2025-05-02 17:14:19 [INFO]: Epoch 272 - training loss (MAE): 0.0585, validation MSE: -0.3354
2025-05-02 17:14:34 [INFO]: Epoch 273 - training loss (MAE): 0.0593, validation MSE: -0.3793
2025-05-02 17:14:49 [INFO]: Epoch 274 - training loss (MAE): 0.0585, validation MSE: -0.3915
2025-05-02 17:15:05 [INFO]: Epoch 275 - training loss (MAE): 0.0580, validation MSE: -0.3633
2025-05-02 17:15:20 [INFO]: Epoch 276 - training loss (MAE): 0.0580, validation MSE: -0.3316
2025-05-02 17:15:35 [INFO]: Epoch 277 - training loss (MAE): 0.0581, validation MSE: -0.4362
2025-05-02 17:15:50 [INFO]: Epoch 278 - training loss (MAE): 0.0588, validation MSE: -0.3730
2025-05-02 17:16:06 [INFO]: Epoch 279 - training loss (MAE): 0.0579, validation MSE: -0.4176
2025-05-02 17:16:21 [INFO]: Epoch 280 - training loss (MAE): 0.0586, validation MSE: -0.3660
2025-05-02 17:16:36 [INFO]: Epoch 281 - training loss (MAE): 0.0590, validation MSE: -0.3978
2025-05-02 17:16:52 [INFO]: Epoch 282 - training loss (MAE): 0.0581, validation MSE: -0.4321
2025-05-02 17:17:07 [INFO]: Epoch 283 - training loss (MAE): 0.0581, validation MSE: -0.3037
2025-05-02 17:17:22 [INFO]: Epoch 284 - training loss (MAE): 0.0578, validation MSE: -0.2749
2025-05-02 17:17:37 [INFO]: Epoch 285 - training loss (MAE): 0.0576, validation MSE: -0.4169
2025-05-02 17:17:53 [INFO]: Epoch 286 - training loss (MAE): 0.0579, validation MSE: -0.3957
2025-05-02 17:18:08 [INFO]: Epoch 287 - training loss (MAE): 0.0580, validation MSE: -0.3851
2025-05-02 17:18:23 [INFO]: Epoch 288 - training loss (MAE): 0.0577, validation MSE: -0.4282
2025-05-02 17:18:38 [INFO]: Epoch 289 - training loss (MAE): 0.0580, validation MSE: -0.4040
2025-05-02 17:18:53 [INFO]: Epoch 290 - training loss (MAE): 0.0582, validation MSE: -0.4203
2025-05-02 17:19:09 [INFO]: Epoch 291 - training loss (MAE): 0.0578, validation MSE: -0.3279
2025-05-02 17:19:24 [INFO]: Epoch 292 - training loss (MAE): 0.0581, validation MSE: -0.3605
2025-05-02 17:19:39 [INFO]: Epoch 293 - training loss (MAE): 0.0574, validation MSE: -0.3804
2025-05-02 17:19:55 [INFO]: Epoch 294 - training loss (MAE): 0.0577, validation MSE: -0.2609
2025-05-02 17:20:10 [INFO]: Epoch 295 - training loss (MAE): 0.0576, validation MSE: -0.2929
2025-05-02 17:20:25 [INFO]: Epoch 296 - training loss (MAE): 0.0578, validation MSE: -0.3356
2025-05-02 17:20:40 [INFO]: Epoch 297 - training loss (MAE): 0.0581, validation MSE: -0.4205
2025-05-02 17:20:56 [INFO]: Epoch 298 - training loss (MAE): 0.0572, validation MSE: -0.4401
2025-05-02 17:21:11 [INFO]: Epoch 299 - training loss (MAE): 0.0577, validation MSE: -0.4236
2025-05-02 17:21:26 [INFO]: Epoch 300 - training loss (MAE): 0.0572, validation MSE: -0.4326
2025-05-02 17:21:41 [INFO]: Epoch 301 - training loss (MAE): 0.0574, validation MSE: -0.2995
2025-05-02 17:21:56 [INFO]: Epoch 302 - training loss (MAE): 0.0581, validation MSE: -0.3469
2025-05-02 17:22:12 [INFO]: Epoch 303 - training loss (MAE): 0.0577, validation MSE: -0.3881
2025-05-02 17:22:27 [INFO]: Epoch 304 - training loss (MAE): 0.0572, validation MSE: -0.3862
2025-05-02 17:22:43 [INFO]: Epoch 305 - training loss (MAE): 0.0575, validation MSE: -0.4179
2025-05-02 17:22:58 [INFO]: Epoch 306 - training loss (MAE): 0.0570, validation MSE: -0.3919
2025-05-02 17:23:13 [INFO]: Epoch 307 - training loss (MAE): 0.0579, validation MSE: -0.4228
2025-05-02 17:23:28 [INFO]: Epoch 308 - training loss (MAE): 0.0576, validation MSE: -0.4186
2025-05-02 17:23:44 [INFO]: Epoch 309 - training loss (MAE): 0.0575, validation MSE: -0.3835
2025-05-02 17:23:59 [INFO]: Epoch 310 - training loss (MAE): 0.0575, validation MSE: -0.3947
2025-05-02 17:24:14 [INFO]: Epoch 311 - training loss (MAE): 0.0572, validation MSE: -0.3595
2025-05-02 17:24:29 [INFO]: Epoch 312 - training loss (MAE): 0.0570, validation MSE: -0.3130
2025-05-02 17:24:45 [INFO]: Epoch 313 - training loss (MAE): 0.0571, validation MSE: -0.2886
2025-05-02 17:25:00 [INFO]: Epoch 314 - training loss (MAE): 0.0574, validation MSE: -0.3763
2025-05-02 17:25:15 [INFO]: Epoch 315 - training loss (MAE): 0.0575, validation MSE: -0.3600
2025-05-02 17:25:30 [INFO]: Epoch 316 - training loss (MAE): 0.0569, validation MSE: -0.3973
2025-05-02 17:25:45 [INFO]: Epoch 317 - training loss (MAE): 0.0575, validation MSE: -0.4038
2025-05-02 17:26:01 [INFO]: Epoch 318 - training loss (MAE): 0.0575, validation MSE: -0.3803
2025-05-02 17:26:16 [INFO]: Epoch 319 - training loss (MAE): 0.0569, validation MSE: -0.4229
2025-05-02 17:26:31 [INFO]: Epoch 320 - training loss (MAE): 0.0577, validation MSE: -0.3920
2025-05-02 17:26:46 [INFO]: Epoch 321 - training loss (MAE): 0.0570, validation MSE: -0.4351
2025-05-02 17:27:02 [INFO]: Epoch 322 - training loss (MAE): 0.0576, validation MSE: -0.3584
2025-05-02 17:27:17 [INFO]: Epoch 323 - training loss (MAE): 0.0564, validation MSE: -0.4149
2025-05-02 17:27:32 [INFO]: Epoch 324 - training loss (MAE): 0.0569, validation MSE: -0.4092
2025-05-02 17:27:47 [INFO]: Epoch 325 - training loss (MAE): 0.0571, validation MSE: -0.4410
2025-05-02 17:28:03 [INFO]: Epoch 326 - training loss (MAE): 0.0569, validation MSE: -0.4206
2025-05-02 17:28:18 [INFO]: Epoch 327 - training loss (MAE): 0.0564, validation MSE: -0.3976
2025-05-02 17:28:33 [INFO]: Epoch 328 - training loss (MAE): 0.0564, validation MSE: -0.3734
2025-05-02 17:28:48 [INFO]: Epoch 329 - training loss (MAE): 0.0562, validation MSE: -0.4229
2025-05-02 17:29:03 [INFO]: Epoch 330 - training loss (MAE): 0.0569, validation MSE: -0.4378
2025-05-02 17:29:18 [INFO]: Epoch 331 - training loss (MAE): 0.0566, validation MSE: -0.3764
2025-05-02 17:29:34 [INFO]: Epoch 332 - training loss (MAE): 0.0573, validation MSE: -0.3984
2025-05-02 17:29:49 [INFO]: Epoch 333 - training loss (MAE): 0.0576, validation MSE: -0.4294
2025-05-02 17:30:04 [INFO]: Epoch 334 - training loss (MAE): 0.0562, validation MSE: -0.3906
2025-05-02 17:30:19 [INFO]: Epoch 335 - training loss (MAE): 0.0565, validation MSE: -0.3966
2025-05-02 17:30:35 [INFO]: Epoch 336 - training loss (MAE): 0.0566, validation MSE: -0.3367
2025-05-02 17:30:50 [INFO]: Epoch 337 - training loss (MAE): 0.0567, validation MSE: -0.3326
2025-05-02 17:31:05 [INFO]: Epoch 338 - training loss (MAE): 0.0567, validation MSE: -0.3614
2025-05-02 17:31:20 [INFO]: Epoch 339 - training loss (MAE): 0.0561, validation MSE: -0.3887
2025-05-02 17:31:36 [INFO]: Epoch 340 - training loss (MAE): 0.0567, validation MSE: -0.2653
2025-05-02 17:31:51 [INFO]: Epoch 341 - training loss (MAE): 0.0563, validation MSE: -0.3534
2025-05-02 17:32:06 [INFO]: Epoch 342 - training loss (MAE): 0.0563, validation MSE: -0.4153
2025-05-02 17:32:21 [INFO]: Epoch 343 - training loss (MAE): 0.0563, validation MSE: -0.4035
2025-05-02 17:32:37 [INFO]: Epoch 344 - training loss (MAE): 0.0566, validation MSE: -0.3849
2025-05-02 17:32:52 [INFO]: Epoch 345 - training loss (MAE): 0.0562, validation MSE: -0.3960
2025-05-02 17:33:07 [INFO]: Epoch 346 - training loss (MAE): 0.0569, validation MSE: -0.4006
2025-05-02 17:33:22 [INFO]: Epoch 347 - training loss (MAE): 0.0561, validation MSE: -0.3410
2025-05-02 17:33:37 [INFO]: Epoch 348 - training loss (MAE): 0.0562, validation MSE: -0.4295
2025-05-02 17:33:53 [INFO]: Epoch 349 - training loss (MAE): 0.0561, validation MSE: -0.3648
2025-05-02 17:34:08 [INFO]: Epoch 350 - training loss (MAE): 0.0562, validation MSE: -0.4192
2025-05-02 17:34:23 [INFO]: Epoch 351 - training loss (MAE): 0.0560, validation MSE: -0.3956
2025-05-02 17:34:38 [INFO]: Epoch 352 - training loss (MAE): 0.0559, validation MSE: -0.3613
2025-05-02 17:34:53 [INFO]: Epoch 353 - training loss (MAE): 0.0559, validation MSE: -0.4079
2025-05-02 17:35:09 [INFO]: Epoch 354 - training loss (MAE): 0.0558, validation MSE: -0.3717
2025-05-02 17:35:24 [INFO]: Epoch 355 - training loss (MAE): 0.0564, validation MSE: -0.3930
2025-05-02 17:35:39 [INFO]: Epoch 356 - training loss (MAE): 0.0563, validation MSE: -0.4162
2025-05-02 17:35:55 [INFO]: Epoch 357 - training loss (MAE): 0.0560, validation MSE: -0.4161
2025-05-02 17:36:10 [INFO]: Epoch 358 - training loss (MAE): 0.0558, validation MSE: -0.4181
2025-05-02 17:36:25 [INFO]: Epoch 359 - training loss (MAE): 0.0560, validation MSE: -0.4320
2025-05-02 17:36:40 [INFO]: Epoch 360 - training loss (MAE): 0.0560, validation MSE: -0.4399
2025-05-02 17:36:56 [INFO]: Epoch 361 - training loss (MAE): 0.0558, validation MSE: -0.4214
2025-05-02 17:37:11 [INFO]: Epoch 362 - training loss (MAE): 0.0563, validation MSE: -0.4097
2025-05-02 17:37:26 [INFO]: Epoch 363 - training loss (MAE): 0.0571, validation MSE: -0.3174
2025-05-02 17:37:41 [INFO]: Epoch 364 - training loss (MAE): 0.0556, validation MSE: -0.3990
2025-05-02 17:37:57 [INFO]: Epoch 365 - training loss (MAE): 0.0555, validation MSE: -0.3541
2025-05-02 17:38:12 [INFO]: Epoch 366 - training loss (MAE): 0.0558, validation MSE: -0.4069
2025-05-02 17:38:27 [INFO]: Epoch 367 - training loss (MAE): 0.0560, validation MSE: -0.4248
2025-05-02 17:38:42 [INFO]: Epoch 368 - training loss (MAE): 0.0556, validation MSE: -0.3394
2025-05-02 17:38:58 [INFO]: Epoch 369 - training loss (MAE): 0.0557, validation MSE: -0.4205
2025-05-02 17:39:13 [INFO]: Epoch 370 - training loss (MAE): 0.0561, validation MSE: -0.4124
2025-05-02 17:39:28 [INFO]: Epoch 371 - training loss (MAE): 0.0552, validation MSE: -0.3982
2025-05-02 17:39:43 [INFO]: Epoch 372 - training loss (MAE): 0.0551, validation MSE: -0.3917
2025-05-02 17:39:58 [INFO]: Epoch 373 - training loss (MAE): 0.0553, validation MSE: -0.4227
2025-05-02 17:40:14 [INFO]: Epoch 374 - training loss (MAE): 0.0560, validation MSE: -0.4186
2025-05-02 17:40:29 [INFO]: Epoch 375 - training loss (MAE): 0.0553, validation MSE: -0.4415
2025-05-02 17:40:44 [INFO]: Epoch 376 - training loss (MAE): 0.0556, validation MSE: -0.4054
2025-05-02 17:40:59 [INFO]: Epoch 377 - training loss (MAE): 0.0557, validation MSE: -0.4046
2025-05-02 17:41:15 [INFO]: Epoch 378 - training loss (MAE): 0.0551, validation MSE: -0.3713
2025-05-02 17:41:30 [INFO]: Epoch 379 - training loss (MAE): 0.0552, validation MSE: -0.3908
2025-05-02 17:41:45 [INFO]: Epoch 380 - training loss (MAE): 0.0552, validation MSE: -0.4086
2025-05-02 17:42:00 [INFO]: Epoch 381 - training loss (MAE): 0.0555, validation MSE: -0.4447
2025-05-02 17:42:15 [INFO]: Epoch 382 - training loss (MAE): 0.0550, validation MSE: -0.4221
2025-05-02 17:42:31 [INFO]: Epoch 383 - training loss (MAE): 0.0549, validation MSE: -0.4088
2025-05-02 17:42:46 [INFO]: Epoch 384 - training loss (MAE): 0.0552, validation MSE: -0.4210
2025-05-02 17:43:01 [INFO]: Epoch 385 - training loss (MAE): 0.0553, validation MSE: -0.4161
2025-05-02 17:43:16 [INFO]: Epoch 386 - training loss (MAE): 0.0552, validation MSE: -0.3903
2025-05-02 17:43:32 [INFO]: Epoch 387 - training loss (MAE): 0.0556, validation MSE: -0.4088
2025-05-02 17:43:47 [INFO]: Epoch 388 - training loss (MAE): 0.0563, validation MSE: -0.4358
2025-05-02 17:44:02 [INFO]: Epoch 389 - training loss (MAE): 0.0544, validation MSE: -0.4253
2025-05-02 17:44:17 [INFO]: Epoch 390 - training loss (MAE): 0.0556, validation MSE: -0.3133
2025-05-02 17:44:32 [INFO]: Epoch 391 - training loss (MAE): 0.0548, validation MSE: -0.4133
2025-05-02 17:44:48 [INFO]: Epoch 392 - training loss (MAE): 0.0547, validation MSE: -0.4179
2025-05-02 17:45:03 [INFO]: Epoch 393 - training loss (MAE): 0.0551, validation MSE: -0.4078
2025-05-02 17:45:18 [INFO]: Epoch 394 - training loss (MAE): 0.0547, validation MSE: -0.4238
2025-05-02 17:45:33 [INFO]: Epoch 395 - training loss (MAE): 0.0553, validation MSE: -0.3878
2025-05-02 17:45:49 [INFO]: Epoch 396 - training loss (MAE): 0.0548, validation MSE: -0.3671
2025-05-02 17:46:04 [INFO]: Epoch 397 - training loss (MAE): 0.0549, validation MSE: -0.3935
2025-05-02 17:46:19 [INFO]: Epoch 398 - training loss (MAE): 0.0547, validation MSE: -0.4341
2025-05-02 17:46:34 [INFO]: Epoch 399 - training loss (MAE): 0.0547, validation MSE: -0.3487
2025-05-02 17:46:49 [INFO]: Epoch 400 - training loss (MAE): 0.0550, validation MSE: -0.3960
2025-05-02 17:47:05 [INFO]: Epoch 401 - training loss (MAE): 0.0551, validation MSE: -0.4222
2025-05-02 17:47:20 [INFO]: Epoch 402 - training loss (MAE): 0.0546, validation MSE: -0.3110
2025-05-02 17:47:35 [INFO]: Epoch 403 - training loss (MAE): 0.0548, validation MSE: -0.4276
2025-05-02 17:47:51 [INFO]: Epoch 404 - training loss (MAE): 0.0547, validation MSE: -0.3525
2025-05-02 17:48:06 [INFO]: Epoch 405 - training loss (MAE): 0.0550, validation MSE: -0.3187
2025-05-02 17:48:21 [INFO]: Epoch 406 - training loss (MAE): 0.0548, validation MSE: -0.4132
2025-05-02 17:48:36 [INFO]: Epoch 407 - training loss (MAE): 0.0549, validation MSE: -0.4170
2025-05-02 17:48:52 [INFO]: Epoch 408 - training loss (MAE): 0.0548, validation MSE: -0.4331
2025-05-02 17:49:07 [INFO]: Epoch 409 - training loss (MAE): 0.0550, validation MSE: -0.4312
2025-05-02 17:49:22 [INFO]: Epoch 410 - training loss (MAE): 0.0547, validation MSE: -0.4544
2025-05-02 17:49:37 [INFO]: Epoch 411 - training loss (MAE): 0.0547, validation MSE: -0.1659
2025-05-02 17:49:52 [INFO]: Epoch 412 - training loss (MAE): 0.0547, validation MSE: -0.4071
2025-05-02 17:50:07 [INFO]: Epoch 413 - training loss (MAE): 0.0538, validation MSE: -0.4260
2025-05-02 17:50:22 [INFO]: Epoch 414 - training loss (MAE): 0.0542, validation MSE: -0.4028
2025-05-02 17:50:38 [INFO]: Epoch 415 - training loss (MAE): 0.0548, validation MSE: -0.4272
2025-05-02 17:50:53 [INFO]: Epoch 416 - training loss (MAE): 0.0539, validation MSE: -0.3311
2025-05-02 17:51:08 [INFO]: Epoch 417 - training loss (MAE): 0.0547, validation MSE: -0.4118
2025-05-02 17:51:23 [INFO]: Epoch 418 - training loss (MAE): 0.0543, validation MSE: -0.3526
2025-05-02 17:51:39 [INFO]: Epoch 419 - training loss (MAE): 0.0546, validation MSE: -0.3988
2025-05-02 17:51:54 [INFO]: Epoch 420 - training loss (MAE): 0.0539, validation MSE: -0.4140
2025-05-02 17:52:09 [INFO]: Epoch 421 - training loss (MAE): 0.0537, validation MSE: -0.3818
2025-05-02 17:52:24 [INFO]: Epoch 422 - training loss (MAE): 0.0541, validation MSE: -0.3387
2025-05-02 17:52:40 [INFO]: Epoch 423 - training loss (MAE): 0.0541, validation MSE: -0.4446
2025-05-02 17:52:55 [INFO]: Epoch 424 - training loss (MAE): 0.0537, validation MSE: -0.4193
2025-05-02 17:53:10 [INFO]: Epoch 425 - training loss (MAE): 0.0540, validation MSE: -0.3774
2025-05-02 17:53:26 [INFO]: Epoch 426 - training loss (MAE): 0.0543, validation MSE: -0.3946
2025-05-02 17:53:41 [INFO]: Epoch 427 - training loss (MAE): 0.0543, validation MSE: -0.4094
2025-05-02 17:53:56 [INFO]: Epoch 428 - training loss (MAE): 0.0541, validation MSE: -0.3709
2025-05-02 17:54:11 [INFO]: Epoch 429 - training loss (MAE): 0.0538, validation MSE: -0.4141
2025-05-02 17:54:27 [INFO]: Epoch 430 - training loss (MAE): 0.0548, validation MSE: -0.2986
2025-05-02 17:54:42 [INFO]: Epoch 431 - training loss (MAE): 0.0533, validation MSE: -0.4175
2025-05-02 17:54:57 [INFO]: Epoch 432 - training loss (MAE): 0.0536, validation MSE: -0.4190
2025-05-02 17:55:12 [INFO]: Epoch 433 - training loss (MAE): 0.0538, validation MSE: -0.4130
2025-05-02 17:55:27 [INFO]: Epoch 434 - training loss (MAE): 0.0536, validation MSE: -0.4295
2025-05-02 17:55:43 [INFO]: Epoch 435 - training loss (MAE): 0.0540, validation MSE: -0.4311
2025-05-02 17:55:58 [INFO]: Epoch 436 - training loss (MAE): 0.0536, validation MSE: -0.4364
2025-05-02 17:56:13 [INFO]: Epoch 437 - training loss (MAE): 0.0534, validation MSE: -0.3748
2025-05-02 17:56:28 [INFO]: Epoch 438 - training loss (MAE): 0.0538, validation MSE: -0.4204
2025-05-02 17:56:44 [INFO]: Epoch 439 - training loss (MAE): 0.0536, validation MSE: -0.4169
2025-05-02 17:56:59 [INFO]: Epoch 440 - training loss (MAE): 0.0531, validation MSE: -0.4002
2025-05-02 17:57:14 [INFO]: Epoch 441 - training loss (MAE): 0.0534, validation MSE: -0.4353
2025-05-02 17:57:29 [INFO]: Epoch 442 - training loss (MAE): 0.0537, validation MSE: -0.3189
2025-05-02 17:57:45 [INFO]: Epoch 443 - training loss (MAE): 0.0532, validation MSE: -0.4239
2025-05-02 17:58:00 [INFO]: Epoch 444 - training loss (MAE): 0.0527, validation MSE: -0.4339
2025-05-02 17:58:15 [INFO]: Epoch 445 - training loss (MAE): 0.0537, validation MSE: -0.4202
2025-05-02 17:58:30 [INFO]: Epoch 446 - training loss (MAE): 0.0547, validation MSE: -0.4181
2025-05-02 17:58:46 [INFO]: Epoch 447 - training loss (MAE): 0.0530, validation MSE: -0.4037
2025-05-02 17:59:01 [INFO]: Epoch 448 - training loss (MAE): 0.0534, validation MSE: -0.4342
2025-05-02 17:59:16 [INFO]: Epoch 449 - training loss (MAE): 0.0531, validation MSE: -0.3295
2025-05-02 17:59:31 [INFO]: Epoch 450 - training loss (MAE): 0.0541, validation MSE: -0.4307
2025-05-02 17:59:47 [INFO]: Epoch 451 - training loss (MAE): 0.0533, validation MSE: -0.4347
2025-05-02 18:00:02 [INFO]: Epoch 452 - training loss (MAE): 0.0535, validation MSE: -0.3940
2025-05-02 18:00:17 [INFO]: Epoch 453 - training loss (MAE): 0.0533, validation MSE: -0.3527
2025-05-02 18:00:33 [INFO]: Epoch 454 - training loss (MAE): 0.0533, validation MSE: -0.4264
2025-05-02 18:00:48 [INFO]: Epoch 455 - training loss (MAE): 0.0534, validation MSE: -0.4219
2025-05-02 18:01:03 [INFO]: Epoch 456 - training loss (MAE): 0.0530, validation MSE: -0.4238
2025-05-02 18:01:19 [INFO]: Epoch 457 - training loss (MAE): 0.0537, validation MSE: -0.3312
2025-05-02 18:01:34 [INFO]: Epoch 458 - training loss (MAE): 0.0533, validation MSE: -0.4120
2025-05-02 18:01:49 [INFO]: Epoch 459 - training loss (MAE): 0.0532, validation MSE: -0.4267
2025-05-02 18:02:04 [INFO]: Epoch 460 - training loss (MAE): 0.0531, validation MSE: -0.3869
2025-05-02 18:02:19 [INFO]: Epoch 461 - training loss (MAE): 0.0530, validation MSE: -0.4362
2025-05-02 18:02:34 [INFO]: Epoch 462 - training loss (MAE): 0.0529, validation MSE: -0.4067
2025-05-02 18:02:50 [INFO]: Epoch 463 - training loss (MAE): 0.0525, validation MSE: -0.4351
2025-05-02 18:03:05 [INFO]: Epoch 464 - training loss (MAE): 0.0529, validation MSE: -0.4349
2025-05-02 18:03:20 [INFO]: Epoch 465 - training loss (MAE): 0.0530, validation MSE: -0.4118
2025-05-02 18:03:35 [INFO]: Epoch 466 - training loss (MAE): 0.0525, validation MSE: -0.3940
2025-05-02 18:03:51 [INFO]: Epoch 467 - training loss (MAE): 0.0533, validation MSE: -0.4177
2025-05-02 18:04:06 [INFO]: Epoch 468 - training loss (MAE): 0.0529, validation MSE: -0.4256
2025-05-02 18:04:21 [INFO]: Epoch 469 - training loss (MAE): 0.0524, validation MSE: -0.4314
2025-05-02 18:04:36 [INFO]: Epoch 470 - training loss (MAE): 0.0523, validation MSE: -0.3581
2025-05-02 18:04:51 [INFO]: Epoch 471 - training loss (MAE): 0.0525, validation MSE: -0.4132
2025-05-02 18:05:07 [INFO]: Epoch 472 - training loss (MAE): 0.0520, validation MSE: -0.4095
2025-05-02 18:05:22 [INFO]: Epoch 473 - training loss (MAE): 0.0522, validation MSE: -0.4093
2025-05-02 18:05:37 [INFO]: Epoch 474 - training loss (MAE): 0.0523, validation MSE: -0.4290
2025-05-02 18:05:52 [INFO]: Epoch 475 - training loss (MAE): 0.0527, validation MSE: -0.3757
2025-05-02 18:06:08 [INFO]: Epoch 476 - training loss (MAE): 0.0524, validation MSE: -0.3863
2025-05-02 18:06:23 [INFO]: Epoch 477 - training loss (MAE): 0.0519, validation MSE: -0.4003
2025-05-02 18:06:38 [INFO]: Epoch 478 - training loss (MAE): 0.0525, validation MSE: -0.4394
2025-05-02 18:06:53 [INFO]: Epoch 479 - training loss (MAE): 0.0524, validation MSE: -0.4209
2025-05-02 18:07:09 [INFO]: Epoch 480 - training loss (MAE): 0.0519, validation MSE: -0.3420
2025-05-02 18:07:24 [INFO]: Epoch 481 - training loss (MAE): 0.0533, validation MSE: -0.4339
2025-05-02 18:07:39 [INFO]: Epoch 482 - training loss (MAE): 0.0529, validation MSE: -0.4002
2025-05-02 18:07:54 [INFO]: Epoch 483 - training loss (MAE): 0.0521, validation MSE: -0.4369
2025-05-02 18:08:09 [INFO]: Epoch 484 - training loss (MAE): 0.0512, validation MSE: -0.4357
2025-05-02 18:08:24 [INFO]: Epoch 485 - training loss (MAE): 0.0517, validation MSE: -0.3903
2025-05-02 18:08:39 [INFO]: Epoch 486 - training loss (MAE): 0.0526, validation MSE: -0.4112
2025-05-02 18:08:54 [INFO]: Epoch 487 - training loss (MAE): 0.0526, validation MSE: -0.4419
2025-05-02 18:09:10 [INFO]: Epoch 488 - training loss (MAE): 0.0521, validation MSE: -0.3960
2025-05-02 18:09:25 [INFO]: Epoch 489 - training loss (MAE): 0.0519, validation MSE: -0.4250
2025-05-02 18:09:40 [INFO]: Epoch 490 - training loss (MAE): 0.0519, validation MSE: -0.4225
2025-05-02 18:09:55 [INFO]: Epoch 491 - training loss (MAE): 0.0516, validation MSE: -0.4333
2025-05-02 18:10:10 [INFO]: Epoch 492 - training loss (MAE): 0.0523, validation MSE: -0.4482
2025-05-02 18:10:26 [INFO]: Epoch 493 - training loss (MAE): 0.0519, validation MSE: -0.4470
2025-05-02 18:10:41 [INFO]: Epoch 494 - training loss (MAE): 0.0510, validation MSE: -0.4303
2025-05-02 18:10:56 [INFO]: Epoch 495 - training loss (MAE): 0.0517, validation MSE: -0.4416
2025-05-02 18:11:12 [INFO]: Epoch 496 - training loss (MAE): 0.0511, validation MSE: -0.4268
2025-05-02 18:11:27 [INFO]: Epoch 497 - training loss (MAE): 0.0510, validation MSE: -0.4123
2025-05-02 18:11:42 [INFO]: Epoch 498 - training loss (MAE): 0.0516, validation MSE: -0.4239
2025-05-02 18:11:58 [INFO]: Epoch 499 - training loss (MAE): 0.0513, validation MSE: -0.4494
2025-05-02 18:12:13 [INFO]: Epoch 500 - training loss (MAE): 0.0512, validation MSE: -0.4278
2025-05-02 18:12:13 [INFO]: Finished training. The best model is from epoch#410.
2025-05-02 18:12:13 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_1.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-02 18:12:13 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_1.pypots
Fold 1 metrics: MAE: 0.299, MSE: 0.334, MRE: 0.418
Fold 1 metrics: MAE: 0.299, MSE: 0.334, MRE: 0.418
Training fold 2/10
2025-05-02 18:12:13 [INFO]: No given device, using default device: cuda
2025-05-02 18:12:13 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-02 18:12:13 [INFO]: Using customized MAE as the training loss function.
2025-05-02 18:12:13 [INFO]: Using customized MSE as the validation metric function.
2025-05-02 18:12:13 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-02 18:12:29 [INFO]: Epoch 001 - training loss (MAE): 1.0553, validation MSE: -0.2244
2025-05-02 18:12:44 [INFO]: Epoch 002 - training loss (MAE): 0.5247, validation MSE: -0.2936
2025-05-02 18:12:59 [INFO]: Epoch 003 - training loss (MAE): 0.4458, validation MSE: -0.3448
2025-05-02 18:13:14 [INFO]: Epoch 004 - training loss (MAE): 0.4050, validation MSE: -0.3696
2025-05-02 18:13:30 [INFO]: Epoch 005 - training loss (MAE): 0.3736, validation MSE: -0.4037
2025-05-02 18:13:45 [INFO]: Epoch 006 - training loss (MAE): 0.3424, validation MSE: -0.4228
2025-05-02 18:14:00 [INFO]: Epoch 007 - training loss (MAE): 0.3189, validation MSE: -0.4101
2025-05-02 18:14:15 [INFO]: Epoch 008 - training loss (MAE): 0.2974, validation MSE: -0.4357
2025-05-02 18:14:31 [INFO]: Epoch 009 - training loss (MAE): 0.2792, validation MSE: -0.4587
2025-05-02 18:14:46 [INFO]: Epoch 010 - training loss (MAE): 0.2612, validation MSE: -0.4752
2025-05-02 18:15:01 [INFO]: Epoch 011 - training loss (MAE): 0.2461, validation MSE: -0.4898
2025-05-02 18:15:16 [INFO]: Epoch 012 - training loss (MAE): 0.2314, validation MSE: -0.4777
2025-05-02 18:15:31 [INFO]: Epoch 013 - training loss (MAE): 0.2181, validation MSE: -0.5165
2025-05-02 18:15:47 [INFO]: Epoch 014 - training loss (MAE): 0.2077, validation MSE: -0.4949
2025-05-02 18:16:02 [INFO]: Epoch 015 - training loss (MAE): 0.1951, validation MSE: -0.4969
2025-05-02 18:16:17 [INFO]: Epoch 016 - training loss (MAE): 0.1858, validation MSE: -0.4944
2025-05-02 18:16:32 [INFO]: Epoch 017 - training loss (MAE): 0.1780, validation MSE: -0.4937
2025-05-02 18:16:48 [INFO]: Epoch 018 - training loss (MAE): 0.1702, validation MSE: -0.4851
2025-05-02 18:17:03 [INFO]: Epoch 019 - training loss (MAE): 0.1632, validation MSE: -0.4960
2025-05-02 18:17:18 [INFO]: Epoch 020 - training loss (MAE): 0.1566, validation MSE: -0.4775
2025-05-02 18:17:34 [INFO]: Epoch 021 - training loss (MAE): 0.1512, validation MSE: -0.4755
2025-05-02 18:17:49 [INFO]: Epoch 022 - training loss (MAE): 0.1460, validation MSE: -0.4581
2025-05-02 18:18:04 [INFO]: Epoch 023 - training loss (MAE): 0.1411, validation MSE: -0.4527
2025-05-02 18:18:20 [INFO]: Epoch 024 - training loss (MAE): 0.1366, validation MSE: -0.4763
2025-05-02 18:18:35 [INFO]: Epoch 025 - training loss (MAE): 0.1326, validation MSE: -0.4828
2025-05-02 18:18:50 [INFO]: Epoch 026 - training loss (MAE): 0.1284, validation MSE: -0.4358
2025-05-02 18:19:05 [INFO]: Epoch 027 - training loss (MAE): 0.1245, validation MSE: -0.4936
2025-05-02 18:19:20 [INFO]: Epoch 028 - training loss (MAE): 0.1207, validation MSE: -0.4913
2025-05-02 18:19:36 [INFO]: Epoch 029 - training loss (MAE): 0.1177, validation MSE: -0.4540
2025-05-02 18:19:51 [INFO]: Epoch 030 - training loss (MAE): 0.1145, validation MSE: -0.5004
2025-05-02 18:20:06 [INFO]: Epoch 031 - training loss (MAE): 0.1133, validation MSE: -0.4794
2025-05-02 18:20:21 [INFO]: Epoch 032 - training loss (MAE): 0.1099, validation MSE: -0.4594
2025-05-02 18:20:37 [INFO]: Epoch 033 - training loss (MAE): 0.1073, validation MSE: -0.4973
2025-05-02 18:20:52 [INFO]: Epoch 034 - training loss (MAE): 0.1052, validation MSE: -0.4979
2025-05-02 18:21:07 [INFO]: Epoch 035 - training loss (MAE): 0.1027, validation MSE: -0.4567
2025-05-02 18:21:22 [INFO]: Epoch 036 - training loss (MAE): 0.1012, validation MSE: -0.4975
2025-05-02 18:21:37 [INFO]: Epoch 037 - training loss (MAE): 0.0993, validation MSE: -0.4861
2025-05-02 18:21:53 [INFO]: Epoch 038 - training loss (MAE): 0.0978, validation MSE: -0.4959
2025-05-02 18:22:08 [INFO]: Epoch 039 - training loss (MAE): 0.0960, validation MSE: -0.4916
2025-05-02 18:22:23 [INFO]: Epoch 040 - training loss (MAE): 0.0954, validation MSE: -0.4694
2025-05-02 18:22:38 [INFO]: Epoch 041 - training loss (MAE): 0.0928, validation MSE: -0.4928
2025-05-02 18:22:54 [INFO]: Epoch 042 - training loss (MAE): 0.0920, validation MSE: -0.5287
2025-05-02 18:23:09 [INFO]: Epoch 043 - training loss (MAE): 0.0907, validation MSE: -0.4979
2025-05-02 18:23:24 [INFO]: Epoch 044 - training loss (MAE): 0.0903, validation MSE: -0.5133
2025-05-02 18:23:40 [INFO]: Epoch 045 - training loss (MAE): 0.0887, validation MSE: -0.4703
2025-05-02 18:23:55 [INFO]: Epoch 046 - training loss (MAE): 0.0879, validation MSE: -0.5283
2025-05-02 18:24:10 [INFO]: Epoch 047 - training loss (MAE): 0.0862, validation MSE: -0.5228
2025-05-02 18:24:25 [INFO]: Epoch 048 - training loss (MAE): 0.0856, validation MSE: -0.4545
2025-05-02 18:24:41 [INFO]: Epoch 049 - training loss (MAE): 0.0848, validation MSE: -0.4868
2025-05-02 18:24:56 [INFO]: Epoch 050 - training loss (MAE): 0.0844, validation MSE: -0.4790
2025-05-02 18:25:11 [INFO]: Epoch 051 - training loss (MAE): 0.0832, validation MSE: -0.4893
2025-05-02 18:25:26 [INFO]: Epoch 052 - training loss (MAE): 0.0824, validation MSE: -0.5210
2025-05-02 18:25:41 [INFO]: Epoch 053 - training loss (MAE): 0.0820, validation MSE: -0.5403
2025-05-02 18:25:56 [INFO]: Epoch 054 - training loss (MAE): 0.0816, validation MSE: -0.4889
2025-05-02 18:26:12 [INFO]: Epoch 055 - training loss (MAE): 0.0804, validation MSE: -0.4915
2025-05-02 18:26:27 [INFO]: Epoch 056 - training loss (MAE): 0.0803, validation MSE: -0.4611
2025-05-02 18:26:42 [INFO]: Epoch 057 - training loss (MAE): 0.0788, validation MSE: -0.5109
2025-05-02 18:26:57 [INFO]: Epoch 058 - training loss (MAE): 0.0789, validation MSE: -0.5164
2025-05-02 18:27:13 [INFO]: Epoch 059 - training loss (MAE): 0.0782, validation MSE: -0.4533
2025-05-02 18:27:28 [INFO]: Epoch 060 - training loss (MAE): 0.0777, validation MSE: -0.5285
2025-05-02 18:27:43 [INFO]: Epoch 061 - training loss (MAE): 0.0767, validation MSE: -0.5437
2025-05-02 18:27:59 [INFO]: Epoch 062 - training loss (MAE): 0.0771, validation MSE: -0.5035
2025-05-02 18:28:14 [INFO]: Epoch 063 - training loss (MAE): 0.0763, validation MSE: -0.5099
2025-05-02 18:28:29 [INFO]: Epoch 064 - training loss (MAE): 0.0760, validation MSE: -0.5331
2025-05-02 18:28:44 [INFO]: Epoch 065 - training loss (MAE): 0.0753, validation MSE: -0.5408
2025-05-02 18:28:59 [INFO]: Epoch 066 - training loss (MAE): 0.0757, validation MSE: -0.5112
2025-05-02 18:29:15 [INFO]: Epoch 067 - training loss (MAE): 0.0742, validation MSE: -0.5245
2025-05-02 18:29:30 [INFO]: Epoch 068 - training loss (MAE): 0.0740, validation MSE: -0.5311
2025-05-02 18:29:46 [INFO]: Epoch 069 - training loss (MAE): 0.0734, validation MSE: -0.5060
2025-05-02 18:30:01 [INFO]: Epoch 070 - training loss (MAE): 0.0754, validation MSE: -0.4797
2025-05-02 18:30:17 [INFO]: Epoch 071 - training loss (MAE): 0.0730, validation MSE: -0.5215
2025-05-02 18:30:32 [INFO]: Epoch 072 - training loss (MAE): 0.0730, validation MSE: -0.4953
2025-05-02 18:30:48 [INFO]: Epoch 073 - training loss (MAE): 0.0723, validation MSE: -0.5071
2025-05-02 18:31:03 [INFO]: Epoch 074 - training loss (MAE): 0.0724, validation MSE: -0.4169
2025-05-02 18:31:18 [INFO]: Epoch 075 - training loss (MAE): 0.0732, validation MSE: -0.4552
2025-05-02 18:31:33 [INFO]: Epoch 076 - training loss (MAE): 0.0723, validation MSE: -0.5176
2025-05-02 18:31:48 [INFO]: Epoch 077 - training loss (MAE): 0.0715, validation MSE: -0.4728
2025-05-02 18:32:03 [INFO]: Epoch 078 - training loss (MAE): 0.0712, validation MSE: -0.5281
2025-05-02 18:32:19 [INFO]: Epoch 079 - training loss (MAE): 0.0710, validation MSE: -0.5283
2025-05-02 18:32:34 [INFO]: Epoch 080 - training loss (MAE): 0.0711, validation MSE: -0.5206
2025-05-02 18:32:49 [INFO]: Epoch 081 - training loss (MAE): 0.0708, validation MSE: -0.5391
2025-05-02 18:33:05 [INFO]: Epoch 082 - training loss (MAE): 0.0701, validation MSE: -0.5137
2025-05-02 18:33:20 [INFO]: Epoch 083 - training loss (MAE): 0.0703, validation MSE: -0.5058
2025-05-02 18:33:35 [INFO]: Epoch 084 - training loss (MAE): 0.0693, validation MSE: -0.5222
2025-05-02 18:33:50 [INFO]: Epoch 085 - training loss (MAE): 0.0703, validation MSE: -0.4920
2025-05-02 18:34:05 [INFO]: Epoch 086 - training loss (MAE): 0.0691, validation MSE: -0.5177
2025-05-02 18:34:21 [INFO]: Epoch 087 - training loss (MAE): 0.0691, validation MSE: -0.5021
2025-05-02 18:34:36 [INFO]: Epoch 088 - training loss (MAE): 0.0690, validation MSE: -0.4626
2025-05-02 18:34:51 [INFO]: Epoch 089 - training loss (MAE): 0.0688, validation MSE: -0.4996
2025-05-02 18:35:07 [INFO]: Epoch 090 - training loss (MAE): 0.0691, validation MSE: -0.5292
2025-05-02 18:35:22 [INFO]: Epoch 091 - training loss (MAE): 0.0683, validation MSE: -0.4995
2025-05-02 18:35:37 [INFO]: Epoch 092 - training loss (MAE): 0.0677, validation MSE: -0.5298
2025-05-02 18:35:52 [INFO]: Epoch 093 - training loss (MAE): 0.0680, validation MSE: -0.5100
2025-05-02 18:36:08 [INFO]: Epoch 094 - training loss (MAE): 0.0688, validation MSE: -0.5312
2025-05-02 18:36:23 [INFO]: Epoch 095 - training loss (MAE): 0.0677, validation MSE: -0.4938
2025-05-02 18:36:38 [INFO]: Epoch 096 - training loss (MAE): 0.0679, validation MSE: -0.5233
2025-05-02 18:36:53 [INFO]: Epoch 097 - training loss (MAE): 0.0672, validation MSE: -0.4573
2025-05-02 18:37:08 [INFO]: Epoch 098 - training loss (MAE): 0.0677, validation MSE: -0.5267
2025-05-02 18:37:24 [INFO]: Epoch 099 - training loss (MAE): 0.0666, validation MSE: -0.5182
2025-05-02 18:37:39 [INFO]: Epoch 100 - training loss (MAE): 0.0678, validation MSE: -0.5297
2025-05-02 18:37:54 [INFO]: Epoch 101 - training loss (MAE): 0.0671, validation MSE: -0.5334
2025-05-02 18:38:10 [INFO]: Epoch 102 - training loss (MAE): 0.0667, validation MSE: -0.4829
2025-05-02 18:38:25 [INFO]: Epoch 103 - training loss (MAE): 0.0666, validation MSE: -0.4843
2025-05-02 18:38:40 [INFO]: Epoch 104 - training loss (MAE): 0.0662, validation MSE: -0.5134
2025-05-02 18:38:56 [INFO]: Epoch 105 - training loss (MAE): 0.0664, validation MSE: -0.5032
2025-05-02 18:39:11 [INFO]: Epoch 106 - training loss (MAE): 0.0659, validation MSE: -0.5369
2025-05-02 18:39:26 [INFO]: Epoch 107 - training loss (MAE): 0.0667, validation MSE: -0.4314
2025-05-02 18:39:41 [INFO]: Epoch 108 - training loss (MAE): 0.0664, validation MSE: -0.4926
2025-05-02 18:39:56 [INFO]: Epoch 109 - training loss (MAE): 0.0659, validation MSE: -0.5295
2025-05-02 18:40:11 [INFO]: Epoch 110 - training loss (MAE): 0.0652, validation MSE: -0.4905
2025-05-02 18:40:27 [INFO]: Epoch 111 - training loss (MAE): 0.0655, validation MSE: -0.5048
2025-05-02 18:40:42 [INFO]: Epoch 112 - training loss (MAE): 0.0661, validation MSE: -0.5044
2025-05-02 18:40:57 [INFO]: Epoch 113 - training loss (MAE): 0.0652, validation MSE: -0.5225
2025-05-02 18:41:12 [INFO]: Epoch 114 - training loss (MAE): 0.0655, validation MSE: -0.5152
2025-05-02 18:41:28 [INFO]: Epoch 115 - training loss (MAE): 0.0652, validation MSE: -0.5508
2025-05-02 18:41:43 [INFO]: Epoch 116 - training loss (MAE): 0.0647, validation MSE: -0.5206
2025-05-02 18:41:58 [INFO]: Epoch 117 - training loss (MAE): 0.0646, validation MSE: -0.5413
2025-05-02 18:42:14 [INFO]: Epoch 118 - training loss (MAE): 0.0651, validation MSE: -0.5205
2025-05-02 18:42:29 [INFO]: Epoch 119 - training loss (MAE): 0.0649, validation MSE: -0.4943
2025-05-02 18:42:44 [INFO]: Epoch 120 - training loss (MAE): 0.0649, validation MSE: -0.5111
2025-05-02 18:42:59 [INFO]: Epoch 121 - training loss (MAE): 0.0647, validation MSE: -0.5427
2025-05-02 18:43:15 [INFO]: Epoch 122 - training loss (MAE): 0.0642, validation MSE: -0.5397
2025-05-02 18:43:30 [INFO]: Epoch 123 - training loss (MAE): 0.0643, validation MSE: -0.5377
2025-05-02 18:43:45 [INFO]: Epoch 124 - training loss (MAE): 0.0640, validation MSE: -0.5473
2025-05-02 18:44:00 [INFO]: Epoch 125 - training loss (MAE): 0.0636, validation MSE: -0.5354
2025-05-02 18:44:16 [INFO]: Epoch 126 - training loss (MAE): 0.0641, validation MSE: -0.5260
2025-05-02 18:44:31 [INFO]: Epoch 127 - training loss (MAE): 0.0641, validation MSE: -0.5426
2025-05-02 18:44:46 [INFO]: Epoch 128 - training loss (MAE): 0.0637, validation MSE: -0.5226
2025-05-02 18:45:01 [INFO]: Epoch 129 - training loss (MAE): 0.0644, validation MSE: -0.5452
2025-05-02 18:45:16 [INFO]: Epoch 130 - training loss (MAE): 0.0637, validation MSE: -0.5289
2025-05-02 18:45:32 [INFO]: Epoch 131 - training loss (MAE): 0.0637, validation MSE: -0.5335
2025-05-02 18:45:47 [INFO]: Epoch 132 - training loss (MAE): 0.0640, validation MSE: -0.5236
2025-05-02 18:46:02 [INFO]: Epoch 133 - training loss (MAE): 0.0632, validation MSE: -0.5508
2025-05-02 18:46:17 [INFO]: Epoch 134 - training loss (MAE): 0.0637, validation MSE: -0.5072
2025-05-02 18:46:33 [INFO]: Epoch 135 - training loss (MAE): 0.0636, validation MSE: -0.5224
2025-05-02 18:46:48 [INFO]: Epoch 136 - training loss (MAE): 0.0633, validation MSE: -0.5329
2025-05-02 18:47:03 [INFO]: Epoch 137 - training loss (MAE): 0.0629, validation MSE: -0.5496
2025-05-02 18:47:18 [INFO]: Epoch 138 - training loss (MAE): 0.0635, validation MSE: -0.5084
2025-05-02 18:47:34 [INFO]: Epoch 139 - training loss (MAE): 0.0638, validation MSE: -0.5205
2025-05-02 18:47:49 [INFO]: Epoch 140 - training loss (MAE): 0.0627, validation MSE: -0.5395
2025-05-02 18:48:04 [INFO]: Epoch 141 - training loss (MAE): 0.0626, validation MSE: -0.4998
2025-05-02 18:48:19 [INFO]: Epoch 142 - training loss (MAE): 0.0630, validation MSE: -0.5064
2025-05-02 18:48:35 [INFO]: Epoch 143 - training loss (MAE): 0.0624, validation MSE: -0.5092
2025-05-02 18:48:50 [INFO]: Epoch 144 - training loss (MAE): 0.0629, validation MSE: -0.5222
2025-05-02 18:49:05 [INFO]: Epoch 145 - training loss (MAE): 0.0631, validation MSE: -0.5323
2025-05-02 18:49:21 [INFO]: Epoch 146 - training loss (MAE): 0.0627, validation MSE: -0.5358
2025-05-02 18:49:36 [INFO]: Epoch 147 - training loss (MAE): 0.0626, validation MSE: -0.5395
2025-05-02 18:49:51 [INFO]: Epoch 148 - training loss (MAE): 0.0621, validation MSE: -0.5432
2025-05-02 18:50:06 [INFO]: Epoch 149 - training loss (MAE): 0.0620, validation MSE: -0.5261
2025-05-02 18:50:22 [INFO]: Epoch 150 - training loss (MAE): 0.0620, validation MSE: -0.5163
2025-05-02 18:50:37 [INFO]: Epoch 151 - training loss (MAE): 0.0621, validation MSE: -0.4936
2025-05-02 18:50:52 [INFO]: Epoch 152 - training loss (MAE): 0.0623, validation MSE: -0.4832
2025-05-02 18:51:07 [INFO]: Epoch 153 - training loss (MAE): 0.0622, validation MSE: -0.5402
2025-05-02 18:51:23 [INFO]: Epoch 154 - training loss (MAE): 0.0623, validation MSE: -0.4989
2025-05-02 18:51:38 [INFO]: Epoch 155 - training loss (MAE): 0.0618, validation MSE: -0.5279
2025-05-02 18:51:53 [INFO]: Epoch 156 - training loss (MAE): 0.0616, validation MSE: -0.5222
2025-05-02 18:52:08 [INFO]: Epoch 157 - training loss (MAE): 0.0620, validation MSE: -0.5334
2025-05-02 18:52:23 [INFO]: Epoch 158 - training loss (MAE): 0.0612, validation MSE: -0.5349
2025-05-02 18:52:39 [INFO]: Epoch 159 - training loss (MAE): 0.0621, validation MSE: -0.5199
2025-05-02 18:52:54 [INFO]: Epoch 160 - training loss (MAE): 0.0616, validation MSE: -0.5268
2025-05-02 18:53:09 [INFO]: Epoch 161 - training loss (MAE): 0.0609, validation MSE: -0.5269
2025-05-02 18:53:24 [INFO]: Epoch 162 - training loss (MAE): 0.0613, validation MSE: -0.5090
2025-05-02 18:53:39 [INFO]: Epoch 163 - training loss (MAE): 0.0621, validation MSE: -0.5335
2025-05-02 18:53:55 [INFO]: Epoch 164 - training loss (MAE): 0.0608, validation MSE: -0.5356
2025-05-02 18:54:10 [INFO]: Epoch 165 - training loss (MAE): 0.0618, validation MSE: -0.4982
2025-05-02 18:54:25 [INFO]: Epoch 166 - training loss (MAE): 0.0614, validation MSE: -0.5295
2025-05-02 18:54:40 [INFO]: Epoch 167 - training loss (MAE): 0.0612, validation MSE: -0.5346
2025-05-02 18:54:56 [INFO]: Epoch 168 - training loss (MAE): 0.0610, validation MSE: -0.5349
2025-05-02 18:55:11 [INFO]: Epoch 169 - training loss (MAE): 0.0613, validation MSE: -0.5427
2025-05-02 18:55:26 [INFO]: Epoch 170 - training loss (MAE): 0.0601, validation MSE: -0.5345
2025-05-02 18:55:41 [INFO]: Epoch 171 - training loss (MAE): 0.0609, validation MSE: -0.4746
2025-05-02 18:55:56 [INFO]: Epoch 172 - training loss (MAE): 0.0604, validation MSE: -0.5534
2025-05-02 18:56:12 [INFO]: Epoch 173 - training loss (MAE): 0.0607, validation MSE: -0.5250
2025-05-02 18:56:27 [INFO]: Epoch 174 - training loss (MAE): 0.0612, validation MSE: -0.5153
2025-05-02 18:56:43 [INFO]: Epoch 175 - training loss (MAE): 0.0610, validation MSE: -0.5392
2025-05-02 18:56:58 [INFO]: Epoch 176 - training loss (MAE): 0.0605, validation MSE: -0.5317
2025-05-02 18:57:13 [INFO]: Epoch 177 - training loss (MAE): 0.0604, validation MSE: -0.5355
2025-05-02 18:57:28 [INFO]: Epoch 178 - training loss (MAE): 0.0609, validation MSE: -0.5443
2025-05-02 18:57:44 [INFO]: Epoch 179 - training loss (MAE): 0.0604, validation MSE: -0.5570
2025-05-02 18:57:59 [INFO]: Epoch 180 - training loss (MAE): 0.0600, validation MSE: -0.5305
2025-05-02 18:58:14 [INFO]: Epoch 181 - training loss (MAE): 0.0606, validation MSE: -0.5492
2025-05-02 18:58:29 [INFO]: Epoch 182 - training loss (MAE): 0.0604, validation MSE: -0.5346
2025-05-02 18:58:45 [INFO]: Epoch 183 - training loss (MAE): 0.0601, validation MSE: -0.5562
2025-05-02 18:59:00 [INFO]: Epoch 184 - training loss (MAE): 0.0601, validation MSE: -0.5523
2025-05-02 18:59:15 [INFO]: Epoch 185 - training loss (MAE): 0.0599, validation MSE: -0.5358
2025-05-02 18:59:31 [INFO]: Epoch 186 - training loss (MAE): 0.0599, validation MSE: -0.5244
2025-05-02 18:59:46 [INFO]: Epoch 187 - training loss (MAE): 0.0600, validation MSE: -0.5485
2025-05-02 19:00:01 [INFO]: Epoch 188 - training loss (MAE): 0.0602, validation MSE: -0.5251
2025-05-02 19:00:17 [INFO]: Epoch 189 - training loss (MAE): 0.0596, validation MSE: -0.4957
2025-05-02 19:00:32 [INFO]: Epoch 190 - training loss (MAE): 0.0603, validation MSE: -0.4737
2025-05-02 19:00:47 [INFO]: Epoch 191 - training loss (MAE): 0.0600, validation MSE: -0.4765
2025-05-02 19:01:02 [INFO]: Epoch 192 - training loss (MAE): 0.0600, validation MSE: -0.5562
2025-05-02 19:01:18 [INFO]: Epoch 193 - training loss (MAE): 0.0598, validation MSE: -0.5486
2025-05-02 19:01:33 [INFO]: Epoch 194 - training loss (MAE): 0.0590, validation MSE: -0.5355
2025-05-02 19:01:48 [INFO]: Epoch 195 - training loss (MAE): 0.0595, validation MSE: -0.5488
2025-05-02 19:02:03 [INFO]: Epoch 196 - training loss (MAE): 0.0594, validation MSE: -0.5324
2025-05-02 19:02:19 [INFO]: Epoch 197 - training loss (MAE): 0.0601, validation MSE: -0.5486
2025-05-02 19:02:34 [INFO]: Epoch 198 - training loss (MAE): 0.0591, validation MSE: -0.5411
2025-05-02 19:02:49 [INFO]: Epoch 199 - training loss (MAE): 0.0599, validation MSE: -0.5405
2025-05-02 19:03:05 [INFO]: Epoch 200 - training loss (MAE): 0.0595, validation MSE: -0.5245
2025-05-02 19:03:20 [INFO]: Epoch 201 - training loss (MAE): 0.0604, validation MSE: -0.5303
2025-05-02 19:03:35 [INFO]: Epoch 202 - training loss (MAE): 0.0592, validation MSE: -0.5110
2025-05-02 19:03:51 [INFO]: Epoch 203 - training loss (MAE): 0.0592, validation MSE: -0.5260
2025-05-02 19:04:06 [INFO]: Epoch 204 - training loss (MAE): 0.0592, validation MSE: -0.5357
2025-05-02 19:04:21 [INFO]: Epoch 205 - training loss (MAE): 0.0589, validation MSE: -0.5530
2025-05-02 19:04:36 [INFO]: Epoch 206 - training loss (MAE): 0.0590, validation MSE: -0.5544
2025-05-02 19:04:51 [INFO]: Epoch 207 - training loss (MAE): 0.0590, validation MSE: -0.5468
2025-05-02 19:05:07 [INFO]: Epoch 208 - training loss (MAE): 0.0588, validation MSE: -0.5553
2025-05-02 19:05:22 [INFO]: Epoch 209 - training loss (MAE): 0.0589, validation MSE: -0.5331
2025-05-02 19:05:37 [INFO]: Epoch 210 - training loss (MAE): 0.0591, validation MSE: -0.5659
2025-05-02 19:05:52 [INFO]: Epoch 211 - training loss (MAE): 0.0592, validation MSE: -0.5530
2025-05-02 19:06:08 [INFO]: Epoch 212 - training loss (MAE): 0.0587, validation MSE: -0.5025
2025-05-02 19:06:23 [INFO]: Epoch 213 - training loss (MAE): 0.0596, validation MSE: -0.5258
2025-05-02 19:06:38 [INFO]: Epoch 214 - training loss (MAE): 0.0588, validation MSE: -0.5260
2025-05-02 19:06:53 [INFO]: Epoch 215 - training loss (MAE): 0.0587, validation MSE: -0.5349
2025-05-02 19:07:09 [INFO]: Epoch 216 - training loss (MAE): 0.0587, validation MSE: -0.5141
2025-05-02 19:07:24 [INFO]: Epoch 217 - training loss (MAE): 0.0583, validation MSE: -0.5403
2025-05-02 19:07:39 [INFO]: Epoch 218 - training loss (MAE): 0.0583, validation MSE: -0.5472
2025-05-02 19:07:54 [INFO]: Epoch 219 - training loss (MAE): 0.0589, validation MSE: -0.5676
2025-05-02 19:08:10 [INFO]: Epoch 220 - training loss (MAE): 0.0585, validation MSE: -0.5358
2025-05-02 19:08:25 [INFO]: Epoch 221 - training loss (MAE): 0.0585, validation MSE: -0.5473
2025-05-02 19:08:40 [INFO]: Epoch 222 - training loss (MAE): 0.0583, validation MSE: -0.5525
2025-05-02 19:08:55 [INFO]: Epoch 223 - training loss (MAE): 0.0588, validation MSE: -0.5428
2025-05-02 19:09:11 [INFO]: Epoch 224 - training loss (MAE): 0.0589, validation MSE: -0.5649
2025-05-02 19:09:26 [INFO]: Epoch 225 - training loss (MAE): 0.0583, validation MSE: -0.5516
2025-05-02 19:09:41 [INFO]: Epoch 226 - training loss (MAE): 0.0587, validation MSE: -0.5413
2025-05-02 19:09:57 [INFO]: Epoch 227 - training loss (MAE): 0.0587, validation MSE: -0.4899
2025-05-02 19:10:13 [INFO]: Epoch 228 - training loss (MAE): 0.0581, validation MSE: -0.5369
2025-05-02 19:10:28 [INFO]: Epoch 229 - training loss (MAE): 0.0581, validation MSE: -0.5406
2025-05-02 19:10:43 [INFO]: Epoch 230 - training loss (MAE): 0.0589, validation MSE: -0.5408
2025-05-02 19:10:59 [INFO]: Epoch 231 - training loss (MAE): 0.0574, validation MSE: -0.5439
2025-05-02 19:11:14 [INFO]: Epoch 232 - training loss (MAE): 0.0580, validation MSE: -0.5464
2025-05-02 19:11:29 [INFO]: Epoch 233 - training loss (MAE): 0.0581, validation MSE: -0.5508
2025-05-02 19:11:45 [INFO]: Epoch 234 - training loss (MAE): 0.0582, validation MSE: -0.5298
2025-05-02 19:12:00 [INFO]: Epoch 235 - training loss (MAE): 0.0579, validation MSE: -0.5334
2025-05-02 19:12:15 [INFO]: Epoch 236 - training loss (MAE): 0.0582, validation MSE: -0.5457
2025-05-02 19:12:31 [INFO]: Epoch 237 - training loss (MAE): 0.0579, validation MSE: -0.5317
2025-05-02 19:12:47 [INFO]: Epoch 238 - training loss (MAE): 0.0576, validation MSE: -0.5556
2025-05-02 19:13:02 [INFO]: Epoch 239 - training loss (MAE): 0.0575, validation MSE: -0.5558
2025-05-02 19:13:17 [INFO]: Epoch 240 - training loss (MAE): 0.0579, validation MSE: -0.5517
2025-05-02 19:13:32 [INFO]: Epoch 241 - training loss (MAE): 0.0580, validation MSE: -0.5055
2025-05-02 19:13:48 [INFO]: Epoch 242 - training loss (MAE): 0.0579, validation MSE: -0.5303
2025-05-02 19:14:03 [INFO]: Epoch 243 - training loss (MAE): 0.0578, validation MSE: -0.5404
2025-05-02 19:14:18 [INFO]: Epoch 244 - training loss (MAE): 0.0577, validation MSE: -0.5370
2025-05-02 19:14:33 [INFO]: Epoch 245 - training loss (MAE): 0.0579, validation MSE: -0.5519
2025-05-02 19:14:48 [INFO]: Epoch 246 - training loss (MAE): 0.0575, validation MSE: -0.5565
2025-05-02 19:15:03 [INFO]: Epoch 247 - training loss (MAE): 0.0575, validation MSE: -0.5604
2025-05-02 19:15:19 [INFO]: Epoch 248 - training loss (MAE): 0.0574, validation MSE: -0.5648
2025-05-02 19:15:34 [INFO]: Epoch 249 - training loss (MAE): 0.0567, validation MSE: -0.5185
2025-05-02 19:15:49 [INFO]: Epoch 250 - training loss (MAE): 0.0577, validation MSE: -0.5314
2025-05-02 19:16:04 [INFO]: Epoch 251 - training loss (MAE): 0.0573, validation MSE: -0.5375
2025-05-02 19:16:20 [INFO]: Epoch 252 - training loss (MAE): 0.0581, validation MSE: -0.5577
2025-05-02 19:16:35 [INFO]: Epoch 253 - training loss (MAE): 0.0571, validation MSE: -0.5446
2025-05-02 19:16:50 [INFO]: Epoch 254 - training loss (MAE): 0.0569, validation MSE: -0.5479
2025-05-02 19:17:06 [INFO]: Epoch 255 - training loss (MAE): 0.0571, validation MSE: -0.5454
2025-05-02 19:17:21 [INFO]: Epoch 256 - training loss (MAE): 0.0574, validation MSE: -0.5257
2025-05-02 19:17:36 [INFO]: Epoch 257 - training loss (MAE): 0.0573, validation MSE: -0.5217
2025-05-02 19:17:52 [INFO]: Epoch 258 - training loss (MAE): 0.0572, validation MSE: -0.5413
2025-05-02 19:18:07 [INFO]: Epoch 259 - training loss (MAE): 0.0568, validation MSE: -0.5389
2025-05-02 19:18:22 [INFO]: Epoch 260 - training loss (MAE): 0.0568, validation MSE: -0.5475
2025-05-02 19:18:38 [INFO]: Epoch 261 - training loss (MAE): 0.0565, validation MSE: -0.5551
2025-05-02 19:18:53 [INFO]: Epoch 262 - training loss (MAE): 0.0567, validation MSE: -0.5484
2025-05-02 19:19:08 [INFO]: Epoch 263 - training loss (MAE): 0.0573, validation MSE: -0.5296
2025-05-02 19:19:23 [INFO]: Epoch 264 - training loss (MAE): 0.0573, validation MSE: -0.5485
2025-05-02 19:19:39 [INFO]: Epoch 265 - training loss (MAE): 0.0572, validation MSE: -0.5187
2025-05-02 19:19:54 [INFO]: Epoch 266 - training loss (MAE): 0.0570, validation MSE: -0.5505
2025-05-02 19:20:10 [INFO]: Epoch 267 - training loss (MAE): 0.0568, validation MSE: -0.5461
2025-05-02 19:20:25 [INFO]: Epoch 268 - training loss (MAE): 0.0568, validation MSE: -0.5514
2025-05-02 19:20:40 [INFO]: Epoch 269 - training loss (MAE): 0.0564, validation MSE: -0.5391
2025-05-02 19:20:55 [INFO]: Epoch 270 - training loss (MAE): 0.0567, validation MSE: -0.5461
2025-05-02 19:21:10 [INFO]: Epoch 271 - training loss (MAE): 0.0569, validation MSE: -0.5407
2025-05-02 19:21:26 [INFO]: Epoch 272 - training loss (MAE): 0.0567, validation MSE: -0.5507
2025-05-02 19:21:41 [INFO]: Epoch 273 - training loss (MAE): 0.0564, validation MSE: -0.5326
2025-05-02 19:21:56 [INFO]: Epoch 274 - training loss (MAE): 0.0563, validation MSE: -0.5241
2025-05-02 19:22:11 [INFO]: Epoch 275 - training loss (MAE): 0.0566, validation MSE: -0.5337
2025-05-02 19:22:27 [INFO]: Epoch 276 - training loss (MAE): 0.0560, validation MSE: -0.5496
2025-05-02 19:22:42 [INFO]: Epoch 277 - training loss (MAE): 0.0564, validation MSE: -0.5481
2025-05-02 19:22:57 [INFO]: Epoch 278 - training loss (MAE): 0.0565, validation MSE: -0.5475
2025-05-02 19:23:12 [INFO]: Epoch 279 - training loss (MAE): 0.0562, validation MSE: -0.5579
2025-05-02 19:23:28 [INFO]: Epoch 280 - training loss (MAE): 0.0564, validation MSE: -0.5311
2025-05-02 19:23:43 [INFO]: Epoch 281 - training loss (MAE): 0.0562, validation MSE: -0.5531
2025-05-02 19:23:58 [INFO]: Epoch 282 - training loss (MAE): 0.0569, validation MSE: -0.3691
2025-05-02 19:24:14 [INFO]: Epoch 283 - training loss (MAE): 0.0565, validation MSE: -0.5399
2025-05-02 19:24:29 [INFO]: Epoch 284 - training loss (MAE): 0.0571, validation MSE: -0.5263
2025-05-02 19:24:44 [INFO]: Epoch 285 - training loss (MAE): 0.0564, validation MSE: -0.5625
2025-05-02 19:24:59 [INFO]: Epoch 286 - training loss (MAE): 0.0564, validation MSE: -0.5522
2025-05-02 19:25:15 [INFO]: Epoch 287 - training loss (MAE): 0.0556, validation MSE: -0.5396
2025-05-02 19:25:30 [INFO]: Epoch 288 - training loss (MAE): 0.0561, validation MSE: -0.5526
2025-05-02 19:25:45 [INFO]: Epoch 289 - training loss (MAE): 0.0600, validation MSE: -0.5055
2025-05-02 19:26:01 [INFO]: Epoch 290 - training loss (MAE): 0.0576, validation MSE: -0.5179
2025-05-02 19:26:16 [INFO]: Epoch 291 - training loss (MAE): 0.0562, validation MSE: -0.5493
2025-05-02 19:26:31 [INFO]: Epoch 292 - training loss (MAE): 0.0554, validation MSE: -0.5321
2025-05-02 19:26:46 [INFO]: Epoch 293 - training loss (MAE): 0.0561, validation MSE: -0.5235
2025-05-02 19:27:01 [INFO]: Epoch 294 - training loss (MAE): 0.0560, validation MSE: -0.5339
2025-05-02 19:27:16 [INFO]: Epoch 295 - training loss (MAE): 0.0559, validation MSE: -0.5508
2025-05-02 19:27:32 [INFO]: Epoch 296 - training loss (MAE): 0.0554, validation MSE: -0.5401
2025-05-02 19:27:47 [INFO]: Epoch 297 - training loss (MAE): 0.0560, validation MSE: -0.5291
2025-05-02 19:28:02 [INFO]: Epoch 298 - training loss (MAE): 0.0557, validation MSE: -0.5338
2025-05-02 19:28:17 [INFO]: Epoch 299 - training loss (MAE): 0.0558, validation MSE: -0.5404
2025-05-02 19:28:32 [INFO]: Epoch 300 - training loss (MAE): 0.0561, validation MSE: -0.5405
2025-05-02 19:28:48 [INFO]: Epoch 301 - training loss (MAE): 0.0550, validation MSE: -0.5448
2025-05-02 19:29:03 [INFO]: Epoch 302 - training loss (MAE): 0.0562, validation MSE: -0.5536
2025-05-02 19:29:18 [INFO]: Epoch 303 - training loss (MAE): 0.0555, validation MSE: -0.5434
2025-05-02 19:29:34 [INFO]: Epoch 304 - training loss (MAE): 0.0565, validation MSE: -0.5429
2025-05-02 19:29:49 [INFO]: Epoch 305 - training loss (MAE): 0.0550, validation MSE: -0.5353
2025-05-02 19:30:04 [INFO]: Epoch 306 - training loss (MAE): 0.0549, validation MSE: -0.5301
2025-05-02 19:30:20 [INFO]: Epoch 307 - training loss (MAE): 0.0553, validation MSE: -0.5444
2025-05-02 19:30:35 [INFO]: Epoch 308 - training loss (MAE): 0.0552, validation MSE: -0.5443
2025-05-02 19:30:50 [INFO]: Epoch 309 - training loss (MAE): 0.0557, validation MSE: -0.5450
2025-05-02 19:31:05 [INFO]: Epoch 310 - training loss (MAE): 0.0552, validation MSE: -0.5316
2025-05-02 19:31:21 [INFO]: Epoch 311 - training loss (MAE): 0.0550, validation MSE: -0.5275
2025-05-02 19:31:36 [INFO]: Epoch 312 - training loss (MAE): 0.0554, validation MSE: -0.5318
2025-05-02 19:31:51 [INFO]: Epoch 313 - training loss (MAE): 0.0554, validation MSE: -0.5549
2025-05-02 19:32:06 [INFO]: Epoch 314 - training loss (MAE): 0.0555, validation MSE: -0.5428
2025-05-02 19:32:22 [INFO]: Epoch 315 - training loss (MAE): 0.0546, validation MSE: -0.5276
2025-05-02 19:32:37 [INFO]: Epoch 316 - training loss (MAE): 0.0556, validation MSE: -0.5483
2025-05-02 19:32:52 [INFO]: Epoch 317 - training loss (MAE): 0.0549, validation MSE: -0.5469
2025-05-02 19:33:08 [INFO]: Epoch 318 - training loss (MAE): 0.0549, validation MSE: -0.5288
2025-05-02 19:33:23 [INFO]: Epoch 319 - training loss (MAE): 0.0561, validation MSE: -0.5335
2025-05-02 19:33:38 [INFO]: Epoch 320 - training loss (MAE): 0.0549, validation MSE: -0.5203
2025-05-02 19:33:53 [INFO]: Epoch 321 - training loss (MAE): 0.0549, validation MSE: -0.5382
2025-05-02 19:34:09 [INFO]: Epoch 322 - training loss (MAE): 0.0550, validation MSE: -0.5293
2025-05-02 19:34:24 [INFO]: Epoch 323 - training loss (MAE): 0.0554, validation MSE: -0.5238
2025-05-02 19:34:39 [INFO]: Epoch 324 - training loss (MAE): 0.0550, validation MSE: -0.5423
2025-05-02 19:34:54 [INFO]: Epoch 325 - training loss (MAE): 0.0549, validation MSE: -0.5493
2025-05-02 19:35:09 [INFO]: Epoch 326 - training loss (MAE): 0.0552, validation MSE: -0.5151
2025-05-02 19:35:25 [INFO]: Epoch 327 - training loss (MAE): 0.0546, validation MSE: -0.5401
2025-05-02 19:35:40 [INFO]: Epoch 328 - training loss (MAE): 0.0549, validation MSE: -0.5510
2025-05-02 19:35:55 [INFO]: Epoch 329 - training loss (MAE): 0.0553, validation MSE: -0.5411
2025-05-02 19:36:10 [INFO]: Epoch 330 - training loss (MAE): 0.0546, validation MSE: -0.5588
2025-05-02 19:36:25 [INFO]: Epoch 331 - training loss (MAE): 0.0548, validation MSE: -0.5373
2025-05-02 19:36:41 [INFO]: Epoch 332 - training loss (MAE): 0.0559, validation MSE: -0.5426
2025-05-02 19:36:56 [INFO]: Epoch 333 - training loss (MAE): 0.0550, validation MSE: -0.5387
2025-05-02 19:37:11 [INFO]: Epoch 334 - training loss (MAE): 0.0548, validation MSE: -0.5395
2025-05-02 19:37:26 [INFO]: Epoch 335 - training loss (MAE): 0.0547, validation MSE: -0.5443
2025-05-02 19:37:42 [INFO]: Epoch 336 - training loss (MAE): 0.0542, validation MSE: -0.5443
2025-05-02 19:37:57 [INFO]: Epoch 337 - training loss (MAE): 0.0547, validation MSE: -0.5462
2025-05-02 19:38:12 [INFO]: Epoch 338 - training loss (MAE): 0.0548, validation MSE: -0.5480
2025-05-02 19:38:28 [INFO]: Epoch 339 - training loss (MAE): 0.0544, validation MSE: -0.5498
2025-05-02 19:38:43 [INFO]: Epoch 340 - training loss (MAE): 0.0544, validation MSE: -0.5389
2025-05-02 19:38:58 [INFO]: Epoch 341 - training loss (MAE): 0.0542, validation MSE: -0.5333
2025-05-02 19:39:14 [INFO]: Epoch 342 - training loss (MAE): 0.0545, validation MSE: -0.5317
2025-05-02 19:39:29 [INFO]: Epoch 343 - training loss (MAE): 0.0544, validation MSE: -0.5399
2025-05-02 19:39:44 [INFO]: Epoch 344 - training loss (MAE): 0.0542, validation MSE: -0.5478
2025-05-02 19:40:00 [INFO]: Epoch 345 - training loss (MAE): 0.0539, validation MSE: -0.5295
2025-05-02 19:40:15 [INFO]: Epoch 346 - training loss (MAE): 0.0550, validation MSE: -0.5253
2025-05-02 19:40:31 [INFO]: Epoch 347 - training loss (MAE): 0.0542, validation MSE: -0.5289
2025-05-02 19:40:46 [INFO]: Epoch 348 - training loss (MAE): 0.0540, validation MSE: -0.5366
2025-05-02 19:41:01 [INFO]: Epoch 349 - training loss (MAE): 0.0541, validation MSE: -0.5137
2025-05-02 19:41:16 [INFO]: Epoch 350 - training loss (MAE): 0.0550, validation MSE: -0.4883
2025-05-02 19:41:32 [INFO]: Epoch 351 - training loss (MAE): 0.0551, validation MSE: -0.5354
2025-05-02 19:41:47 [INFO]: Epoch 352 - training loss (MAE): 0.0539, validation MSE: -0.5178
2025-05-02 19:42:02 [INFO]: Epoch 353 - training loss (MAE): 0.0546, validation MSE: -0.5275
2025-05-02 19:42:17 [INFO]: Epoch 354 - training loss (MAE): 0.0544, validation MSE: -0.5379
2025-05-02 19:42:33 [INFO]: Epoch 355 - training loss (MAE): 0.0542, validation MSE: -0.5330
2025-05-02 19:42:48 [INFO]: Epoch 356 - training loss (MAE): 0.0545, validation MSE: -0.5272
2025-05-02 19:43:03 [INFO]: Epoch 357 - training loss (MAE): 0.0544, validation MSE: -0.5409
2025-05-02 19:43:19 [INFO]: Epoch 358 - training loss (MAE): 0.0538, validation MSE: -0.5381
2025-05-02 19:43:34 [INFO]: Epoch 359 - training loss (MAE): 0.0540, validation MSE: -0.5375
2025-05-02 19:43:49 [INFO]: Epoch 360 - training loss (MAE): 0.0537, validation MSE: -0.5487
2025-05-02 19:44:04 [INFO]: Epoch 361 - training loss (MAE): 0.0542, validation MSE: -0.5614
2025-05-02 19:44:20 [INFO]: Epoch 362 - training loss (MAE): 0.0537, validation MSE: -0.5347
2025-05-02 19:44:35 [INFO]: Epoch 363 - training loss (MAE): 0.0540, validation MSE: -0.5351
2025-05-02 19:44:50 [INFO]: Epoch 364 - training loss (MAE): 0.0541, validation MSE: -0.5290
2025-05-02 19:45:05 [INFO]: Epoch 365 - training loss (MAE): 0.0537, validation MSE: -0.5531
2025-05-02 19:45:21 [INFO]: Epoch 366 - training loss (MAE): 0.0541, validation MSE: -0.5471
2025-05-02 19:45:37 [INFO]: Epoch 367 - training loss (MAE): 0.0535, validation MSE: -0.5507
2025-05-02 19:45:52 [INFO]: Epoch 368 - training loss (MAE): 0.0538, validation MSE: -0.5261
2025-05-02 19:46:07 [INFO]: Epoch 369 - training loss (MAE): 0.0538, validation MSE: -0.5491
2025-05-02 19:46:22 [INFO]: Epoch 370 - training loss (MAE): 0.0541, validation MSE: -0.5373
2025-05-02 19:46:37 [INFO]: Epoch 371 - training loss (MAE): 0.0534, validation MSE: -0.5082
2025-05-02 19:46:53 [INFO]: Epoch 372 - training loss (MAE): 0.0538, validation MSE: -0.5432
2025-05-02 19:47:08 [INFO]: Epoch 373 - training loss (MAE): 0.0533, validation MSE: -0.5327
2025-05-02 19:47:23 [INFO]: Epoch 374 - training loss (MAE): 0.0536, validation MSE: -0.5389
2025-05-02 19:47:38 [INFO]: Epoch 375 - training loss (MAE): 0.0534, validation MSE: -0.5463
2025-05-02 19:47:54 [INFO]: Epoch 376 - training loss (MAE): 0.0534, validation MSE: -0.5330
2025-05-02 19:48:09 [INFO]: Epoch 377 - training loss (MAE): 0.0538, validation MSE: -0.5315
2025-05-02 19:48:24 [INFO]: Epoch 378 - training loss (MAE): 0.0534, validation MSE: -0.5385
2025-05-02 19:48:40 [INFO]: Epoch 379 - training loss (MAE): 0.0534, validation MSE: -0.5200
2025-05-02 19:48:55 [INFO]: Epoch 380 - training loss (MAE): 0.0534, validation MSE: -0.5313
2025-05-02 19:49:10 [INFO]: Epoch 381 - training loss (MAE): 0.0533, validation MSE: -0.5269
2025-05-02 19:49:25 [INFO]: Epoch 382 - training loss (MAE): 0.0537, validation MSE: -0.5375
2025-05-02 19:49:41 [INFO]: Epoch 383 - training loss (MAE): 0.0535, validation MSE: -0.5325
2025-05-02 19:49:56 [INFO]: Epoch 384 - training loss (MAE): 0.0536, validation MSE: -0.5263
2025-05-02 19:50:11 [INFO]: Epoch 385 - training loss (MAE): 0.0536, validation MSE: -0.5278
2025-05-02 19:50:26 [INFO]: Epoch 386 - training loss (MAE): 0.0533, validation MSE: -0.5333
2025-05-02 19:50:41 [INFO]: Epoch 387 - training loss (MAE): 0.0536, validation MSE: -0.5461
2025-05-02 19:50:57 [INFO]: Epoch 388 - training loss (MAE): 0.0535, validation MSE: -0.5284
2025-05-02 19:51:12 [INFO]: Epoch 389 - training loss (MAE): 0.0527, validation MSE: -0.5335
2025-05-02 19:51:27 [INFO]: Epoch 390 - training loss (MAE): 0.0531, validation MSE: -0.5179
2025-05-02 19:51:43 [INFO]: Epoch 391 - training loss (MAE): 0.0534, validation MSE: -0.5441
2025-05-02 19:51:58 [INFO]: Epoch 392 - training loss (MAE): 0.0532, validation MSE: -0.5243
2025-05-02 19:52:14 [INFO]: Epoch 393 - training loss (MAE): 0.0528, validation MSE: -0.5480
2025-05-02 19:52:29 [INFO]: Epoch 394 - training loss (MAE): 0.0529, validation MSE: -0.5469
2025-05-02 19:52:44 [INFO]: Epoch 395 - training loss (MAE): 0.0527, validation MSE: -0.5549
2025-05-02 19:53:00 [INFO]: Epoch 396 - training loss (MAE): 0.0532, validation MSE: -0.5378
2025-05-02 19:53:15 [INFO]: Epoch 397 - training loss (MAE): 0.0526, validation MSE: -0.5409
2025-05-02 19:53:30 [INFO]: Epoch 398 - training loss (MAE): 0.0530, validation MSE: -0.5107
2025-05-02 19:53:45 [INFO]: Epoch 399 - training loss (MAE): 0.0529, validation MSE: -0.5457
2025-05-02 19:54:01 [INFO]: Epoch 400 - training loss (MAE): 0.0535, validation MSE: -0.5253
2025-05-02 19:54:16 [INFO]: Epoch 401 - training loss (MAE): 0.0528, validation MSE: -0.5328
2025-05-02 19:54:31 [INFO]: Epoch 402 - training loss (MAE): 0.0528, validation MSE: -0.5543
2025-05-02 19:54:46 [INFO]: Epoch 403 - training loss (MAE): 0.0524, validation MSE: -0.5465
2025-05-02 19:55:02 [INFO]: Epoch 404 - training loss (MAE): 0.0526, validation MSE: -0.5344
2025-05-02 19:55:17 [INFO]: Epoch 405 - training loss (MAE): 0.0527, validation MSE: -0.5290
2025-05-02 19:55:32 [INFO]: Epoch 406 - training loss (MAE): 0.0532, validation MSE: -0.5321
2025-05-02 19:55:47 [INFO]: Epoch 407 - training loss (MAE): 0.0529, validation MSE: -0.5335
2025-05-02 19:56:03 [INFO]: Epoch 408 - training loss (MAE): 0.0525, validation MSE: -0.5217
2025-05-02 19:56:18 [INFO]: Epoch 409 - training loss (MAE): 0.0523, validation MSE: -0.5368
2025-05-02 19:56:33 [INFO]: Epoch 410 - training loss (MAE): 0.0530, validation MSE: -0.5322
2025-05-02 19:56:48 [INFO]: Epoch 411 - training loss (MAE): 0.0528, validation MSE: -0.5324
2025-05-02 19:57:03 [INFO]: Epoch 412 - training loss (MAE): 0.0525, validation MSE: -0.5357
2025-05-02 19:57:19 [INFO]: Epoch 413 - training loss (MAE): 0.0524, validation MSE: -0.5344
2025-05-02 19:57:34 [INFO]: Epoch 414 - training loss (MAE): 0.0526, validation MSE: -0.5326
2025-05-02 19:57:49 [INFO]: Epoch 415 - training loss (MAE): 0.0523, validation MSE: -0.5203
2025-05-02 19:58:05 [INFO]: Epoch 416 - training loss (MAE): 0.0522, validation MSE: -0.5383
2025-05-02 19:58:20 [INFO]: Epoch 417 - training loss (MAE): 0.0528, validation MSE: -0.5316
2025-05-02 19:58:35 [INFO]: Epoch 418 - training loss (MAE): 0.0525, validation MSE: -0.5446
2025-05-02 19:58:50 [INFO]: Epoch 419 - training loss (MAE): 0.0528, validation MSE: -0.5272
2025-05-02 19:59:06 [INFO]: Epoch 420 - training loss (MAE): 0.0520, validation MSE: -0.5430
2025-05-02 19:59:21 [INFO]: Epoch 421 - training loss (MAE): 0.0524, validation MSE: -0.5242
2025-05-02 19:59:36 [INFO]: Epoch 422 - training loss (MAE): 0.0521, validation MSE: -0.5375
2025-05-02 19:59:51 [INFO]: Epoch 423 - training loss (MAE): 0.0517, validation MSE: -0.5238
2025-05-02 20:00:07 [INFO]: Epoch 424 - training loss (MAE): 0.0521, validation MSE: -0.5201
2025-05-02 20:00:22 [INFO]: Epoch 425 - training loss (MAE): 0.0516, validation MSE: -0.5227
2025-05-02 20:00:37 [INFO]: Epoch 426 - training loss (MAE): 0.0525, validation MSE: -0.5376
2025-05-02 20:00:52 [INFO]: Epoch 427 - training loss (MAE): 0.0522, validation MSE: -0.5253
2025-05-02 20:01:08 [INFO]: Epoch 428 - training loss (MAE): 0.0517, validation MSE: -0.5336
2025-05-02 20:01:23 [INFO]: Epoch 429 - training loss (MAE): 0.0522, validation MSE: -0.5252
2025-05-02 20:01:39 [INFO]: Epoch 430 - training loss (MAE): 0.0519, validation MSE: -0.5277
2025-05-02 20:01:54 [INFO]: Epoch 431 - training loss (MAE): 0.0520, validation MSE: -0.5313
2025-05-02 20:02:09 [INFO]: Epoch 432 - training loss (MAE): 0.0522, validation MSE: -0.5185
2025-05-02 20:02:24 [INFO]: Epoch 433 - training loss (MAE): 0.0517, validation MSE: -0.5230
2025-05-02 20:02:40 [INFO]: Epoch 434 - training loss (MAE): 0.0522, validation MSE: -0.5195
2025-05-02 20:02:55 [INFO]: Epoch 435 - training loss (MAE): 0.0519, validation MSE: -0.5125
2025-05-02 20:03:10 [INFO]: Epoch 436 - training loss (MAE): 0.0522, validation MSE: -0.5168
2025-05-02 20:03:26 [INFO]: Epoch 437 - training loss (MAE): 0.0517, validation MSE: -0.4995
2025-05-02 20:03:41 [INFO]: Epoch 438 - training loss (MAE): 0.0522, validation MSE: -0.4588
2025-05-02 20:03:56 [INFO]: Epoch 439 - training loss (MAE): 0.0521, validation MSE: -0.4703
2025-05-02 20:04:12 [INFO]: Epoch 440 - training loss (MAE): 0.0516, validation MSE: -0.4865
2025-05-02 20:04:27 [INFO]: Epoch 441 - training loss (MAE): 0.0518, validation MSE: -0.5055
2025-05-02 20:04:42 [INFO]: Epoch 442 - training loss (MAE): 0.0512, validation MSE: -0.5150
2025-05-02 20:04:58 [INFO]: Epoch 443 - training loss (MAE): 0.0512, validation MSE: -0.5137
2025-05-02 20:05:14 [INFO]: Epoch 444 - training loss (MAE): 0.0521, validation MSE: -0.5110
2025-05-02 20:05:29 [INFO]: Epoch 445 - training loss (MAE): 0.0512, validation MSE: -0.5124
2025-05-02 20:05:44 [INFO]: Epoch 446 - training loss (MAE): 0.0513, validation MSE: -0.5184
2025-05-02 20:06:00 [INFO]: Epoch 447 - training loss (MAE): 0.0514, validation MSE: -0.5070
2025-05-02 20:06:15 [INFO]: Epoch 448 - training loss (MAE): 0.0508, validation MSE: -0.5167
2025-05-02 20:06:30 [INFO]: Epoch 449 - training loss (MAE): 0.0514, validation MSE: -0.5295
2025-05-02 20:06:45 [INFO]: Epoch 450 - training loss (MAE): 0.0515, validation MSE: -0.5227
2025-05-02 20:07:01 [INFO]: Epoch 451 - training loss (MAE): 0.0509, validation MSE: -0.5122
2025-05-02 20:07:16 [INFO]: Epoch 452 - training loss (MAE): 0.0521, validation MSE: -0.5220
2025-05-02 20:07:31 [INFO]: Epoch 453 - training loss (MAE): 0.0514, validation MSE: -0.5177
2025-05-02 20:07:46 [INFO]: Epoch 454 - training loss (MAE): 0.0515, validation MSE: -0.5083
2025-05-02 20:08:01 [INFO]: Epoch 455 - training loss (MAE): 0.0520, validation MSE: -0.5073
2025-05-02 20:08:16 [INFO]: Epoch 456 - training loss (MAE): 0.0510, validation MSE: -0.5129
2025-05-02 20:08:32 [INFO]: Epoch 457 - training loss (MAE): 0.0508, validation MSE: -0.5048
2025-05-02 20:08:47 [INFO]: Epoch 458 - training loss (MAE): 0.0509, validation MSE: -0.5120
2025-05-02 20:09:02 [INFO]: Epoch 459 - training loss (MAE): 0.0506, validation MSE: -0.5143
2025-05-02 20:09:18 [INFO]: Epoch 460 - training loss (MAE): 0.0510, validation MSE: -0.5119
2025-05-02 20:09:33 [INFO]: Epoch 461 - training loss (MAE): 0.0511, validation MSE: -0.5163
2025-05-02 20:09:48 [INFO]: Epoch 462 - training loss (MAE): 0.0505, validation MSE: -0.5106
2025-05-02 20:10:03 [INFO]: Epoch 463 - training loss (MAE): 0.0505, validation MSE: -0.5044
2025-05-02 20:10:19 [INFO]: Epoch 464 - training loss (MAE): 0.0508, validation MSE: -0.4900
2025-05-02 20:10:34 [INFO]: Epoch 465 - training loss (MAE): 0.0503, validation MSE: -0.5166
2025-05-02 20:10:49 [INFO]: Epoch 466 - training loss (MAE): 0.0508, validation MSE: -0.5076
2025-05-02 20:11:04 [INFO]: Epoch 467 - training loss (MAE): 0.0513, validation MSE: -0.5144
2025-05-02 20:11:20 [INFO]: Epoch 468 - training loss (MAE): 0.0509, validation MSE: -0.5111
2025-05-02 20:11:35 [INFO]: Epoch 469 - training loss (MAE): 0.0514, validation MSE: -0.5133
2025-05-02 20:11:50 [INFO]: Epoch 470 - training loss (MAE): 0.0510, validation MSE: -0.5140
2025-05-02 20:12:05 [INFO]: Epoch 471 - training loss (MAE): 0.0503, validation MSE: -0.5146
2025-05-02 20:12:20 [INFO]: Epoch 472 - training loss (MAE): 0.0501, validation MSE: -0.5208
2025-05-02 20:12:36 [INFO]: Epoch 473 - training loss (MAE): 0.0507, validation MSE: -0.5130
2025-05-02 20:12:51 [INFO]: Epoch 474 - training loss (MAE): 0.0502, validation MSE: -0.5056
2025-05-02 20:13:07 [INFO]: Epoch 475 - training loss (MAE): 0.0504, validation MSE: -0.5081
2025-05-02 20:13:22 [INFO]: Epoch 476 - training loss (MAE): 0.0504, validation MSE: -0.5064
2025-05-02 20:13:37 [INFO]: Epoch 477 - training loss (MAE): 0.0507, validation MSE: -0.5138
2025-05-02 20:13:52 [INFO]: Epoch 478 - training loss (MAE): 0.0501, validation MSE: -0.5167
2025-05-02 20:14:08 [INFO]: Epoch 479 - training loss (MAE): 0.0517, validation MSE: -0.5151
2025-05-02 20:14:23 [INFO]: Epoch 480 - training loss (MAE): 0.0505, validation MSE: -0.5127
2025-05-02 20:14:38 [INFO]: Epoch 481 - training loss (MAE): 0.0501, validation MSE: -0.5322
2025-05-02 20:14:54 [INFO]: Epoch 482 - training loss (MAE): 0.0503, validation MSE: -0.5178
2025-05-02 20:15:09 [INFO]: Epoch 483 - training loss (MAE): 0.0503, validation MSE: -0.5319
2025-05-02 20:15:24 [INFO]: Epoch 484 - training loss (MAE): 0.0501, validation MSE: -0.5125
2025-05-02 20:15:39 [INFO]: Epoch 485 - training loss (MAE): 0.0500, validation MSE: -0.5275
2025-05-02 20:15:54 [INFO]: Epoch 486 - training loss (MAE): 0.0498, validation MSE: -0.5082
2025-05-02 20:16:10 [INFO]: Epoch 487 - training loss (MAE): 0.0502, validation MSE: -0.5104
2025-05-02 20:16:25 [INFO]: Epoch 488 - training loss (MAE): 0.0500, validation MSE: -0.5095
2025-05-02 20:16:40 [INFO]: Epoch 489 - training loss (MAE): 0.0505, validation MSE: -0.5186
2025-05-02 20:16:55 [INFO]: Epoch 490 - training loss (MAE): 0.0501, validation MSE: -0.5122
2025-05-02 20:17:11 [INFO]: Epoch 491 - training loss (MAE): 0.0497, validation MSE: -0.5229
2025-05-02 20:17:26 [INFO]: Epoch 492 - training loss (MAE): 0.0499, validation MSE: -0.5106
2025-05-02 20:17:42 [INFO]: Epoch 493 - training loss (MAE): 0.0495, validation MSE: -0.5132
2025-05-02 20:17:57 [INFO]: Epoch 494 - training loss (MAE): 0.0494, validation MSE: -0.5027
2025-05-02 20:18:12 [INFO]: Epoch 495 - training loss (MAE): 0.0501, validation MSE: -0.4991
2025-05-02 20:18:28 [INFO]: Epoch 496 - training loss (MAE): 0.0496, validation MSE: -0.5120
2025-05-02 20:18:43 [INFO]: Epoch 497 - training loss (MAE): 0.0500, validation MSE: -0.4836
2025-05-02 20:18:58 [INFO]: Epoch 498 - training loss (MAE): 0.0497, validation MSE: -0.4903
2025-05-02 20:19:13 [INFO]: Epoch 499 - training loss (MAE): 0.0492, validation MSE: -0.5058
2025-05-02 20:19:29 [INFO]: Epoch 500 - training loss (MAE): 0.0498, validation MSE: -0.5075
2025-05-02 20:19:29 [INFO]: Finished training. The best model is from epoch#219.
2025-05-02 20:19:29 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_2.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-02 20:19:29 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_2.pypots
Fold 2 metrics: MAE: 0.281, MSE: 0.342, MRE: 0.387
Fold 2 metrics: MAE: 0.281, MSE: 0.342, MRE: 0.387
Training fold 3/10
2025-05-02 20:19:29 [INFO]: No given device, using default device: cuda
2025-05-02 20:19:29 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-02 20:19:29 [INFO]: Using customized MAE as the training loss function.
2025-05-02 20:19:29 [INFO]: Using customized MSE as the validation metric function.
2025-05-02 20:19:29 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-02 20:19:44 [INFO]: Epoch 001 - training loss (MAE): 0.8903, validation MSE: -0.4891
2025-05-02 20:19:59 [INFO]: Epoch 002 - training loss (MAE): 0.3819, validation MSE: -0.5280
2025-05-02 20:20:14 [INFO]: Epoch 003 - training loss (MAE): 0.3243, validation MSE: -0.5967
2025-05-02 20:20:30 [INFO]: Epoch 004 - training loss (MAE): 0.2919, validation MSE: -0.6056
2025-05-02 20:20:45 [INFO]: Epoch 005 - training loss (MAE): 0.2652, validation MSE: -0.6692
2025-05-02 20:21:00 [INFO]: Epoch 006 - training loss (MAE): 0.2380, validation MSE: -0.7285
2025-05-02 20:21:15 [INFO]: Epoch 007 - training loss (MAE): 0.2144, validation MSE: -0.7416
2025-05-02 20:21:30 [INFO]: Epoch 008 - training loss (MAE): 0.1978, validation MSE: -0.7379
2025-05-02 20:21:45 [INFO]: Epoch 009 - training loss (MAE): 0.1825, validation MSE: -0.7437
2025-05-02 20:22:01 [INFO]: Epoch 010 - training loss (MAE): 0.1693, validation MSE: -0.7646
2025-05-02 20:22:16 [INFO]: Epoch 011 - training loss (MAE): 0.1595, validation MSE: -0.7833
2025-05-02 20:22:31 [INFO]: Epoch 012 - training loss (MAE): 0.1506, validation MSE: -0.7743
2025-05-02 20:22:46 [INFO]: Epoch 013 - training loss (MAE): 0.1458, validation MSE: -0.7613
2025-05-02 20:23:01 [INFO]: Epoch 014 - training loss (MAE): 0.1396, validation MSE: -0.7838
2025-05-02 20:23:16 [INFO]: Epoch 015 - training loss (MAE): 0.1347, validation MSE: -0.7884
2025-05-02 20:23:31 [INFO]: Epoch 016 - training loss (MAE): 0.1306, validation MSE: -0.7712
2025-05-02 20:23:47 [INFO]: Epoch 017 - training loss (MAE): 0.1271, validation MSE: -0.7568
2025-05-02 20:24:02 [INFO]: Epoch 018 - training loss (MAE): 0.1242, validation MSE: -0.8215
2025-05-02 20:24:17 [INFO]: Epoch 019 - training loss (MAE): 0.1218, validation MSE: -0.7922
2025-05-02 20:24:33 [INFO]: Epoch 020 - training loss (MAE): 0.1184, validation MSE: -0.7987
2025-05-02 20:24:48 [INFO]: Epoch 021 - training loss (MAE): 0.1158, validation MSE: -0.7840
2025-05-02 20:25:03 [INFO]: Epoch 022 - training loss (MAE): 0.1136, validation MSE: -0.7906
2025-05-02 20:25:19 [INFO]: Epoch 023 - training loss (MAE): 0.1114, validation MSE: -0.7777
2025-05-02 20:25:34 [INFO]: Epoch 024 - training loss (MAE): 0.1091, validation MSE: -0.8058
2025-05-02 20:25:49 [INFO]: Epoch 025 - training loss (MAE): 0.1074, validation MSE: -0.7989
2025-05-02 20:26:04 [INFO]: Epoch 026 - training loss (MAE): 0.1059, validation MSE: -0.8096
2025-05-02 20:26:19 [INFO]: Epoch 027 - training loss (MAE): 0.1045, validation MSE: -0.8058
2025-05-02 20:26:35 [INFO]: Epoch 028 - training loss (MAE): 0.1022, validation MSE: -0.8151
2025-05-02 20:26:50 [INFO]: Epoch 029 - training loss (MAE): 0.1010, validation MSE: -0.7934
2025-05-02 20:27:05 [INFO]: Epoch 030 - training loss (MAE): 0.0992, validation MSE: -0.7853
2025-05-02 20:27:21 [INFO]: Epoch 031 - training loss (MAE): 0.0980, validation MSE: -0.7931
2025-05-02 20:27:36 [INFO]: Epoch 032 - training loss (MAE): 0.0967, validation MSE: -0.7815
2025-05-02 20:27:51 [INFO]: Epoch 033 - training loss (MAE): 0.0961, validation MSE: -0.7708
2025-05-02 20:28:07 [INFO]: Epoch 034 - training loss (MAE): 0.0947, validation MSE: -0.7957
2025-05-02 20:28:22 [INFO]: Epoch 035 - training loss (MAE): 0.0934, validation MSE: -0.7873
2025-05-02 20:28:37 [INFO]: Epoch 036 - training loss (MAE): 0.0932, validation MSE: -0.8096
2025-05-02 20:28:52 [INFO]: Epoch 037 - training loss (MAE): 0.0915, validation MSE: -0.8086
2025-05-02 20:29:08 [INFO]: Epoch 038 - training loss (MAE): 0.0902, validation MSE: -0.7881
2025-05-02 20:29:23 [INFO]: Epoch 039 - training loss (MAE): 0.0905, validation MSE: -0.7938
2025-05-02 20:29:38 [INFO]: Epoch 040 - training loss (MAE): 0.0892, validation MSE: -0.8086
2025-05-02 20:29:53 [INFO]: Epoch 041 - training loss (MAE): 0.0895, validation MSE: -0.7838
2025-05-02 20:30:08 [INFO]: Epoch 042 - training loss (MAE): 0.0865, validation MSE: -0.7838
2025-05-02 20:30:24 [INFO]: Epoch 043 - training loss (MAE): 0.0874, validation MSE: -0.7830
2025-05-02 20:30:39 [INFO]: Epoch 044 - training loss (MAE): 0.0862, validation MSE: -0.8025
2025-05-02 20:30:54 [INFO]: Epoch 045 - training loss (MAE): 0.0853, validation MSE: -0.8027
2025-05-02 20:31:09 [INFO]: Epoch 046 - training loss (MAE): 0.0862, validation MSE: -0.8130
2025-05-02 20:31:24 [INFO]: Epoch 047 - training loss (MAE): 0.0844, validation MSE: -0.7979
2025-05-02 20:31:40 [INFO]: Epoch 048 - training loss (MAE): 0.0840, validation MSE: -0.7937
2025-05-02 20:31:55 [INFO]: Epoch 049 - training loss (MAE): 0.0827, validation MSE: -0.7774
2025-05-02 20:32:10 [INFO]: Epoch 050 - training loss (MAE): 0.0830, validation MSE: -0.8122
2025-05-02 20:32:26 [INFO]: Epoch 051 - training loss (MAE): 0.0827, validation MSE: -0.7764
2025-05-02 20:32:41 [INFO]: Epoch 052 - training loss (MAE): 0.0817, validation MSE: -0.7863
2025-05-02 20:32:56 [INFO]: Epoch 053 - training loss (MAE): 0.0817, validation MSE: -0.7695
2025-05-02 20:33:11 [INFO]: Epoch 054 - training loss (MAE): 0.0820, validation MSE: -0.7792
2025-05-02 20:33:26 [INFO]: Epoch 055 - training loss (MAE): 0.0805, validation MSE: -0.7674
2025-05-02 20:33:42 [INFO]: Epoch 056 - training loss (MAE): 0.0801, validation MSE: -0.7840
2025-05-02 20:33:57 [INFO]: Epoch 057 - training loss (MAE): 0.0796, validation MSE: -0.7825
2025-05-02 20:34:12 [INFO]: Epoch 058 - training loss (MAE): 0.0797, validation MSE: -0.8218
2025-05-02 20:34:27 [INFO]: Epoch 059 - training loss (MAE): 0.0780, validation MSE: -0.7755
2025-05-02 20:34:42 [INFO]: Epoch 060 - training loss (MAE): 0.0785, validation MSE: -0.7667
2025-05-02 20:34:58 [INFO]: Epoch 061 - training loss (MAE): 0.0779, validation MSE: -0.7517
2025-05-02 20:35:13 [INFO]: Epoch 062 - training loss (MAE): 0.0785, validation MSE: -0.7989
2025-05-02 20:35:28 [INFO]: Epoch 063 - training loss (MAE): 0.0775, validation MSE: -0.7995
2025-05-02 20:35:44 [INFO]: Epoch 064 - training loss (MAE): 0.0766, validation MSE: -0.7927
2025-05-02 20:35:59 [INFO]: Epoch 065 - training loss (MAE): 0.0765, validation MSE: -0.7719
2025-05-02 20:36:14 [INFO]: Epoch 066 - training loss (MAE): 0.0768, validation MSE: -0.7615
2025-05-02 20:36:29 [INFO]: Epoch 067 - training loss (MAE): 0.0763, validation MSE: -0.8130
2025-05-02 20:36:44 [INFO]: Epoch 068 - training loss (MAE): 0.0756, validation MSE: -0.7764
2025-05-02 20:37:00 [INFO]: Epoch 069 - training loss (MAE): 0.0762, validation MSE: -0.7876
2025-05-02 20:37:15 [INFO]: Epoch 070 - training loss (MAE): 0.0751, validation MSE: -0.7648
2025-05-02 20:37:30 [INFO]: Epoch 071 - training loss (MAE): 0.0755, validation MSE: -0.7826
2025-05-02 20:37:45 [INFO]: Epoch 072 - training loss (MAE): 0.0744, validation MSE: -0.7866
2025-05-02 20:38:00 [INFO]: Epoch 073 - training loss (MAE): 0.0752, validation MSE: -0.8012
2025-05-02 20:38:16 [INFO]: Epoch 074 - training loss (MAE): 0.0747, validation MSE: -0.8184
2025-05-02 20:38:32 [INFO]: Epoch 075 - training loss (MAE): 0.0740, validation MSE: -0.7908
2025-05-02 20:38:47 [INFO]: Epoch 076 - training loss (MAE): 0.0739, validation MSE: -0.8014
2025-05-02 20:39:02 [INFO]: Epoch 077 - training loss (MAE): 0.0738, validation MSE: -0.7684
2025-05-02 20:39:17 [INFO]: Epoch 078 - training loss (MAE): 0.0737, validation MSE: -0.8051
2025-05-02 20:39:33 [INFO]: Epoch 079 - training loss (MAE): 0.0732, validation MSE: -0.7748
2025-05-02 20:39:48 [INFO]: Epoch 080 - training loss (MAE): 0.0730, validation MSE: -0.7755
2025-05-02 20:40:04 [INFO]: Epoch 081 - training loss (MAE): 0.0731, validation MSE: -0.8028
2025-05-02 20:40:19 [INFO]: Epoch 082 - training loss (MAE): 0.0726, validation MSE: -0.8047
2025-05-02 20:40:34 [INFO]: Epoch 083 - training loss (MAE): 0.0736, validation MSE: -0.7285
2025-05-02 20:40:49 [INFO]: Epoch 084 - training loss (MAE): 0.0727, validation MSE: -0.7531
2025-05-02 20:41:05 [INFO]: Epoch 085 - training loss (MAE): 0.0721, validation MSE: -0.7847
2025-05-02 20:41:20 [INFO]: Epoch 086 - training loss (MAE): 0.0723, validation MSE: -0.7802
2025-05-02 20:41:35 [INFO]: Epoch 087 - training loss (MAE): 0.0717, validation MSE: -0.7675
2025-05-02 20:41:51 [INFO]: Epoch 088 - training loss (MAE): 0.0718, validation MSE: -0.8015
2025-05-02 20:42:06 [INFO]: Epoch 089 - training loss (MAE): 0.0710, validation MSE: -0.7736
2025-05-02 20:42:21 [INFO]: Epoch 090 - training loss (MAE): 0.0711, validation MSE: -0.7609
2025-05-02 20:42:36 [INFO]: Epoch 091 - training loss (MAE): 0.0713, validation MSE: -0.7653
2025-05-02 20:42:52 [INFO]: Epoch 092 - training loss (MAE): 0.0723, validation MSE: -0.7456
2025-05-02 20:43:07 [INFO]: Epoch 093 - training loss (MAE): 0.0709, validation MSE: -0.7888
2025-05-02 20:43:22 [INFO]: Epoch 094 - training loss (MAE): 0.0701, validation MSE: -0.7651
2025-05-02 20:43:37 [INFO]: Epoch 095 - training loss (MAE): 0.0701, validation MSE: -0.7850
2025-05-02 20:43:52 [INFO]: Epoch 096 - training loss (MAE): 0.0703, validation MSE: -0.7885
2025-05-02 20:44:07 [INFO]: Epoch 097 - training loss (MAE): 0.0699, validation MSE: -0.8097
2025-05-02 20:44:23 [INFO]: Epoch 098 - training loss (MAE): 0.0704, validation MSE: -0.7996
2025-05-02 20:44:38 [INFO]: Epoch 099 - training loss (MAE): 0.0694, validation MSE: -0.8017
2025-05-02 20:44:53 [INFO]: Epoch 100 - training loss (MAE): 0.0694, validation MSE: -0.7462
2025-05-02 20:45:08 [INFO]: Epoch 101 - training loss (MAE): 0.0692, validation MSE: -0.8111
2025-05-02 20:45:24 [INFO]: Epoch 102 - training loss (MAE): 0.0690, validation MSE: -0.7763
2025-05-02 20:45:39 [INFO]: Epoch 103 - training loss (MAE): 0.0694, validation MSE: -0.7704
2025-05-02 20:45:54 [INFO]: Epoch 104 - training loss (MAE): 0.0685, validation MSE: -0.7940
2025-05-02 20:46:09 [INFO]: Epoch 105 - training loss (MAE): 0.0690, validation MSE: -0.7592
2025-05-02 20:46:24 [INFO]: Epoch 106 - training loss (MAE): 0.0691, validation MSE: -0.8362
2025-05-02 20:46:40 [INFO]: Epoch 107 - training loss (MAE): 0.0682, validation MSE: -0.7837
2025-05-02 20:46:55 [INFO]: Epoch 108 - training loss (MAE): 0.0677, validation MSE: -0.7803
2025-05-02 20:47:10 [INFO]: Epoch 109 - training loss (MAE): 0.0681, validation MSE: -0.7777
2025-05-02 20:47:25 [INFO]: Epoch 110 - training loss (MAE): 0.0694, validation MSE: -0.7299
2025-05-02 20:47:40 [INFO]: Epoch 111 - training loss (MAE): 0.0686, validation MSE: -0.7716
2025-05-02 20:47:55 [INFO]: Epoch 112 - training loss (MAE): 0.0684, validation MSE: -0.7608
2025-05-02 20:48:10 [INFO]: Epoch 113 - training loss (MAE): 0.0684, validation MSE: -0.7869
2025-05-02 20:48:26 [INFO]: Epoch 114 - training loss (MAE): 0.0674, validation MSE: -0.7784
2025-05-02 20:48:41 [INFO]: Epoch 115 - training loss (MAE): 0.0675, validation MSE: -0.8072
2025-05-02 20:48:56 [INFO]: Epoch 116 - training loss (MAE): 0.0677, validation MSE: -0.7940
2025-05-02 20:49:11 [INFO]: Epoch 117 - training loss (MAE): 0.0679, validation MSE: -0.7857
2025-05-02 20:49:26 [INFO]: Epoch 118 - training loss (MAE): 0.0669, validation MSE: -0.7792
2025-05-02 20:49:42 [INFO]: Epoch 119 - training loss (MAE): 0.0677, validation MSE: -0.7648
2025-05-02 20:49:57 [INFO]: Epoch 120 - training loss (MAE): 0.0671, validation MSE: -0.7732
2025-05-02 20:50:12 [INFO]: Epoch 121 - training loss (MAE): 0.0680, validation MSE: -0.7951
2025-05-02 20:50:27 [INFO]: Epoch 122 - training loss (MAE): 0.0668, validation MSE: -0.7865
2025-05-02 20:50:43 [INFO]: Epoch 123 - training loss (MAE): 0.0671, validation MSE: -0.7917
2025-05-02 20:50:58 [INFO]: Epoch 124 - training loss (MAE): 0.0664, validation MSE: -0.7921
2025-05-02 20:51:14 [INFO]: Epoch 125 - training loss (MAE): 0.0669, validation MSE: -0.7616
2025-05-02 20:51:29 [INFO]: Epoch 126 - training loss (MAE): 0.0665, validation MSE: -0.7850
2025-05-02 20:51:44 [INFO]: Epoch 127 - training loss (MAE): 0.0666, validation MSE: -0.7613
2025-05-02 20:51:59 [INFO]: Epoch 128 - training loss (MAE): 0.0660, validation MSE: -0.7515
2025-05-02 20:52:14 [INFO]: Epoch 129 - training loss (MAE): 0.0668, validation MSE: -0.7770
2025-05-02 20:52:29 [INFO]: Epoch 130 - training loss (MAE): 0.0666, validation MSE: -0.7699
2025-05-02 20:52:44 [INFO]: Epoch 131 - training loss (MAE): 0.0653, validation MSE: -0.7791
2025-05-02 20:53:00 [INFO]: Epoch 132 - training loss (MAE): 0.0663, validation MSE: -0.7759
2025-05-02 20:53:15 [INFO]: Epoch 133 - training loss (MAE): 0.0673, validation MSE: -0.7424
2025-05-02 20:53:30 [INFO]: Epoch 134 - training loss (MAE): 0.0653, validation MSE: -0.7220
2025-05-02 20:53:45 [INFO]: Epoch 135 - training loss (MAE): 0.0654, validation MSE: -0.7809
2025-05-02 20:54:00 [INFO]: Epoch 136 - training loss (MAE): 0.0651, validation MSE: -0.7269
2025-05-02 20:54:16 [INFO]: Epoch 137 - training loss (MAE): 0.0657, validation MSE: -0.6926
2025-05-02 20:54:31 [INFO]: Epoch 138 - training loss (MAE): 0.0651, validation MSE: -0.7294
2025-05-02 20:54:47 [INFO]: Epoch 139 - training loss (MAE): 0.0651, validation MSE: -0.7967
2025-05-02 20:55:02 [INFO]: Epoch 140 - training loss (MAE): 0.0656, validation MSE: -0.7536
2025-05-02 20:55:17 [INFO]: Epoch 141 - training loss (MAE): 0.0651, validation MSE: -0.7555
2025-05-02 20:55:32 [INFO]: Epoch 142 - training loss (MAE): 0.0646, validation MSE: -0.7763
2025-05-02 20:55:47 [INFO]: Epoch 143 - training loss (MAE): 0.0646, validation MSE: -0.8029
2025-05-02 20:56:03 [INFO]: Epoch 144 - training loss (MAE): 0.0651, validation MSE: -0.7844
2025-05-02 20:56:18 [INFO]: Epoch 145 - training loss (MAE): 0.0644, validation MSE: -0.7169
2025-05-02 20:56:33 [INFO]: Epoch 146 - training loss (MAE): 0.0647, validation MSE: -0.7984
2025-05-02 20:56:49 [INFO]: Epoch 147 - training loss (MAE): 0.0647, validation MSE: -0.7776
2025-05-02 20:57:04 [INFO]: Epoch 148 - training loss (MAE): 0.0643, validation MSE: -0.7442
2025-05-02 20:57:19 [INFO]: Epoch 149 - training loss (MAE): 0.0643, validation MSE: -0.8177
2025-05-02 20:57:34 [INFO]: Epoch 150 - training loss (MAE): 0.0643, validation MSE: -0.7821
2025-05-02 20:57:50 [INFO]: Epoch 151 - training loss (MAE): 0.0644, validation MSE: -0.8252
2025-05-02 20:58:05 [INFO]: Epoch 152 - training loss (MAE): 0.0646, validation MSE: -0.8201
2025-05-02 20:58:20 [INFO]: Epoch 153 - training loss (MAE): 0.0641, validation MSE: -0.7969
2025-05-02 20:58:35 [INFO]: Epoch 154 - training loss (MAE): 0.0650, validation MSE: -0.7876
2025-05-02 20:58:51 [INFO]: Epoch 155 - training loss (MAE): 0.0640, validation MSE: -0.7825
2025-05-02 20:59:06 [INFO]: Epoch 156 - training loss (MAE): 0.0636, validation MSE: -0.8038
2025-05-02 20:59:21 [INFO]: Epoch 157 - training loss (MAE): 0.0638, validation MSE: -0.7586
2025-05-02 20:59:37 [INFO]: Epoch 158 - training loss (MAE): 0.0642, validation MSE: -0.7911
2025-05-02 20:59:52 [INFO]: Epoch 159 - training loss (MAE): 0.0638, validation MSE: -0.7916
2025-05-02 21:00:07 [INFO]: Epoch 160 - training loss (MAE): 0.0636, validation MSE: -0.7875
2025-05-02 21:00:23 [INFO]: Epoch 161 - training loss (MAE): 0.0640, validation MSE: -0.7844
2025-05-02 21:00:38 [INFO]: Epoch 162 - training loss (MAE): 0.0631, validation MSE: -0.7469
2025-05-02 21:00:53 [INFO]: Epoch 163 - training loss (MAE): 0.0635, validation MSE: -0.7883
2025-05-02 21:01:08 [INFO]: Epoch 164 - training loss (MAE): 0.0632, validation MSE: -0.7957
2025-05-02 21:01:24 [INFO]: Epoch 165 - training loss (MAE): 0.0636, validation MSE: -0.7792
2025-05-02 21:01:39 [INFO]: Epoch 166 - training loss (MAE): 0.0631, validation MSE: -0.7850
2025-05-02 21:01:54 [INFO]: Epoch 167 - training loss (MAE): 0.0630, validation MSE: -0.7824
2025-05-02 21:02:10 [INFO]: Epoch 168 - training loss (MAE): 0.0631, validation MSE: -0.7986
2025-05-02 21:02:25 [INFO]: Epoch 169 - training loss (MAE): 0.0633, validation MSE: -0.7576
2025-05-02 21:02:40 [INFO]: Epoch 170 - training loss (MAE): 0.0626, validation MSE: -0.7492
2025-05-02 21:02:55 [INFO]: Epoch 171 - training loss (MAE): 0.0631, validation MSE: -0.7365
2025-05-02 21:03:10 [INFO]: Epoch 172 - training loss (MAE): 0.0640, validation MSE: -0.7663
2025-05-02 21:03:25 [INFO]: Epoch 173 - training loss (MAE): 0.0630, validation MSE: -0.7630
2025-05-02 21:03:41 [INFO]: Epoch 174 - training loss (MAE): 0.0627, validation MSE: -0.7784
2025-05-02 21:03:56 [INFO]: Epoch 175 - training loss (MAE): 0.0626, validation MSE: -0.7801
2025-05-02 21:04:11 [INFO]: Epoch 176 - training loss (MAE): 0.0627, validation MSE: -0.7687
2025-05-02 21:04:26 [INFO]: Epoch 177 - training loss (MAE): 0.0625, validation MSE: -0.7538
2025-05-02 21:04:42 [INFO]: Epoch 178 - training loss (MAE): 0.0629, validation MSE: -0.7905
2025-05-02 21:04:57 [INFO]: Epoch 179 - training loss (MAE): 0.0626, validation MSE: -0.8007
2025-05-02 21:05:12 [INFO]: Epoch 180 - training loss (MAE): 0.0624, validation MSE: -0.7800
2025-05-02 21:05:27 [INFO]: Epoch 181 - training loss (MAE): 0.0626, validation MSE: -0.7366
2025-05-02 21:05:43 [INFO]: Epoch 182 - training loss (MAE): 0.0622, validation MSE: -0.7763
2025-05-02 21:05:58 [INFO]: Epoch 183 - training loss (MAE): 0.0617, validation MSE: -0.7677
2025-05-02 21:06:13 [INFO]: Epoch 184 - training loss (MAE): 0.0621, validation MSE: -0.7923
2025-05-02 21:06:28 [INFO]: Epoch 185 - training loss (MAE): 0.0616, validation MSE: -0.8144
2025-05-02 21:06:43 [INFO]: Epoch 186 - training loss (MAE): 0.0617, validation MSE: -0.7833
2025-05-02 21:06:59 [INFO]: Epoch 187 - training loss (MAE): 0.0630, validation MSE: -0.8157
2025-05-02 21:07:14 [INFO]: Epoch 188 - training loss (MAE): 0.0618, validation MSE: -0.8066
2025-05-02 21:07:29 [INFO]: Epoch 189 - training loss (MAE): 0.0626, validation MSE: -0.7661
2025-05-02 21:07:45 [INFO]: Epoch 190 - training loss (MAE): 0.0616, validation MSE: -0.7064
2025-05-02 21:08:00 [INFO]: Epoch 191 - training loss (MAE): 0.0615, validation MSE: -0.7665
2025-05-02 21:08:15 [INFO]: Epoch 192 - training loss (MAE): 0.0629, validation MSE: -0.7367
2025-05-02 21:08:30 [INFO]: Epoch 193 - training loss (MAE): 0.0617, validation MSE: -0.7556
2025-05-02 21:08:45 [INFO]: Epoch 194 - training loss (MAE): 0.0611, validation MSE: -0.7538
2025-05-02 21:09:01 [INFO]: Epoch 195 - training loss (MAE): 0.0618, validation MSE: -0.7602
2025-05-02 21:09:16 [INFO]: Epoch 196 - training loss (MAE): 0.0612, validation MSE: -0.7341
2025-05-02 21:09:31 [INFO]: Epoch 197 - training loss (MAE): 0.0617, validation MSE: -0.7257
2025-05-02 21:09:47 [INFO]: Epoch 198 - training loss (MAE): 0.0650, validation MSE: -0.7426
2025-05-02 21:10:02 [INFO]: Epoch 199 - training loss (MAE): 0.0628, validation MSE: -0.7844
2025-05-02 21:10:17 [INFO]: Epoch 200 - training loss (MAE): 0.0621, validation MSE: -0.7641
2025-05-02 21:10:32 [INFO]: Epoch 201 - training loss (MAE): 0.0612, validation MSE: -0.7998
2025-05-02 21:10:47 [INFO]: Epoch 202 - training loss (MAE): 0.0613, validation MSE: -0.8093
2025-05-02 21:11:02 [INFO]: Epoch 203 - training loss (MAE): 0.0610, validation MSE: -0.7879
2025-05-02 21:11:18 [INFO]: Epoch 204 - training loss (MAE): 0.0617, validation MSE: -0.7735
2025-05-02 21:11:33 [INFO]: Epoch 205 - training loss (MAE): 0.0615, validation MSE: -0.6799
2025-05-02 21:11:48 [INFO]: Epoch 206 - training loss (MAE): 0.0617, validation MSE: -0.7416
2025-05-02 21:12:03 [INFO]: Epoch 207 - training loss (MAE): 0.0614, validation MSE: -0.6366
2025-05-02 21:12:18 [INFO]: Epoch 208 - training loss (MAE): 0.0617, validation MSE: -0.7582
2025-05-02 21:12:34 [INFO]: Epoch 209 - training loss (MAE): 0.0611, validation MSE: -0.7453
2025-05-02 21:12:49 [INFO]: Epoch 210 - training loss (MAE): 0.0605, validation MSE: -0.7708
2025-05-02 21:13:04 [INFO]: Epoch 211 - training loss (MAE): 0.0610, validation MSE: -0.7563
2025-05-02 21:13:20 [INFO]: Epoch 212 - training loss (MAE): 0.0608, validation MSE: -0.7937
2025-05-02 21:13:35 [INFO]: Epoch 213 - training loss (MAE): 0.0608, validation MSE: -0.7602
2025-05-02 21:13:50 [INFO]: Epoch 214 - training loss (MAE): 0.0610, validation MSE: -0.7880
2025-05-02 21:14:06 [INFO]: Epoch 215 - training loss (MAE): 0.0611, validation MSE: -0.7203
2025-05-02 21:14:21 [INFO]: Epoch 216 - training loss (MAE): 0.0601, validation MSE: -0.7854
2025-05-02 21:14:36 [INFO]: Epoch 217 - training loss (MAE): 0.0601, validation MSE: -0.7462
2025-05-02 21:14:51 [INFO]: Epoch 218 - training loss (MAE): 0.0610, validation MSE: -0.8138
2025-05-02 21:15:06 [INFO]: Epoch 219 - training loss (MAE): 0.0605, validation MSE: -0.8040
2025-05-02 21:15:21 [INFO]: Epoch 220 - training loss (MAE): 0.0618, validation MSE: -0.7776
2025-05-02 21:15:37 [INFO]: Epoch 221 - training loss (MAE): 0.0616, validation MSE: -0.7930
2025-05-02 21:15:52 [INFO]: Epoch 222 - training loss (MAE): 0.0598, validation MSE: -0.7563
2025-05-02 21:16:07 [INFO]: Epoch 223 - training loss (MAE): 0.0605, validation MSE: -0.7718
2025-05-02 21:16:22 [INFO]: Epoch 224 - training loss (MAE): 0.0597, validation MSE: -0.7553
2025-05-02 21:16:37 [INFO]: Epoch 225 - training loss (MAE): 0.0600, validation MSE: -0.7881
2025-05-02 21:16:52 [INFO]: Epoch 226 - training loss (MAE): 0.0599, validation MSE: -0.7919
2025-05-02 21:17:08 [INFO]: Epoch 227 - training loss (MAE): 0.0595, validation MSE: -0.7805
2025-05-02 21:17:23 [INFO]: Epoch 228 - training loss (MAE): 0.0599, validation MSE: -0.7687
2025-05-02 21:17:38 [INFO]: Epoch 229 - training loss (MAE): 0.0594, validation MSE: -0.7924
2025-05-02 21:17:53 [INFO]: Epoch 230 - training loss (MAE): 0.0594, validation MSE: -0.7781
2025-05-02 21:18:08 [INFO]: Epoch 231 - training loss (MAE): 0.0599, validation MSE: -0.7504
2025-05-02 21:18:23 [INFO]: Epoch 232 - training loss (MAE): 0.0597, validation MSE: -0.7396
2025-05-02 21:18:38 [INFO]: Epoch 233 - training loss (MAE): 0.0600, validation MSE: -0.7790
2025-05-02 21:18:54 [INFO]: Epoch 234 - training loss (MAE): 0.0600, validation MSE: -0.7639
2025-05-02 21:19:09 [INFO]: Epoch 235 - training loss (MAE): 0.0593, validation MSE: -0.8086
2025-05-02 21:19:24 [INFO]: Epoch 236 - training loss (MAE): 0.0596, validation MSE: -0.7934
2025-05-02 21:19:39 [INFO]: Epoch 237 - training loss (MAE): 0.0595, validation MSE: -0.7939
2025-05-02 21:19:54 [INFO]: Epoch 238 - training loss (MAE): 0.0605, validation MSE: -0.7520
2025-05-02 21:20:10 [INFO]: Epoch 239 - training loss (MAE): 0.0596, validation MSE: -0.7568
2025-05-02 21:20:25 [INFO]: Epoch 240 - training loss (MAE): 0.0596, validation MSE: -0.7419
2025-05-02 21:20:40 [INFO]: Epoch 241 - training loss (MAE): 0.0595, validation MSE: -0.8029
2025-05-02 21:20:55 [INFO]: Epoch 242 - training loss (MAE): 0.0597, validation MSE: -0.7323
2025-05-02 21:21:11 [INFO]: Epoch 243 - training loss (MAE): 0.0592, validation MSE: -0.7102
2025-05-02 21:21:26 [INFO]: Epoch 244 - training loss (MAE): 0.0593, validation MSE: -0.7945
2025-05-02 21:21:41 [INFO]: Epoch 245 - training loss (MAE): 0.0592, validation MSE: -0.7852
2025-05-02 21:21:56 [INFO]: Epoch 246 - training loss (MAE): 0.0604, validation MSE: -0.7693
2025-05-02 21:22:12 [INFO]: Epoch 247 - training loss (MAE): 0.0594, validation MSE: -0.7817
2025-05-02 21:22:28 [INFO]: Epoch 248 - training loss (MAE): 0.0596, validation MSE: -0.8043
2025-05-02 21:22:43 [INFO]: Epoch 249 - training loss (MAE): 0.0588, validation MSE: -0.7960
2025-05-02 21:22:58 [INFO]: Epoch 250 - training loss (MAE): 0.0591, validation MSE: -0.7575
2025-05-02 21:23:13 [INFO]: Epoch 251 - training loss (MAE): 0.0594, validation MSE: -0.7951
2025-05-02 21:23:29 [INFO]: Epoch 252 - training loss (MAE): 0.0593, validation MSE: -0.7699
2025-05-02 21:23:44 [INFO]: Epoch 253 - training loss (MAE): 0.0587, validation MSE: -0.7911
2025-05-02 21:23:59 [INFO]: Epoch 254 - training loss (MAE): 0.0586, validation MSE: -0.7609
2025-05-02 21:24:14 [INFO]: Epoch 255 - training loss (MAE): 0.0610, validation MSE: -0.7896
2025-05-02 21:24:30 [INFO]: Epoch 256 - training loss (MAE): 0.0612, validation MSE: -0.7973
2025-05-02 21:24:45 [INFO]: Epoch 257 - training loss (MAE): 0.0592, validation MSE: -0.7966
2025-05-02 21:25:00 [INFO]: Epoch 258 - training loss (MAE): 0.0585, validation MSE: -0.7706
2025-05-02 21:25:15 [INFO]: Epoch 259 - training loss (MAE): 0.0586, validation MSE: -0.8033
2025-05-02 21:25:31 [INFO]: Epoch 260 - training loss (MAE): 0.0590, validation MSE: -0.7942
2025-05-02 21:25:46 [INFO]: Epoch 261 - training loss (MAE): 0.0586, validation MSE: -0.7621
2025-05-02 21:26:01 [INFO]: Epoch 262 - training loss (MAE): 0.0581, validation MSE: -0.7990
2025-05-02 21:26:17 [INFO]: Epoch 263 - training loss (MAE): 0.0578, validation MSE: -0.7550
2025-05-02 21:26:32 [INFO]: Epoch 264 - training loss (MAE): 0.0580, validation MSE: -0.7566
2025-05-02 21:26:47 [INFO]: Epoch 265 - training loss (MAE): 0.0581, validation MSE: -0.7505
2025-05-02 21:27:02 [INFO]: Epoch 266 - training loss (MAE): 0.0585, validation MSE: -0.7576
2025-05-02 21:27:17 [INFO]: Epoch 267 - training loss (MAE): 0.0575, validation MSE: -0.7665
2025-05-02 21:27:32 [INFO]: Epoch 268 - training loss (MAE): 0.0584, validation MSE: -0.7252
2025-05-02 21:27:48 [INFO]: Epoch 269 - training loss (MAE): 0.0579, validation MSE: -0.7985
2025-05-02 21:28:03 [INFO]: Epoch 270 - training loss (MAE): 0.0580, validation MSE: -0.7573
2025-05-02 21:28:18 [INFO]: Epoch 271 - training loss (MAE): 0.0578, validation MSE: -0.7744
2025-05-02 21:28:34 [INFO]: Epoch 272 - training loss (MAE): 0.0577, validation MSE: -0.7127
2025-05-02 21:28:49 [INFO]: Epoch 273 - training loss (MAE): 0.0586, validation MSE: -0.7770
2025-05-02 21:29:04 [INFO]: Epoch 274 - training loss (MAE): 0.0579, validation MSE: -0.7605
2025-05-02 21:29:19 [INFO]: Epoch 275 - training loss (MAE): 0.0579, validation MSE: -0.7553
2025-05-02 21:29:34 [INFO]: Epoch 276 - training loss (MAE): 0.0577, validation MSE: -0.7387
2025-05-02 21:29:50 [INFO]: Epoch 277 - training loss (MAE): 0.0591, validation MSE: -0.8107
2025-05-02 21:30:05 [INFO]: Epoch 278 - training loss (MAE): 0.0577, validation MSE: -0.7983
2025-05-02 21:30:20 [INFO]: Epoch 279 - training loss (MAE): 0.0574, validation MSE: -0.7529
2025-05-02 21:30:35 [INFO]: Epoch 280 - training loss (MAE): 0.0583, validation MSE: -0.7905
2025-05-02 21:30:51 [INFO]: Epoch 281 - training loss (MAE): 0.0582, validation MSE: -0.7899
2025-05-02 21:31:06 [INFO]: Epoch 282 - training loss (MAE): 0.0576, validation MSE: -0.7240
2025-05-02 21:31:21 [INFO]: Epoch 283 - training loss (MAE): 0.0573, validation MSE: -0.7849
2025-05-02 21:31:36 [INFO]: Epoch 284 - training loss (MAE): 0.0574, validation MSE: -0.8149
2025-05-02 21:31:51 [INFO]: Epoch 285 - training loss (MAE): 0.0578, validation MSE: -0.7940
2025-05-02 21:32:07 [INFO]: Epoch 286 - training loss (MAE): 0.0574, validation MSE: -0.8043
2025-05-02 21:32:22 [INFO]: Epoch 287 - training loss (MAE): 0.0575, validation MSE: -0.7387
2025-05-02 21:32:37 [INFO]: Epoch 288 - training loss (MAE): 0.0573, validation MSE: -0.7701
2025-05-02 21:32:53 [INFO]: Epoch 289 - training loss (MAE): 0.0575, validation MSE: -0.7721
2025-05-02 21:33:08 [INFO]: Epoch 290 - training loss (MAE): 0.0570, validation MSE: -0.8090
2025-05-02 21:33:23 [INFO]: Epoch 291 - training loss (MAE): 0.0569, validation MSE: -0.8120
2025-05-02 21:33:38 [INFO]: Epoch 292 - training loss (MAE): 0.0573, validation MSE: -0.7547
2025-05-02 21:33:53 [INFO]: Epoch 293 - training loss (MAE): 0.0573, validation MSE: -0.7701
2025-05-02 21:34:09 [INFO]: Epoch 294 - training loss (MAE): 0.0572, validation MSE: -0.8043
2025-05-02 21:34:24 [INFO]: Epoch 295 - training loss (MAE): 0.0574, validation MSE: -0.7907
2025-05-02 21:34:39 [INFO]: Epoch 296 - training loss (MAE): 0.0577, validation MSE: -0.7727
2025-05-02 21:34:54 [INFO]: Epoch 297 - training loss (MAE): 0.0575, validation MSE: -0.7996
2025-05-02 21:35:09 [INFO]: Epoch 298 - training loss (MAE): 0.0571, validation MSE: -0.7465
2025-05-02 21:35:24 [INFO]: Epoch 299 - training loss (MAE): 0.0572, validation MSE: -0.7858
2025-05-02 21:35:40 [INFO]: Epoch 300 - training loss (MAE): 0.0574, validation MSE: -0.7580
2025-05-02 21:35:55 [INFO]: Epoch 301 - training loss (MAE): 0.0571, validation MSE: -0.7724
2025-05-02 21:36:10 [INFO]: Epoch 302 - training loss (MAE): 0.0573, validation MSE: -0.7943
2025-05-02 21:36:25 [INFO]: Epoch 303 - training loss (MAE): 0.0569, validation MSE: -0.7728
2025-05-02 21:36:41 [INFO]: Epoch 304 - training loss (MAE): 0.0570, validation MSE: -0.7845
2025-05-02 21:36:56 [INFO]: Epoch 305 - training loss (MAE): 0.0566, validation MSE: -0.7716
2025-05-02 21:37:11 [INFO]: Epoch 306 - training loss (MAE): 0.0567, validation MSE: -0.7485
2025-05-02 21:37:27 [INFO]: Epoch 307 - training loss (MAE): 0.0567, validation MSE: -0.7719
2025-05-02 21:37:42 [INFO]: Epoch 308 - training loss (MAE): 0.0567, validation MSE: -0.7687
2025-05-02 21:37:57 [INFO]: Epoch 309 - training loss (MAE): 0.0576, validation MSE: -0.7473
2025-05-02 21:38:12 [INFO]: Epoch 310 - training loss (MAE): 0.0567, validation MSE: -0.7962
2025-05-02 21:38:28 [INFO]: Epoch 311 - training loss (MAE): 0.0563, validation MSE: -0.7898
2025-05-02 21:38:43 [INFO]: Epoch 312 - training loss (MAE): 0.0565, validation MSE: -0.7738
2025-05-02 21:38:58 [INFO]: Epoch 313 - training loss (MAE): 0.0565, validation MSE: -0.7406
2025-05-02 21:39:13 [INFO]: Epoch 314 - training loss (MAE): 0.0563, validation MSE: -0.7452
2025-05-02 21:39:28 [INFO]: Epoch 315 - training loss (MAE): 0.0571, validation MSE: -0.7877
2025-05-02 21:39:44 [INFO]: Epoch 316 - training loss (MAE): 0.0565, validation MSE: -0.7523
2025-05-02 21:40:00 [INFO]: Epoch 317 - training loss (MAE): 0.0557, validation MSE: -0.7823
2025-05-02 21:40:15 [INFO]: Epoch 318 - training loss (MAE): 0.0563, validation MSE: -0.7626
2025-05-02 21:40:30 [INFO]: Epoch 319 - training loss (MAE): 0.0559, validation MSE: -0.8039
2025-05-02 21:40:45 [INFO]: Epoch 320 - training loss (MAE): 0.0563, validation MSE: -0.7592
2025-05-02 21:41:00 [INFO]: Epoch 321 - training loss (MAE): 0.0573, validation MSE: -0.7585
2025-05-02 21:41:15 [INFO]: Epoch 322 - training loss (MAE): 0.0572, validation MSE: -0.8001
2025-05-02 21:41:30 [INFO]: Epoch 323 - training loss (MAE): 0.0564, validation MSE: -0.7696
2025-05-02 21:41:46 [INFO]: Epoch 324 - training loss (MAE): 0.0566, validation MSE: -0.7754
2025-05-02 21:42:01 [INFO]: Epoch 325 - training loss (MAE): 0.0567, validation MSE: -0.7704
2025-05-02 21:42:16 [INFO]: Epoch 326 - training loss (MAE): 0.0565, validation MSE: -0.7407
2025-05-02 21:42:31 [INFO]: Epoch 327 - training loss (MAE): 0.0556, validation MSE: -0.7375
2025-05-02 21:42:47 [INFO]: Epoch 328 - training loss (MAE): 0.0557, validation MSE: -0.7856
2025-05-02 21:43:02 [INFO]: Epoch 329 - training loss (MAE): 0.0561, validation MSE: -0.7875
2025-05-02 21:43:17 [INFO]: Epoch 330 - training loss (MAE): 0.0566, validation MSE: -0.7966
2025-05-02 21:43:33 [INFO]: Epoch 331 - training loss (MAE): 0.0556, validation MSE: -0.7467
2025-05-02 21:43:48 [INFO]: Epoch 332 - training loss (MAE): 0.0559, validation MSE: -0.7788
2025-05-02 21:44:03 [INFO]: Epoch 333 - training loss (MAE): 0.0562, validation MSE: -0.7815
2025-05-02 21:44:18 [INFO]: Epoch 334 - training loss (MAE): 0.0562, validation MSE: -0.7782
2025-05-02 21:44:34 [INFO]: Epoch 335 - training loss (MAE): 0.0553, validation MSE: -0.7769
2025-05-02 21:44:49 [INFO]: Epoch 336 - training loss (MAE): 0.0563, validation MSE: -0.7953
2025-05-02 21:45:04 [INFO]: Epoch 337 - training loss (MAE): 0.0561, validation MSE: -0.7864
2025-05-02 21:45:19 [INFO]: Epoch 338 - training loss (MAE): 0.0559, validation MSE: -0.7717
2025-05-02 21:45:34 [INFO]: Epoch 339 - training loss (MAE): 0.0558, validation MSE: -0.7914
2025-05-02 21:45:50 [INFO]: Epoch 340 - training loss (MAE): 0.0552, validation MSE: -0.8075
2025-05-02 21:46:05 [INFO]: Epoch 341 - training loss (MAE): 0.0558, validation MSE: -0.7722
2025-05-02 21:46:20 [INFO]: Epoch 342 - training loss (MAE): 0.0563, validation MSE: -0.7651
2025-05-02 21:46:35 [INFO]: Epoch 343 - training loss (MAE): 0.0556, validation MSE: -0.7705
2025-05-02 21:46:51 [INFO]: Epoch 344 - training loss (MAE): 0.0554, validation MSE: -0.7975
2025-05-02 21:47:06 [INFO]: Epoch 345 - training loss (MAE): 0.0549, validation MSE: -0.8085
2025-05-02 21:47:21 [INFO]: Epoch 346 - training loss (MAE): 0.0549, validation MSE: -0.7727
2025-05-02 21:47:36 [INFO]: Epoch 347 - training loss (MAE): 0.0554, validation MSE: -0.7891
2025-05-02 21:47:51 [INFO]: Epoch 348 - training loss (MAE): 0.0549, validation MSE: -0.8017
2025-05-02 21:48:06 [INFO]: Epoch 349 - training loss (MAE): 0.0556, validation MSE: -0.7851
2025-05-02 21:48:22 [INFO]: Epoch 350 - training loss (MAE): 0.0556, validation MSE: -0.7805
2025-05-02 21:48:37 [INFO]: Epoch 351 - training loss (MAE): 0.0549, validation MSE: -0.7820
2025-05-02 21:48:52 [INFO]: Epoch 352 - training loss (MAE): 0.0553, validation MSE: -0.7718
2025-05-02 21:49:07 [INFO]: Epoch 353 - training loss (MAE): 0.0545, validation MSE: -0.7603
2025-05-02 21:49:22 [INFO]: Epoch 354 - training loss (MAE): 0.0551, validation MSE: -0.7702
2025-05-02 21:49:38 [INFO]: Epoch 355 - training loss (MAE): 0.0547, validation MSE: -0.7578
2025-05-02 21:49:53 [INFO]: Epoch 356 - training loss (MAE): 0.0551, validation MSE: -0.7761
2025-05-02 21:50:08 [INFO]: Epoch 357 - training loss (MAE): 0.0550, validation MSE: -0.7486
2025-05-02 21:50:23 [INFO]: Epoch 358 - training loss (MAE): 0.0544, validation MSE: -0.7647
2025-05-02 21:50:38 [INFO]: Epoch 359 - training loss (MAE): 0.0549, validation MSE: -0.7769
2025-05-02 21:50:54 [INFO]: Epoch 360 - training loss (MAE): 0.0556, validation MSE: -0.7752
2025-05-02 21:51:09 [INFO]: Epoch 361 - training loss (MAE): 0.0547, validation MSE: -0.7585
2025-05-02 21:51:24 [INFO]: Epoch 362 - training loss (MAE): 0.0548, validation MSE: -0.7808
2025-05-02 21:51:39 [INFO]: Epoch 363 - training loss (MAE): 0.0542, validation MSE: -0.7888
2025-05-02 21:51:54 [INFO]: Epoch 364 - training loss (MAE): 0.0549, validation MSE: -0.7212
2025-05-02 21:52:10 [INFO]: Epoch 365 - training loss (MAE): 0.0550, validation MSE: -0.7463
2025-05-02 21:52:25 [INFO]: Epoch 366 - training loss (MAE): 0.0549, validation MSE: -0.7755
2025-05-02 21:52:40 [INFO]: Epoch 367 - training loss (MAE): 0.0546, validation MSE: -0.7695
2025-05-02 21:52:55 [INFO]: Epoch 368 - training loss (MAE): 0.0550, validation MSE: -0.7316
2025-05-02 21:53:10 [INFO]: Epoch 369 - training loss (MAE): 0.0548, validation MSE: -0.7892
2025-05-02 21:53:26 [INFO]: Epoch 370 - training loss (MAE): 0.0542, validation MSE: -0.8025
2025-05-02 21:53:41 [INFO]: Epoch 371 - training loss (MAE): 0.0543, validation MSE: -0.8070
2025-05-02 21:53:56 [INFO]: Epoch 372 - training loss (MAE): 0.0544, validation MSE: -0.7596
2025-05-02 21:54:11 [INFO]: Epoch 373 - training loss (MAE): 0.0537, validation MSE: -0.7774
2025-05-02 21:54:26 [INFO]: Epoch 374 - training loss (MAE): 0.0538, validation MSE: -0.7590
2025-05-02 21:54:41 [INFO]: Epoch 375 - training loss (MAE): 0.0545, validation MSE: -0.7532
2025-05-02 21:54:57 [INFO]: Epoch 376 - training loss (MAE): 0.0540, validation MSE: -0.7535
2025-05-02 21:55:12 [INFO]: Epoch 377 - training loss (MAE): 0.0539, validation MSE: -0.7821
2025-05-02 21:55:27 [INFO]: Epoch 378 - training loss (MAE): 0.0543, validation MSE: -0.7661
2025-05-02 21:55:43 [INFO]: Epoch 379 - training loss (MAE): 0.0540, validation MSE: -0.7762
2025-05-02 21:55:58 [INFO]: Epoch 380 - training loss (MAE): 0.0549, validation MSE: -0.7459
2025-05-02 21:56:13 [INFO]: Epoch 381 - training loss (MAE): 0.0545, validation MSE: -0.7413
2025-05-02 21:56:28 [INFO]: Epoch 382 - training loss (MAE): 0.0536, validation MSE: -0.7666
2025-05-02 21:56:44 [INFO]: Epoch 383 - training loss (MAE): 0.0538, validation MSE: -0.7758
2025-05-02 21:56:59 [INFO]: Epoch 384 - training loss (MAE): 0.0539, validation MSE: -0.7377
2025-05-02 21:57:15 [INFO]: Epoch 385 - training loss (MAE): 0.0542, validation MSE: -0.7350
2025-05-02 21:57:31 [INFO]: Epoch 386 - training loss (MAE): 0.0543, validation MSE: -0.7741
2025-05-02 21:57:46 [INFO]: Epoch 387 - training loss (MAE): 0.0533, validation MSE: -0.7599
2025-05-02 21:58:01 [INFO]: Epoch 388 - training loss (MAE): 0.0538, validation MSE: -0.7481
2025-05-02 21:58:16 [INFO]: Epoch 389 - training loss (MAE): 0.0539, validation MSE: -0.7670
2025-05-02 21:58:32 [INFO]: Epoch 390 - training loss (MAE): 0.0537, validation MSE: -0.7489
2025-05-02 21:58:47 [INFO]: Epoch 391 - training loss (MAE): 0.0536, validation MSE: -0.7623
2025-05-02 21:59:02 [INFO]: Epoch 392 - training loss (MAE): 0.0535, validation MSE: -0.7370
2025-05-02 21:59:17 [INFO]: Epoch 393 - training loss (MAE): 0.0534, validation MSE: -0.7479
2025-05-02 21:59:33 [INFO]: Epoch 394 - training loss (MAE): 0.0543, validation MSE: -0.7699
2025-05-02 21:59:48 [INFO]: Epoch 395 - training loss (MAE): 0.0538, validation MSE: -0.7565
2025-05-02 22:00:03 [INFO]: Epoch 396 - training loss (MAE): 0.0531, validation MSE: -0.7320
2025-05-02 22:00:18 [INFO]: Epoch 397 - training loss (MAE): 0.0532, validation MSE: -0.7750
2025-05-02 22:00:33 [INFO]: Epoch 398 - training loss (MAE): 0.0533, validation MSE: -0.7578
2025-05-02 22:00:48 [INFO]: Epoch 399 - training loss (MAE): 0.0536, validation MSE: -0.7376
2025-05-02 22:01:03 [INFO]: Epoch 400 - training loss (MAE): 0.0530, validation MSE: -0.7752
2025-05-02 22:01:19 [INFO]: Epoch 401 - training loss (MAE): 0.0535, validation MSE: -0.7475
2025-05-02 22:01:34 [INFO]: Epoch 402 - training loss (MAE): 0.0538, validation MSE: -0.7415
2025-05-02 22:01:49 [INFO]: Epoch 403 - training loss (MAE): 0.0533, validation MSE: -0.7214
2025-05-02 22:02:04 [INFO]: Epoch 404 - training loss (MAE): 0.0529, validation MSE: -0.7331
2025-05-02 22:02:19 [INFO]: Epoch 405 - training loss (MAE): 0.0536, validation MSE: -0.6780
2025-05-02 22:02:35 [INFO]: Epoch 406 - training loss (MAE): 0.0533, validation MSE: -0.7372
2025-05-02 22:02:50 [INFO]: Epoch 407 - training loss (MAE): 0.0529, validation MSE: -0.7506
2025-05-02 22:03:05 [INFO]: Epoch 408 - training loss (MAE): 0.0533, validation MSE: -0.7615
2025-05-02 22:03:21 [INFO]: Epoch 409 - training loss (MAE): 0.0534, validation MSE: -0.7435
2025-05-02 22:03:36 [INFO]: Epoch 410 - training loss (MAE): 0.0528, validation MSE: -0.7590
2025-05-02 22:03:51 [INFO]: Epoch 411 - training loss (MAE): 0.0528, validation MSE: -0.7238
2025-05-02 22:04:06 [INFO]: Epoch 412 - training loss (MAE): 0.0526, validation MSE: -0.7200
2025-05-02 22:04:21 [INFO]: Epoch 413 - training loss (MAE): 0.0530, validation MSE: -0.7283
2025-05-02 22:04:36 [INFO]: Epoch 414 - training loss (MAE): 0.0530, validation MSE: -0.6770
2025-05-02 22:04:51 [INFO]: Epoch 415 - training loss (MAE): 0.0526, validation MSE: -0.7373
2025-05-02 22:05:07 [INFO]: Epoch 416 - training loss (MAE): 0.0525, validation MSE: -0.7226
2025-05-02 22:05:22 [INFO]: Epoch 417 - training loss (MAE): 0.0524, validation MSE: -0.7222
2025-05-02 22:05:37 [INFO]: Epoch 418 - training loss (MAE): 0.0529, validation MSE: -0.7083
2025-05-02 22:05:52 [INFO]: Epoch 419 - training loss (MAE): 0.0535, validation MSE: -0.7427
2025-05-02 22:06:08 [INFO]: Epoch 420 - training loss (MAE): 0.0524, validation MSE: -0.7152
2025-05-02 22:06:23 [INFO]: Epoch 421 - training loss (MAE): 0.0530, validation MSE: -0.7046
2025-05-02 22:06:38 [INFO]: Epoch 422 - training loss (MAE): 0.0527, validation MSE: -0.7381
2025-05-02 22:06:53 [INFO]: Epoch 423 - training loss (MAE): 0.0525, validation MSE: -0.7149
2025-05-02 22:07:09 [INFO]: Epoch 424 - training loss (MAE): 0.0527, validation MSE: -0.7171
2025-05-02 22:07:24 [INFO]: Epoch 425 - training loss (MAE): 0.0538, validation MSE: -0.7245
2025-05-02 22:07:39 [INFO]: Epoch 426 - training loss (MAE): 0.0532, validation MSE: -0.7533
2025-05-02 22:07:54 [INFO]: Epoch 427 - training loss (MAE): 0.0521, validation MSE: -0.7263
2025-05-02 22:08:09 [INFO]: Epoch 428 - training loss (MAE): 0.0521, validation MSE: -0.7360
2025-05-02 22:08:24 [INFO]: Epoch 429 - training loss (MAE): 0.0521, validation MSE: -0.7310
2025-05-02 22:08:40 [INFO]: Epoch 430 - training loss (MAE): 0.0524, validation MSE: -0.7230
2025-05-02 22:08:55 [INFO]: Epoch 431 - training loss (MAE): 0.0528, validation MSE: -0.7389
2025-05-02 22:09:10 [INFO]: Epoch 432 - training loss (MAE): 0.0521, validation MSE: -0.7408
2025-05-02 22:09:25 [INFO]: Epoch 433 - training loss (MAE): 0.0526, validation MSE: -0.7360
2025-05-02 22:09:40 [INFO]: Epoch 434 - training loss (MAE): 0.0526, validation MSE: -0.7270
2025-05-02 22:09:55 [INFO]: Epoch 435 - training loss (MAE): 0.0523, validation MSE: -0.7538
2025-05-02 22:10:11 [INFO]: Epoch 436 - training loss (MAE): 0.0529, validation MSE: -0.7326
2025-05-02 22:10:26 [INFO]: Epoch 437 - training loss (MAE): 0.0522, validation MSE: -0.7345
2025-05-02 22:10:41 [INFO]: Epoch 438 - training loss (MAE): 0.0521, validation MSE: -0.7291
2025-05-02 22:10:56 [INFO]: Epoch 439 - training loss (MAE): 0.0519, validation MSE: -0.7350
2025-05-02 22:11:11 [INFO]: Epoch 440 - training loss (MAE): 0.0519, validation MSE: -0.7115
2025-05-02 22:11:27 [INFO]: Epoch 441 - training loss (MAE): 0.0522, validation MSE: -0.7148
2025-05-02 22:11:42 [INFO]: Epoch 442 - training loss (MAE): 0.0518, validation MSE: -0.7076
2025-05-02 22:11:57 [INFO]: Epoch 443 - training loss (MAE): 0.0524, validation MSE: -0.7321
2025-05-02 22:12:12 [INFO]: Epoch 444 - training loss (MAE): 0.0517, validation MSE: -0.7310
2025-05-02 22:12:28 [INFO]: Epoch 445 - training loss (MAE): 0.0517, validation MSE: -0.7325
2025-05-02 22:12:43 [INFO]: Epoch 446 - training loss (MAE): 0.0514, validation MSE: -0.7285
2025-05-02 22:12:58 [INFO]: Epoch 447 - training loss (MAE): 0.0515, validation MSE: -0.7001
2025-05-02 22:13:14 [INFO]: Epoch 448 - training loss (MAE): 0.0511, validation MSE: -0.7220
2025-05-02 22:13:29 [INFO]: Epoch 449 - training loss (MAE): 0.0519, validation MSE: -0.7394
2025-05-02 22:13:44 [INFO]: Epoch 450 - training loss (MAE): 0.0515, validation MSE: -0.7348
2025-05-02 22:13:59 [INFO]: Epoch 451 - training loss (MAE): 0.0528, validation MSE: -0.7247
2025-05-02 22:14:15 [INFO]: Epoch 452 - training loss (MAE): 0.0516, validation MSE: -0.7298
2025-05-02 22:14:30 [INFO]: Epoch 453 - training loss (MAE): 0.0516, validation MSE: -0.7257
2025-05-02 22:14:45 [INFO]: Epoch 454 - training loss (MAE): 0.0522, validation MSE: -0.7187
2025-05-02 22:15:00 [INFO]: Epoch 455 - training loss (MAE): 0.0514, validation MSE: -0.7103
2025-05-02 22:15:16 [INFO]: Epoch 456 - training loss (MAE): 0.0513, validation MSE: -0.7161
2025-05-02 22:15:31 [INFO]: Epoch 457 - training loss (MAE): 0.0513, validation MSE: -0.7334
2025-05-02 22:15:46 [INFO]: Epoch 458 - training loss (MAE): 0.0521, validation MSE: -0.7137
2025-05-02 22:16:02 [INFO]: Epoch 459 - training loss (MAE): 0.0514, validation MSE: -0.7205
2025-05-02 22:16:17 [INFO]: Epoch 460 - training loss (MAE): 0.0527, validation MSE: -0.7084
2025-05-02 22:16:32 [INFO]: Epoch 461 - training loss (MAE): 0.0514, validation MSE: -0.7397
2025-05-02 22:16:48 [INFO]: Epoch 462 - training loss (MAE): 0.0514, validation MSE: -0.7274
2025-05-02 22:17:03 [INFO]: Epoch 463 - training loss (MAE): 0.0513, validation MSE: -0.7263
2025-05-02 22:17:18 [INFO]: Epoch 464 - training loss (MAE): 0.0521, validation MSE: -0.7324
2025-05-02 22:17:34 [INFO]: Epoch 465 - training loss (MAE): 0.0511, validation MSE: -0.7375
2025-05-02 22:17:49 [INFO]: Epoch 466 - training loss (MAE): 0.0513, validation MSE: -0.7054
2025-05-02 22:18:04 [INFO]: Epoch 467 - training loss (MAE): 0.0525, validation MSE: -0.6922
2025-05-02 22:18:19 [INFO]: Epoch 468 - training loss (MAE): 0.0512, validation MSE: -0.7333
2025-05-02 22:18:35 [INFO]: Epoch 469 - training loss (MAE): 0.0510, validation MSE: -0.7257
2025-05-02 22:18:50 [INFO]: Epoch 470 - training loss (MAE): 0.0513, validation MSE: -0.7043
2025-05-02 22:19:05 [INFO]: Epoch 471 - training loss (MAE): 0.0510, validation MSE: -0.7364
2025-05-02 22:19:20 [INFO]: Epoch 472 - training loss (MAE): 0.0520, validation MSE: -0.7206
2025-05-02 22:19:35 [INFO]: Epoch 473 - training loss (MAE): 0.0527, validation MSE: -0.7180
2025-05-02 22:19:50 [INFO]: Epoch 474 - training loss (MAE): 0.0504, validation MSE: -0.7421
2025-05-02 22:20:06 [INFO]: Epoch 475 - training loss (MAE): 0.0524, validation MSE: -0.7357
2025-05-02 22:20:21 [INFO]: Epoch 476 - training loss (MAE): 0.0511, validation MSE: -0.7362
2025-05-02 22:20:36 [INFO]: Epoch 477 - training loss (MAE): 0.0509, validation MSE: -0.7339
2025-05-02 22:20:52 [INFO]: Epoch 478 - training loss (MAE): 0.0516, validation MSE: -0.7316
2025-05-02 22:21:07 [INFO]: Epoch 479 - training loss (MAE): 0.0507, validation MSE: -0.7184
2025-05-02 22:21:22 [INFO]: Epoch 480 - training loss (MAE): 0.0507, validation MSE: -0.7073
2025-05-02 22:21:37 [INFO]: Epoch 481 - training loss (MAE): 0.0510, validation MSE: -0.7273
2025-05-02 22:21:52 [INFO]: Epoch 482 - training loss (MAE): 0.0508, validation MSE: -0.7128
2025-05-02 22:22:07 [INFO]: Epoch 483 - training loss (MAE): 0.0514, validation MSE: -0.7122
2025-05-02 22:22:23 [INFO]: Epoch 484 - training loss (MAE): 0.0508, validation MSE: -0.6958
2025-05-02 22:22:38 [INFO]: Epoch 485 - training loss (MAE): 0.0504, validation MSE: -0.7127
2025-05-02 22:22:53 [INFO]: Epoch 486 - training loss (MAE): 0.0510, validation MSE: -0.6963
2025-05-02 22:23:08 [INFO]: Epoch 487 - training loss (MAE): 0.0515, validation MSE: -0.7012
2025-05-02 22:23:24 [INFO]: Epoch 488 - training loss (MAE): 0.0504, validation MSE: -0.6821
2025-05-02 22:23:39 [INFO]: Epoch 489 - training loss (MAE): 0.0505, validation MSE: -0.7225
2025-05-02 22:23:54 [INFO]: Epoch 490 - training loss (MAE): 0.0514, validation MSE: -0.7418
2025-05-02 22:24:09 [INFO]: Epoch 491 - training loss (MAE): 0.0510, validation MSE: -0.7168
2025-05-02 22:24:24 [INFO]: Epoch 492 - training loss (MAE): 0.0508, validation MSE: -0.7097
2025-05-02 22:24:40 [INFO]: Epoch 493 - training loss (MAE): 0.0506, validation MSE: -0.7007
2025-05-02 22:24:55 [INFO]: Epoch 494 - training loss (MAE): 0.0501, validation MSE: -0.6763
2025-05-02 22:25:10 [INFO]: Epoch 495 - training loss (MAE): 0.0508, validation MSE: -0.7141
2025-05-02 22:25:25 [INFO]: Epoch 496 - training loss (MAE): 0.0502, validation MSE: -0.6889
2025-05-02 22:25:41 [INFO]: Epoch 497 - training loss (MAE): 0.0510, validation MSE: -0.7268
2025-05-02 22:25:56 [INFO]: Epoch 498 - training loss (MAE): 0.0513, validation MSE: -0.6966
2025-05-02 22:26:12 [INFO]: Epoch 499 - training loss (MAE): 0.0513, validation MSE: -0.7126
2025-05-02 22:26:27 [INFO]: Epoch 500 - training loss (MAE): 0.0504, validation MSE: -0.6988
2025-05-02 22:26:27 [INFO]: Finished training. The best model is from epoch#106.
2025-05-02 22:26:27 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_3.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-02 22:26:27 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_3.pypots
Fold 3 metrics: MAE: 0.355, MSE: 0.469, MRE: 0.532
Fold 3 metrics: MAE: 0.355, MSE: 0.469, MRE: 0.532
Training fold 4/10
2025-05-02 22:26:27 [INFO]: No given device, using default device: cuda
2025-05-02 22:26:27 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-02 22:26:27 [INFO]: Using customized MAE as the training loss function.
2025-05-02 22:26:27 [INFO]: Using customized MSE as the validation metric function.
2025-05-02 22:26:27 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-02 22:26:43 [INFO]: Epoch 001 - training loss (MAE): 0.9079, validation MSE: -0.2612
2025-05-02 22:26:58 [INFO]: Epoch 002 - training loss (MAE): 0.3901, validation MSE: -0.4084
2025-05-02 22:27:13 [INFO]: Epoch 003 - training loss (MAE): 0.3185, validation MSE: -0.4602
2025-05-02 22:27:28 [INFO]: Epoch 004 - training loss (MAE): 0.2877, validation MSE: -0.5062
2025-05-02 22:27:43 [INFO]: Epoch 005 - training loss (MAE): 0.2651, validation MSE: -0.5535
2025-05-02 22:27:59 [INFO]: Epoch 006 - training loss (MAE): 0.2458, validation MSE: -0.5715
2025-05-02 22:28:14 [INFO]: Epoch 007 - training loss (MAE): 0.2311, validation MSE: -0.6020
2025-05-02 22:28:29 [INFO]: Epoch 008 - training loss (MAE): 0.2156, validation MSE: -0.6225
2025-05-02 22:28:44 [INFO]: Epoch 009 - training loss (MAE): 0.2037, validation MSE: -0.6289
2025-05-02 22:28:59 [INFO]: Epoch 010 - training loss (MAE): 0.1884, validation MSE: -0.6365
2025-05-02 22:29:14 [INFO]: Epoch 011 - training loss (MAE): 0.1783, validation MSE: -0.6751
2025-05-02 22:29:29 [INFO]: Epoch 012 - training loss (MAE): 0.1678, validation MSE: -0.6687
2025-05-02 22:29:45 [INFO]: Epoch 013 - training loss (MAE): 0.1586, validation MSE: -0.6759
2025-05-02 22:30:00 [INFO]: Epoch 014 - training loss (MAE): 0.1492, validation MSE: -0.6949
2025-05-02 22:30:15 [INFO]: Epoch 015 - training loss (MAE): 0.1420, validation MSE: -0.6786
2025-05-02 22:30:30 [INFO]: Epoch 016 - training loss (MAE): 0.1390, validation MSE: -0.7161
2025-05-02 22:30:46 [INFO]: Epoch 017 - training loss (MAE): 0.1314, validation MSE: -0.6979
2025-05-02 22:31:01 [INFO]: Epoch 018 - training loss (MAE): 0.1275, validation MSE: -0.7542
2025-05-02 22:31:17 [INFO]: Epoch 019 - training loss (MAE): 0.1253, validation MSE: -0.7065
2025-05-02 22:31:32 [INFO]: Epoch 020 - training loss (MAE): 0.1200, validation MSE: -0.7561
2025-05-02 22:31:47 [INFO]: Epoch 021 - training loss (MAE): 0.1171, validation MSE: -0.6982
2025-05-02 22:32:02 [INFO]: Epoch 022 - training loss (MAE): 0.1149, validation MSE: -0.7533
2025-05-02 22:32:17 [INFO]: Epoch 023 - training loss (MAE): 0.1120, validation MSE: -0.7206
2025-05-02 22:32:32 [INFO]: Epoch 024 - training loss (MAE): 0.1102, validation MSE: -0.7510
2025-05-02 22:32:47 [INFO]: Epoch 025 - training loss (MAE): 0.1078, validation MSE: -0.7336
2025-05-02 22:33:02 [INFO]: Epoch 026 - training loss (MAE): 0.1063, validation MSE: -0.7409
2025-05-02 22:33:17 [INFO]: Epoch 027 - training loss (MAE): 0.1048, validation MSE: -0.7623
2025-05-02 22:33:33 [INFO]: Epoch 028 - training loss (MAE): 0.1032, validation MSE: -0.7356
2025-05-02 22:33:48 [INFO]: Epoch 029 - training loss (MAE): 0.1006, validation MSE: -0.7644
2025-05-02 22:34:03 [INFO]: Epoch 030 - training loss (MAE): 0.1003, validation MSE: -0.7718
2025-05-02 22:34:18 [INFO]: Epoch 031 - training loss (MAE): 0.0978, validation MSE: -0.7644
2025-05-02 22:34:33 [INFO]: Epoch 032 - training loss (MAE): 0.0968, validation MSE: -0.7532
2025-05-02 22:34:49 [INFO]: Epoch 033 - training loss (MAE): 0.0956, validation MSE: -0.7487
2025-05-02 22:35:04 [INFO]: Epoch 034 - training loss (MAE): 0.0944, validation MSE: -0.7287
2025-05-02 22:35:19 [INFO]: Epoch 035 - training loss (MAE): 0.0935, validation MSE: -0.7615
2025-05-02 22:35:34 [INFO]: Epoch 036 - training loss (MAE): 0.0929, validation MSE: -0.7814
2025-05-02 22:35:49 [INFO]: Epoch 037 - training loss (MAE): 0.0909, validation MSE: -0.7528
2025-05-02 22:36:05 [INFO]: Epoch 038 - training loss (MAE): 0.0905, validation MSE: -0.7793
2025-05-02 22:36:20 [INFO]: Epoch 039 - training loss (MAE): 0.0896, validation MSE: -0.7703
2025-05-02 22:36:35 [INFO]: Epoch 040 - training loss (MAE): 0.0882, validation MSE: -0.7643
2025-05-02 22:36:50 [INFO]: Epoch 041 - training loss (MAE): 0.0881, validation MSE: -0.7579
2025-05-02 22:37:05 [INFO]: Epoch 042 - training loss (MAE): 0.0880, validation MSE: -0.7504
2025-05-02 22:37:21 [INFO]: Epoch 043 - training loss (MAE): 0.0861, validation MSE: -0.7774
2025-05-02 22:37:36 [INFO]: Epoch 044 - training loss (MAE): 0.0850, validation MSE: -0.7652
2025-05-02 22:37:51 [INFO]: Epoch 045 - training loss (MAE): 0.0846, validation MSE: -0.7582
2025-05-02 22:38:06 [INFO]: Epoch 046 - training loss (MAE): 0.0836, validation MSE: -0.7787
2025-05-02 22:38:21 [INFO]: Epoch 047 - training loss (MAE): 0.0838, validation MSE: -0.7644
2025-05-02 22:38:37 [INFO]: Epoch 048 - training loss (MAE): 0.0824, validation MSE: -0.7663
2025-05-02 22:38:52 [INFO]: Epoch 049 - training loss (MAE): 0.0818, validation MSE: -0.7872
2025-05-02 22:39:07 [INFO]: Epoch 050 - training loss (MAE): 0.0809, validation MSE: -0.7947
2025-05-02 22:39:22 [INFO]: Epoch 051 - training loss (MAE): 0.0812, validation MSE: -0.7894
2025-05-02 22:39:38 [INFO]: Epoch 052 - training loss (MAE): 0.0797, validation MSE: -0.8071
2025-05-02 22:39:53 [INFO]: Epoch 053 - training loss (MAE): 0.0802, validation MSE: -0.8313
2025-05-02 22:40:08 [INFO]: Epoch 054 - training loss (MAE): 0.0796, validation MSE: -0.7987
2025-05-02 22:40:23 [INFO]: Epoch 055 - training loss (MAE): 0.0784, validation MSE: -0.7822
2025-05-02 22:40:38 [INFO]: Epoch 056 - training loss (MAE): 0.0782, validation MSE: -0.7943
2025-05-02 22:40:53 [INFO]: Epoch 057 - training loss (MAE): 0.0776, validation MSE: -0.8084
2025-05-02 22:41:09 [INFO]: Epoch 058 - training loss (MAE): 0.0768, validation MSE: -0.7855
2025-05-02 22:41:24 [INFO]: Epoch 059 - training loss (MAE): 0.0781, validation MSE: -0.7913
2025-05-02 22:41:39 [INFO]: Epoch 060 - training loss (MAE): 0.0760, validation MSE: -0.7719
2025-05-02 22:41:54 [INFO]: Epoch 061 - training loss (MAE): 0.0761, validation MSE: -0.7937
2025-05-02 22:42:09 [INFO]: Epoch 062 - training loss (MAE): 0.0770, validation MSE: -0.8206
2025-05-02 22:42:25 [INFO]: Epoch 063 - training loss (MAE): 0.0754, validation MSE: -0.8078
2025-05-02 22:42:40 [INFO]: Epoch 064 - training loss (MAE): 0.0751, validation MSE: -0.8133
2025-05-02 22:42:55 [INFO]: Epoch 065 - training loss (MAE): 0.0753, validation MSE: -0.7901
2025-05-02 22:43:10 [INFO]: Epoch 066 - training loss (MAE): 0.0746, validation MSE: -0.7790
2025-05-02 22:43:26 [INFO]: Epoch 067 - training loss (MAE): 0.0743, validation MSE: -0.8256
2025-05-02 22:43:41 [INFO]: Epoch 068 - training loss (MAE): 0.0740, validation MSE: -0.7951
2025-05-02 22:43:56 [INFO]: Epoch 069 - training loss (MAE): 0.0736, validation MSE: -0.8062
2025-05-02 22:44:11 [INFO]: Epoch 070 - training loss (MAE): 0.0731, validation MSE: -0.8167
2025-05-02 22:44:27 [INFO]: Epoch 071 - training loss (MAE): 0.0730, validation MSE: -0.8005
2025-05-02 22:44:42 [INFO]: Epoch 072 - training loss (MAE): 0.0728, validation MSE: -0.8082
2025-05-02 22:44:57 [INFO]: Epoch 073 - training loss (MAE): 0.0726, validation MSE: -0.8174
2025-05-02 22:45:12 [INFO]: Epoch 074 - training loss (MAE): 0.0723, validation MSE: -0.7954
2025-05-02 22:45:27 [INFO]: Epoch 075 - training loss (MAE): 0.0719, validation MSE: -0.7913
2025-05-02 22:45:42 [INFO]: Epoch 076 - training loss (MAE): 0.0720, validation MSE: -0.7844
2025-05-02 22:45:58 [INFO]: Epoch 077 - training loss (MAE): 0.0722, validation MSE: -0.8220
2025-05-02 22:46:13 [INFO]: Epoch 078 - training loss (MAE): 0.0713, validation MSE: -0.8148
2025-05-02 22:46:28 [INFO]: Epoch 079 - training loss (MAE): 0.0712, validation MSE: -0.8342
2025-05-02 22:46:43 [INFO]: Epoch 080 - training loss (MAE): 0.0711, validation MSE: -0.8267
2025-05-02 22:46:58 [INFO]: Epoch 081 - training loss (MAE): 0.0706, validation MSE: -0.8019
2025-05-02 22:47:14 [INFO]: Epoch 082 - training loss (MAE): 0.0706, validation MSE: -0.7971
2025-05-02 22:47:29 [INFO]: Epoch 083 - training loss (MAE): 0.0706, validation MSE: -0.8088
2025-05-02 22:47:44 [INFO]: Epoch 084 - training loss (MAE): 0.0702, validation MSE: -0.8264
2025-05-02 22:47:59 [INFO]: Epoch 085 - training loss (MAE): 0.0701, validation MSE: -0.8219
2025-05-02 22:48:14 [INFO]: Epoch 086 - training loss (MAE): 0.0694, validation MSE: -0.8083
2025-05-02 22:48:30 [INFO]: Epoch 087 - training loss (MAE): 0.0693, validation MSE: -0.8501
2025-05-02 22:48:45 [INFO]: Epoch 088 - training loss (MAE): 0.0690, validation MSE: -0.8502
2025-05-02 22:49:00 [INFO]: Epoch 089 - training loss (MAE): 0.0696, validation MSE: -0.8186
2025-05-02 22:49:15 [INFO]: Epoch 090 - training loss (MAE): 0.0692, validation MSE: -0.7872
2025-05-02 22:49:30 [INFO]: Epoch 091 - training loss (MAE): 0.0687, validation MSE: -0.7924
2025-05-02 22:49:45 [INFO]: Epoch 092 - training loss (MAE): 0.0684, validation MSE: -0.8302
2025-05-02 22:50:01 [INFO]: Epoch 093 - training loss (MAE): 0.0690, validation MSE: -0.8079
2025-05-02 22:50:16 [INFO]: Epoch 094 - training loss (MAE): 0.0683, validation MSE: -0.8153
2025-05-02 22:50:31 [INFO]: Epoch 095 - training loss (MAE): 0.0676, validation MSE: -0.8113
2025-05-02 22:50:46 [INFO]: Epoch 096 - training loss (MAE): 0.0687, validation MSE: -0.8096
2025-05-02 22:51:01 [INFO]: Epoch 097 - training loss (MAE): 0.0679, validation MSE: -0.8349
2025-05-02 22:51:17 [INFO]: Epoch 098 - training loss (MAE): 0.0675, validation MSE: -0.8119
2025-05-02 22:51:32 [INFO]: Epoch 099 - training loss (MAE): 0.0674, validation MSE: -0.8285
2025-05-02 22:51:47 [INFO]: Epoch 100 - training loss (MAE): 0.0677, validation MSE: -0.8268
2025-05-02 22:52:02 [INFO]: Epoch 101 - training loss (MAE): 0.0671, validation MSE: -0.7980
2025-05-02 22:52:18 [INFO]: Epoch 102 - training loss (MAE): 0.0670, validation MSE: -0.8230
2025-05-02 22:52:33 [INFO]: Epoch 103 - training loss (MAE): 0.0668, validation MSE: -0.8078
2025-05-02 22:52:48 [INFO]: Epoch 104 - training loss (MAE): 0.0666, validation MSE: -0.8046
2025-05-02 22:53:03 [INFO]: Epoch 105 - training loss (MAE): 0.0666, validation MSE: -0.8120
2025-05-02 22:53:18 [INFO]: Epoch 106 - training loss (MAE): 0.0664, validation MSE: -0.7868
2025-05-02 22:53:33 [INFO]: Epoch 107 - training loss (MAE): 0.0674, validation MSE: -0.8355
2025-05-02 22:53:48 [INFO]: Epoch 108 - training loss (MAE): 0.0668, validation MSE: -0.8153
2025-05-02 22:54:04 [INFO]: Epoch 109 - training loss (MAE): 0.0662, validation MSE: -0.8259
2025-05-02 22:54:19 [INFO]: Epoch 110 - training loss (MAE): 0.0663, validation MSE: -0.8248
2025-05-02 22:54:35 [INFO]: Epoch 111 - training loss (MAE): 0.0660, validation MSE: -0.7839
2025-05-02 22:54:50 [INFO]: Epoch 112 - training loss (MAE): 0.0669, validation MSE: -0.8090
2025-05-02 22:55:05 [INFO]: Epoch 113 - training loss (MAE): 0.0657, validation MSE: -0.8317
2025-05-02 22:55:20 [INFO]: Epoch 114 - training loss (MAE): 0.0662, validation MSE: -0.8047
2025-05-02 22:55:35 [INFO]: Epoch 115 - training loss (MAE): 0.0655, validation MSE: -0.8214
2025-05-02 22:55:50 [INFO]: Epoch 116 - training loss (MAE): 0.0655, validation MSE: -0.8004
2025-05-02 22:56:05 [INFO]: Epoch 117 - training loss (MAE): 0.0657, validation MSE: -0.8048
2025-05-02 22:56:20 [INFO]: Epoch 118 - training loss (MAE): 0.0655, validation MSE: -0.8231
2025-05-02 22:56:35 [INFO]: Epoch 119 - training loss (MAE): 0.0657, validation MSE: -0.8229
2025-05-02 22:56:50 [INFO]: Epoch 120 - training loss (MAE): 0.0656, validation MSE: -0.7866
2025-05-02 22:57:05 [INFO]: Epoch 121 - training loss (MAE): 0.0650, validation MSE: -0.7811
2025-05-02 22:57:21 [INFO]: Epoch 122 - training loss (MAE): 0.0656, validation MSE: -0.8294
2025-05-02 22:57:36 [INFO]: Epoch 123 - training loss (MAE): 0.0661, validation MSE: -0.8182
2025-05-02 22:57:51 [INFO]: Epoch 124 - training loss (MAE): 0.0649, validation MSE: -0.8121
2025-05-02 22:58:06 [INFO]: Epoch 125 - training loss (MAE): 0.0651, validation MSE: -0.8214
2025-05-02 22:58:21 [INFO]: Epoch 126 - training loss (MAE): 0.0646, validation MSE: -0.8230
2025-05-02 22:58:36 [INFO]: Epoch 127 - training loss (MAE): 0.0643, validation MSE: -0.8312
2025-05-02 22:58:51 [INFO]: Epoch 128 - training loss (MAE): 0.0649, validation MSE: -0.8311
2025-05-02 22:59:07 [INFO]: Epoch 129 - training loss (MAE): 0.0655, validation MSE: -0.8058
2025-05-02 22:59:22 [INFO]: Epoch 130 - training loss (MAE): 0.0644, validation MSE: -0.7926
2025-05-02 22:59:37 [INFO]: Epoch 131 - training loss (MAE): 0.0644, validation MSE: -0.8021
2025-05-02 22:59:52 [INFO]: Epoch 132 - training loss (MAE): 0.0639, validation MSE: -0.8263
2025-05-02 23:00:07 [INFO]: Epoch 133 - training loss (MAE): 0.0644, validation MSE: -0.8381
2025-05-02 23:00:22 [INFO]: Epoch 134 - training loss (MAE): 0.0642, validation MSE: -0.8420
2025-05-02 23:00:38 [INFO]: Epoch 135 - training loss (MAE): 0.0642, validation MSE: -0.8202
2025-05-02 23:00:53 [INFO]: Epoch 136 - training loss (MAE): 0.0647, validation MSE: -0.8252
2025-05-02 23:01:08 [INFO]: Epoch 137 - training loss (MAE): 0.0634, validation MSE: -0.8244
2025-05-02 23:01:23 [INFO]: Epoch 138 - training loss (MAE): 0.0646, validation MSE: -0.8016
2025-05-02 23:01:38 [INFO]: Epoch 139 - training loss (MAE): 0.0636, validation MSE: -0.8221
2025-05-02 23:01:53 [INFO]: Epoch 140 - training loss (MAE): 0.0635, validation MSE: -0.8143
2025-05-02 23:02:09 [INFO]: Epoch 141 - training loss (MAE): 0.0636, validation MSE: -0.8361
2025-05-02 23:02:24 [INFO]: Epoch 142 - training loss (MAE): 0.0633, validation MSE: -0.7864
2025-05-02 23:02:39 [INFO]: Epoch 143 - training loss (MAE): 0.0640, validation MSE: -0.8228
2025-05-02 23:02:54 [INFO]: Epoch 144 - training loss (MAE): 0.0629, validation MSE: -0.8064
2025-05-02 23:03:10 [INFO]: Epoch 145 - training loss (MAE): 0.0631, validation MSE: -0.8151
2025-05-02 23:03:25 [INFO]: Epoch 146 - training loss (MAE): 0.0639, validation MSE: -0.8664
2025-05-02 23:03:40 [INFO]: Epoch 147 - training loss (MAE): 0.0634, validation MSE: -0.8446
2025-05-02 23:03:55 [INFO]: Epoch 148 - training loss (MAE): 0.0638, validation MSE: -0.8452
2025-05-02 23:04:10 [INFO]: Epoch 149 - training loss (MAE): 0.0632, validation MSE: -0.8294
2025-05-02 23:04:25 [INFO]: Epoch 150 - training loss (MAE): 0.0628, validation MSE: -0.8136
2025-05-02 23:04:40 [INFO]: Epoch 151 - training loss (MAE): 0.0629, validation MSE: -0.8149
2025-05-02 23:04:55 [INFO]: Epoch 152 - training loss (MAE): 0.0629, validation MSE: -0.7878
2025-05-02 23:05:11 [INFO]: Epoch 153 - training loss (MAE): 0.0624, validation MSE: -0.8132
2025-05-02 23:05:25 [INFO]: Epoch 154 - training loss (MAE): 0.0627, validation MSE: -0.8153
2025-05-02 23:05:40 [INFO]: Epoch 155 - training loss (MAE): 0.0626, validation MSE: -0.8267
2025-05-02 23:05:56 [INFO]: Epoch 156 - training loss (MAE): 0.0625, validation MSE: -0.8529
2025-05-02 23:06:11 [INFO]: Epoch 157 - training loss (MAE): 0.0626, validation MSE: -0.8271
2025-05-02 23:06:26 [INFO]: Epoch 158 - training loss (MAE): 0.0630, validation MSE: -0.8366
2025-05-02 23:06:41 [INFO]: Epoch 159 - training loss (MAE): 0.0621, validation MSE: -0.8316
2025-05-02 23:06:56 [INFO]: Epoch 160 - training loss (MAE): 0.0622, validation MSE: -0.8353
2025-05-02 23:07:12 [INFO]: Epoch 161 - training loss (MAE): 0.0629, validation MSE: -0.7941
2025-05-02 23:07:27 [INFO]: Epoch 162 - training loss (MAE): 0.0624, validation MSE: -0.8247
2025-05-02 23:07:42 [INFO]: Epoch 163 - training loss (MAE): 0.0623, validation MSE: -0.7830
2025-05-02 23:07:57 [INFO]: Epoch 164 - training loss (MAE): 0.0622, validation MSE: -0.8270
2025-05-02 23:08:12 [INFO]: Epoch 165 - training loss (MAE): 0.0621, validation MSE: -0.8222
2025-05-02 23:08:27 [INFO]: Epoch 166 - training loss (MAE): 0.0620, validation MSE: -0.7967
2025-05-02 23:08:42 [INFO]: Epoch 167 - training loss (MAE): 0.0617, validation MSE: -0.7895
2025-05-02 23:08:57 [INFO]: Epoch 168 - training loss (MAE): 0.0617, validation MSE: -0.8217
2025-05-02 23:09:12 [INFO]: Epoch 169 - training loss (MAE): 0.0617, validation MSE: -0.8535
2025-05-02 23:09:27 [INFO]: Epoch 170 - training loss (MAE): 0.0624, validation MSE: -0.8280
2025-05-02 23:09:43 [INFO]: Epoch 171 - training loss (MAE): 0.0618, validation MSE: -0.8273
2025-05-02 23:09:58 [INFO]: Epoch 172 - training loss (MAE): 0.0620, validation MSE: -0.8168
2025-05-02 23:10:13 [INFO]: Epoch 173 - training loss (MAE): 0.0614, validation MSE: -0.8368
2025-05-02 23:10:28 [INFO]: Epoch 174 - training loss (MAE): 0.0635, validation MSE: -0.8260
2025-05-02 23:10:43 [INFO]: Epoch 175 - training loss (MAE): 0.0613, validation MSE: -0.8190
2025-05-02 23:10:58 [INFO]: Epoch 176 - training loss (MAE): 0.0614, validation MSE: -0.8179
2025-05-02 23:11:13 [INFO]: Epoch 177 - training loss (MAE): 0.0618, validation MSE: -0.7776
2025-05-02 23:11:28 [INFO]: Epoch 178 - training loss (MAE): 0.0622, validation MSE: -0.8348
2025-05-02 23:11:44 [INFO]: Epoch 179 - training loss (MAE): 0.0615, validation MSE: -0.8240
2025-05-02 23:11:59 [INFO]: Epoch 180 - training loss (MAE): 0.0611, validation MSE: -0.8171
2025-05-02 23:12:14 [INFO]: Epoch 181 - training loss (MAE): 0.0614, validation MSE: -0.7771
2025-05-02 23:12:29 [INFO]: Epoch 182 - training loss (MAE): 0.0606, validation MSE: -0.8337
2025-05-02 23:12:44 [INFO]: Epoch 183 - training loss (MAE): 0.0604, validation MSE: -0.8235
2025-05-02 23:13:00 [INFO]: Epoch 184 - training loss (MAE): 0.0606, validation MSE: -0.8232
2025-05-02 23:13:15 [INFO]: Epoch 185 - training loss (MAE): 0.0611, validation MSE: -0.8388
2025-05-02 23:13:30 [INFO]: Epoch 186 - training loss (MAE): 0.0611, validation MSE: -0.8217
2025-05-02 23:13:45 [INFO]: Epoch 187 - training loss (MAE): 0.0608, validation MSE: -0.8296
2025-05-02 23:14:00 [INFO]: Epoch 188 - training loss (MAE): 0.0612, validation MSE: -0.8222
2025-05-02 23:14:16 [INFO]: Epoch 189 - training loss (MAE): 0.0608, validation MSE: -0.8759
2025-05-02 23:14:30 [INFO]: Epoch 190 - training loss (MAE): 0.0606, validation MSE: -0.8290
2025-05-02 23:14:46 [INFO]: Epoch 191 - training loss (MAE): 0.0617, validation MSE: -0.8035
2025-05-02 23:15:01 [INFO]: Epoch 192 - training loss (MAE): 0.0612, validation MSE: -0.8485
2025-05-02 23:15:16 [INFO]: Epoch 193 - training loss (MAE): 0.0609, validation MSE: -0.8448
2025-05-02 23:15:31 [INFO]: Epoch 194 - training loss (MAE): 0.0608, validation MSE: -0.8270
2025-05-02 23:15:46 [INFO]: Epoch 195 - training loss (MAE): 0.0602, validation MSE: -0.8145
2025-05-02 23:16:01 [INFO]: Epoch 196 - training loss (MAE): 0.0608, validation MSE: -0.8009
2025-05-02 23:16:16 [INFO]: Epoch 197 - training loss (MAE): 0.0605, validation MSE: -0.8362
2025-05-02 23:16:31 [INFO]: Epoch 198 - training loss (MAE): 0.0611, validation MSE: -0.8064
2025-05-02 23:16:46 [INFO]: Epoch 199 - training loss (MAE): 0.0602, validation MSE: -0.8644
2025-05-02 23:17:02 [INFO]: Epoch 200 - training loss (MAE): 0.0607, validation MSE: -0.8305
2025-05-02 23:17:17 [INFO]: Epoch 201 - training loss (MAE): 0.0605, validation MSE: -0.8210
2025-05-02 23:17:32 [INFO]: Epoch 202 - training loss (MAE): 0.0597, validation MSE: -0.8021
2025-05-02 23:17:47 [INFO]: Epoch 203 - training loss (MAE): 0.0611, validation MSE: -0.8050
2025-05-02 23:18:02 [INFO]: Epoch 204 - training loss (MAE): 0.0600, validation MSE: -0.8516
2025-05-02 23:18:17 [INFO]: Epoch 205 - training loss (MAE): 0.0600, validation MSE: -0.8277
2025-05-02 23:18:32 [INFO]: Epoch 206 - training loss (MAE): 0.0603, validation MSE: -0.8376
2025-05-02 23:18:48 [INFO]: Epoch 207 - training loss (MAE): 0.0598, validation MSE: -0.8102
2025-05-02 23:19:03 [INFO]: Epoch 208 - training loss (MAE): 0.0596, validation MSE: -0.8719
2025-05-02 23:19:18 [INFO]: Epoch 209 - training loss (MAE): 0.0603, validation MSE: -0.8365
2025-05-02 23:19:33 [INFO]: Epoch 210 - training loss (MAE): 0.0597, validation MSE: -0.8558
2025-05-02 23:19:48 [INFO]: Epoch 211 - training loss (MAE): 0.0598, validation MSE: -0.8424
2025-05-02 23:20:04 [INFO]: Epoch 212 - training loss (MAE): 0.0600, validation MSE: -0.8428
2025-05-02 23:20:19 [INFO]: Epoch 213 - training loss (MAE): 0.0598, validation MSE: -0.7969
2025-05-02 23:20:34 [INFO]: Epoch 214 - training loss (MAE): 0.0598, validation MSE: -0.8431
2025-05-02 23:20:49 [INFO]: Epoch 215 - training loss (MAE): 0.0596, validation MSE: -0.8391
2025-05-02 23:21:04 [INFO]: Epoch 216 - training loss (MAE): 0.0593, validation MSE: -0.8503
2025-05-02 23:21:19 [INFO]: Epoch 217 - training loss (MAE): 0.0590, validation MSE: -0.8215
2025-05-02 23:21:34 [INFO]: Epoch 218 - training loss (MAE): 0.0599, validation MSE: -0.8401
2025-05-02 23:21:50 [INFO]: Epoch 219 - training loss (MAE): 0.0600, validation MSE: -0.8261
2025-05-02 23:22:05 [INFO]: Epoch 220 - training loss (MAE): 0.0599, validation MSE: -0.8116
2025-05-02 23:22:20 [INFO]: Epoch 221 - training loss (MAE): 0.0596, validation MSE: -0.8380
2025-05-02 23:22:35 [INFO]: Epoch 222 - training loss (MAE): 0.0603, validation MSE: -0.7822
2025-05-02 23:22:50 [INFO]: Epoch 223 - training loss (MAE): 0.0592, validation MSE: -0.8078
2025-05-02 23:23:05 [INFO]: Epoch 224 - training loss (MAE): 0.0590, validation MSE: -0.8324
2025-05-02 23:23:20 [INFO]: Epoch 225 - training loss (MAE): 0.0595, validation MSE: -0.8079
2025-05-02 23:23:35 [INFO]: Epoch 226 - training loss (MAE): 0.0589, validation MSE: -0.8490
2025-05-02 23:23:51 [INFO]: Epoch 227 - training loss (MAE): 0.0594, validation MSE: -0.8298
2025-05-02 23:24:06 [INFO]: Epoch 228 - training loss (MAE): 0.0593, validation MSE: -0.8511
2025-05-02 23:24:21 [INFO]: Epoch 229 - training loss (MAE): 0.0592, validation MSE: -0.8382
2025-05-02 23:24:36 [INFO]: Epoch 230 - training loss (MAE): 0.0594, validation MSE: -0.8045
2025-05-02 23:24:51 [INFO]: Epoch 231 - training loss (MAE): 0.0598, validation MSE: -0.8351
2025-05-02 23:25:07 [INFO]: Epoch 232 - training loss (MAE): 0.0590, validation MSE: -0.8264
2025-05-02 23:25:22 [INFO]: Epoch 233 - training loss (MAE): 0.0587, validation MSE: -0.8306
2025-05-02 23:25:37 [INFO]: Epoch 234 - training loss (MAE): 0.0586, validation MSE: -0.8174
2025-05-02 23:25:52 [INFO]: Epoch 235 - training loss (MAE): 0.0594, validation MSE: -0.8204
2025-05-02 23:26:07 [INFO]: Epoch 236 - training loss (MAE): 0.0582, validation MSE: -0.8613
2025-05-02 23:26:22 [INFO]: Epoch 237 - training loss (MAE): 0.0591, validation MSE: -0.8440
2025-05-02 23:26:37 [INFO]: Epoch 238 - training loss (MAE): 0.0590, validation MSE: -0.8300
2025-05-02 23:26:52 [INFO]: Epoch 239 - training loss (MAE): 0.0593, validation MSE: -0.8224
2025-05-02 23:27:07 [INFO]: Epoch 240 - training loss (MAE): 0.0583, validation MSE: -0.8183
2025-05-02 23:27:22 [INFO]: Epoch 241 - training loss (MAE): 0.0587, validation MSE: -0.8435
2025-05-02 23:27:38 [INFO]: Epoch 242 - training loss (MAE): 0.0587, validation MSE: -0.8417
2025-05-02 23:27:53 [INFO]: Epoch 243 - training loss (MAE): 0.0586, validation MSE: -0.8239
2025-05-02 23:28:08 [INFO]: Epoch 244 - training loss (MAE): 0.0594, validation MSE: -0.8402
2025-05-02 23:28:23 [INFO]: Epoch 245 - training loss (MAE): 0.0591, validation MSE: -0.8558
2025-05-02 23:28:38 [INFO]: Epoch 246 - training loss (MAE): 0.0585, validation MSE: -0.8494
2025-05-02 23:28:53 [INFO]: Epoch 247 - training loss (MAE): 0.0584, validation MSE: -0.8460
2025-05-02 23:29:09 [INFO]: Epoch 248 - training loss (MAE): 0.0581, validation MSE: -0.8143
2025-05-02 23:29:24 [INFO]: Epoch 249 - training loss (MAE): 0.0595, validation MSE: -0.8348
2025-05-02 23:29:39 [INFO]: Epoch 250 - training loss (MAE): 0.0587, validation MSE: -0.8233
2025-05-02 23:29:54 [INFO]: Epoch 251 - training loss (MAE): 0.0584, validation MSE: -0.8193
2025-05-02 23:30:10 [INFO]: Epoch 252 - training loss (MAE): 0.0582, validation MSE: -0.8421
2025-05-02 23:30:25 [INFO]: Epoch 253 - training loss (MAE): 0.0582, validation MSE: -0.8264
2025-05-02 23:30:40 [INFO]: Epoch 254 - training loss (MAE): 0.0584, validation MSE: -0.8213
2025-05-02 23:30:55 [INFO]: Epoch 255 - training loss (MAE): 0.0583, validation MSE: -0.8889
2025-05-02 23:31:10 [INFO]: Epoch 256 - training loss (MAE): 0.0585, validation MSE: -0.8125
2025-05-02 23:31:25 [INFO]: Epoch 257 - training loss (MAE): 0.0584, validation MSE: -0.8389
2025-05-02 23:31:40 [INFO]: Epoch 258 - training loss (MAE): 0.0579, validation MSE: -0.8232
2025-05-02 23:31:56 [INFO]: Epoch 259 - training loss (MAE): 0.0581, validation MSE: -0.8481
2025-05-02 23:32:11 [INFO]: Epoch 260 - training loss (MAE): 0.0588, validation MSE: -0.8127
2025-05-02 23:32:26 [INFO]: Epoch 261 - training loss (MAE): 0.0587, validation MSE: -0.8334
2025-05-02 23:32:41 [INFO]: Epoch 262 - training loss (MAE): 0.0578, validation MSE: -0.8423
2025-05-02 23:32:57 [INFO]: Epoch 263 - training loss (MAE): 0.0580, validation MSE: -0.8361
2025-05-02 23:33:12 [INFO]: Epoch 264 - training loss (MAE): 0.0576, validation MSE: -0.7937
2025-05-02 23:33:27 [INFO]: Epoch 265 - training loss (MAE): 0.0577, validation MSE: -0.8150
2025-05-02 23:33:42 [INFO]: Epoch 266 - training loss (MAE): 0.0579, validation MSE: -0.7956
2025-05-02 23:33:57 [INFO]: Epoch 267 - training loss (MAE): 0.0578, validation MSE: -0.8546
2025-05-02 23:34:12 [INFO]: Epoch 268 - training loss (MAE): 0.0586, validation MSE: -0.7880
2025-05-02 23:34:28 [INFO]: Epoch 269 - training loss (MAE): 0.0577, validation MSE: -0.8033
2025-05-02 23:34:43 [INFO]: Epoch 270 - training loss (MAE): 0.0575, validation MSE: -0.8318
2025-05-02 23:34:58 [INFO]: Epoch 271 - training loss (MAE): 0.0582, validation MSE: -0.8159
2025-05-02 23:35:13 [INFO]: Epoch 272 - training loss (MAE): 0.0575, validation MSE: -0.7951
2025-05-02 23:35:28 [INFO]: Epoch 273 - training loss (MAE): 0.0576, validation MSE: -0.8290
2025-05-02 23:35:43 [INFO]: Epoch 274 - training loss (MAE): 0.0585, validation MSE: -0.7568
2025-05-02 23:35:58 [INFO]: Epoch 275 - training loss (MAE): 0.0596, validation MSE: -0.8058
2025-05-02 23:36:14 [INFO]: Epoch 276 - training loss (MAE): 0.0571, validation MSE: -0.7970
2025-05-02 23:36:29 [INFO]: Epoch 277 - training loss (MAE): 0.0578, validation MSE: -0.7799
2025-05-02 23:36:44 [INFO]: Epoch 278 - training loss (MAE): 0.0575, validation MSE: -0.8375
2025-05-02 23:36:59 [INFO]: Epoch 279 - training loss (MAE): 0.0575, validation MSE: -0.8173
2025-05-02 23:37:14 [INFO]: Epoch 280 - training loss (MAE): 0.0573, validation MSE: -0.7861
2025-05-02 23:37:29 [INFO]: Epoch 281 - training loss (MAE): 0.0576, validation MSE: -0.8234
2025-05-02 23:37:45 [INFO]: Epoch 282 - training loss (MAE): 0.0573, validation MSE: -0.8119
2025-05-02 23:38:00 [INFO]: Epoch 283 - training loss (MAE): 0.0570, validation MSE: -0.8133
2025-05-02 23:38:15 [INFO]: Epoch 284 - training loss (MAE): 0.0576, validation MSE: -0.8418
2025-05-02 23:38:30 [INFO]: Epoch 285 - training loss (MAE): 0.0568, validation MSE: -0.8412
2025-05-02 23:38:45 [INFO]: Epoch 286 - training loss (MAE): 0.0570, validation MSE: -0.8346
2025-05-02 23:39:01 [INFO]: Epoch 287 - training loss (MAE): 0.0572, validation MSE: -0.8069
2025-05-02 23:39:16 [INFO]: Epoch 288 - training loss (MAE): 0.0570, validation MSE: -0.8131
2025-05-02 23:39:31 [INFO]: Epoch 289 - training loss (MAE): 0.0577, validation MSE: -0.8103
2025-05-02 23:39:46 [INFO]: Epoch 290 - training loss (MAE): 0.0566, validation MSE: -0.8234
2025-05-02 23:40:01 [INFO]: Epoch 291 - training loss (MAE): 0.0569, validation MSE: -0.8137
2025-05-02 23:40:16 [INFO]: Epoch 292 - training loss (MAE): 0.0567, validation MSE: -0.8061
2025-05-02 23:40:31 [INFO]: Epoch 293 - training loss (MAE): 0.0569, validation MSE: -0.8073
2025-05-02 23:40:46 [INFO]: Epoch 294 - training loss (MAE): 0.0570, validation MSE: -0.8106
2025-05-02 23:41:01 [INFO]: Epoch 295 - training loss (MAE): 0.0570, validation MSE: -0.8095
2025-05-02 23:41:16 [INFO]: Epoch 296 - training loss (MAE): 0.0567, validation MSE: -0.8284
2025-05-02 23:41:31 [INFO]: Epoch 297 - training loss (MAE): 0.0561, validation MSE: -0.8247
2025-05-02 23:41:46 [INFO]: Epoch 298 - training loss (MAE): 0.0569, validation MSE: -0.8242
2025-05-02 23:42:01 [INFO]: Epoch 299 - training loss (MAE): 0.0565, validation MSE: -0.8057
2025-05-02 23:42:17 [INFO]: Epoch 300 - training loss (MAE): 0.0575, validation MSE: -0.8069
2025-05-02 23:42:32 [INFO]: Epoch 301 - training loss (MAE): 0.0572, validation MSE: -0.8274
2025-05-02 23:42:47 [INFO]: Epoch 302 - training loss (MAE): 0.0561, validation MSE: -0.8087
2025-05-02 23:43:02 [INFO]: Epoch 303 - training loss (MAE): 0.0562, validation MSE: -0.8317
2025-05-02 23:43:17 [INFO]: Epoch 304 - training loss (MAE): 0.0563, validation MSE: -0.8221
2025-05-02 23:43:32 [INFO]: Epoch 305 - training loss (MAE): 0.0565, validation MSE: -0.7873
2025-05-02 23:43:47 [INFO]: Epoch 306 - training loss (MAE): 0.0562, validation MSE: -0.8407
2025-05-02 23:44:02 [INFO]: Epoch 307 - training loss (MAE): 0.0574, validation MSE: -0.7911
2025-05-02 23:44:17 [INFO]: Epoch 308 - training loss (MAE): 0.0566, validation MSE: -0.7976
2025-05-02 23:44:32 [INFO]: Epoch 309 - training loss (MAE): 0.0567, validation MSE: -0.8374
2025-05-02 23:44:47 [INFO]: Epoch 310 - training loss (MAE): 0.0560, validation MSE: -0.7799
2025-05-02 23:45:02 [INFO]: Epoch 311 - training loss (MAE): 0.0561, validation MSE: -0.8189
2025-05-02 23:45:18 [INFO]: Epoch 312 - training loss (MAE): 0.0555, validation MSE: -0.8096
2025-05-02 23:45:32 [INFO]: Epoch 313 - training loss (MAE): 0.0557, validation MSE: -0.7661
2025-05-02 23:45:47 [INFO]: Epoch 314 - training loss (MAE): 0.0560, validation MSE: -0.8120
2025-05-02 23:46:03 [INFO]: Epoch 315 - training loss (MAE): 0.0557, validation MSE: -0.8036
2025-05-02 23:46:18 [INFO]: Epoch 316 - training loss (MAE): 0.0556, validation MSE: -0.8048
2025-05-02 23:46:33 [INFO]: Epoch 317 - training loss (MAE): 0.0560, validation MSE: -0.7713
2025-05-02 23:46:48 [INFO]: Epoch 318 - training loss (MAE): 0.0559, validation MSE: -0.7868
2025-05-02 23:47:03 [INFO]: Epoch 319 - training loss (MAE): 0.0562, validation MSE: -0.7981
2025-05-02 23:47:18 [INFO]: Epoch 320 - training loss (MAE): 0.0556, validation MSE: -0.8402
2025-05-02 23:47:33 [INFO]: Epoch 321 - training loss (MAE): 0.0566, validation MSE: -0.8032
2025-05-02 23:47:48 [INFO]: Epoch 322 - training loss (MAE): 0.0555, validation MSE: -0.8148
2025-05-02 23:48:04 [INFO]: Epoch 323 - training loss (MAE): 0.0552, validation MSE: -0.8095
2025-05-02 23:48:19 [INFO]: Epoch 324 - training loss (MAE): 0.0553, validation MSE: -0.8208
2025-05-02 23:48:34 [INFO]: Epoch 325 - training loss (MAE): 0.0556, validation MSE: -0.7497
2025-05-02 23:48:49 [INFO]: Epoch 326 - training loss (MAE): 0.0550, validation MSE: -0.8037
2025-05-02 23:49:04 [INFO]: Epoch 327 - training loss (MAE): 0.0554, validation MSE: -0.7917
2025-05-02 23:49:20 [INFO]: Epoch 328 - training loss (MAE): 0.0546, validation MSE: -0.7761
2025-05-02 23:49:35 [INFO]: Epoch 329 - training loss (MAE): 0.0551, validation MSE: -0.7963
2025-05-02 23:49:50 [INFO]: Epoch 330 - training loss (MAE): 0.0556, validation MSE: -0.7711
2025-05-02 23:50:05 [INFO]: Epoch 331 - training loss (MAE): 0.0555, validation MSE: -0.7558
2025-05-02 23:50:20 [INFO]: Epoch 332 - training loss (MAE): 0.0549, validation MSE: -0.8020
2025-05-02 23:50:35 [INFO]: Epoch 333 - training loss (MAE): 0.0549, validation MSE: -0.7933
2025-05-02 23:50:50 [INFO]: Epoch 334 - training loss (MAE): 0.0550, validation MSE: -0.8016
2025-05-02 23:51:05 [INFO]: Epoch 335 - training loss (MAE): 0.0546, validation MSE: -0.8011
2025-05-02 23:51:20 [INFO]: Epoch 336 - training loss (MAE): 0.0547, validation MSE: -0.7306
2025-05-02 23:51:35 [INFO]: Epoch 337 - training loss (MAE): 0.0549, validation MSE: -0.7213
2025-05-02 23:51:51 [INFO]: Epoch 338 - training loss (MAE): 0.0544, validation MSE: -0.7843
2025-05-02 23:52:06 [INFO]: Epoch 339 - training loss (MAE): 0.0545, validation MSE: -0.7785
2025-05-02 23:52:21 [INFO]: Epoch 340 - training loss (MAE): 0.0547, validation MSE: -0.7575
2025-05-02 23:52:36 [INFO]: Epoch 341 - training loss (MAE): 0.0546, validation MSE: -0.8046
2025-05-02 23:52:52 [INFO]: Epoch 342 - training loss (MAE): 0.0547, validation MSE: -0.7709
2025-05-02 23:53:07 [INFO]: Epoch 343 - training loss (MAE): 0.0544, validation MSE: -0.7505
2025-05-02 23:53:22 [INFO]: Epoch 344 - training loss (MAE): 0.0544, validation MSE: -0.7868
2025-05-02 23:53:37 [INFO]: Epoch 345 - training loss (MAE): 0.0542, validation MSE: -0.7893
2025-05-02 23:53:52 [INFO]: Epoch 346 - training loss (MAE): 0.0547, validation MSE: -0.7871
2025-05-02 23:54:07 [INFO]: Epoch 347 - training loss (MAE): 0.0541, validation MSE: -0.8032
2025-05-02 23:54:22 [INFO]: Epoch 348 - training loss (MAE): 0.0544, validation MSE: -0.7513
2025-05-02 23:54:37 [INFO]: Epoch 349 - training loss (MAE): 0.0540, validation MSE: -0.7896
2025-05-02 23:54:53 [INFO]: Epoch 350 - training loss (MAE): 0.0550, validation MSE: -0.7395
2025-05-02 23:55:08 [INFO]: Epoch 351 - training loss (MAE): 0.0541, validation MSE: -0.7552
2025-05-02 23:55:23 [INFO]: Epoch 352 - training loss (MAE): 0.0536, validation MSE: -0.7734
2025-05-02 23:55:38 [INFO]: Epoch 353 - training loss (MAE): 0.0534, validation MSE: -0.7611
2025-05-02 23:55:53 [INFO]: Epoch 354 - training loss (MAE): 0.0539, validation MSE: -0.7830
2025-05-02 23:56:09 [INFO]: Epoch 355 - training loss (MAE): 0.0542, validation MSE: -0.7570
2025-05-02 23:56:24 [INFO]: Epoch 356 - training loss (MAE): 0.0542, validation MSE: -0.8064
2025-05-02 23:56:39 [INFO]: Epoch 357 - training loss (MAE): 0.0536, validation MSE: -0.7805
2025-05-02 23:56:55 [INFO]: Epoch 358 - training loss (MAE): 0.0555, validation MSE: -0.7308
2025-05-02 23:57:10 [INFO]: Epoch 359 - training loss (MAE): 0.0531, validation MSE: -0.7478
2025-05-02 23:57:25 [INFO]: Epoch 360 - training loss (MAE): 0.0534, validation MSE: -0.7653
2025-05-02 23:57:40 [INFO]: Epoch 361 - training loss (MAE): 0.0532, validation MSE: -0.7660
2025-05-02 23:57:55 [INFO]: Epoch 362 - training loss (MAE): 0.0538, validation MSE: -0.8021
2025-05-02 23:58:11 [INFO]: Epoch 363 - training loss (MAE): 0.0537, validation MSE: -0.7752
2025-05-02 23:58:26 [INFO]: Epoch 364 - training loss (MAE): 0.0531, validation MSE: -0.7419
2025-05-02 23:58:41 [INFO]: Epoch 365 - training loss (MAE): 0.0534, validation MSE: -0.7578
2025-05-02 23:58:57 [INFO]: Epoch 366 - training loss (MAE): 0.0534, validation MSE: -0.6904
2025-05-02 23:59:12 [INFO]: Epoch 367 - training loss (MAE): 0.0531, validation MSE: -0.7933
2025-05-02 23:59:27 [INFO]: Epoch 368 - training loss (MAE): 0.0539, validation MSE: -0.7616
2025-05-02 23:59:42 [INFO]: Epoch 369 - training loss (MAE): 0.0529, validation MSE: -0.7407
2025-05-02 23:59:57 [INFO]: Epoch 370 - training loss (MAE): 0.0540, validation MSE: -0.7674
2025-05-03 00:00:13 [INFO]: Epoch 371 - training loss (MAE): 0.0529, validation MSE: -0.7758
2025-05-03 00:00:28 [INFO]: Epoch 372 - training loss (MAE): 0.0530, validation MSE: -0.7575
2025-05-03 00:00:43 [INFO]: Epoch 373 - training loss (MAE): 0.0529, validation MSE: -0.7944
2025-05-03 00:00:58 [INFO]: Epoch 374 - training loss (MAE): 0.0535, validation MSE: -0.7493
2025-05-03 00:01:14 [INFO]: Epoch 375 - training loss (MAE): 0.0534, validation MSE: -0.7662
2025-05-03 00:01:29 [INFO]: Epoch 376 - training loss (MAE): 0.0524, validation MSE: -0.7514
2025-05-03 00:01:44 [INFO]: Epoch 377 - training loss (MAE): 0.0524, validation MSE: -0.7858
2025-05-03 00:01:59 [INFO]: Epoch 378 - training loss (MAE): 0.0526, validation MSE: -0.7714
2025-05-03 00:02:15 [INFO]: Epoch 379 - training loss (MAE): 0.0523, validation MSE: -0.7029
2025-05-03 00:02:30 [INFO]: Epoch 380 - training loss (MAE): 0.0527, validation MSE: -0.7670
2025-05-03 00:02:45 [INFO]: Epoch 381 - training loss (MAE): 0.0527, validation MSE: -0.7437
2025-05-03 00:03:00 [INFO]: Epoch 382 - training loss (MAE): 0.0524, validation MSE: -0.7335
2025-05-03 00:03:16 [INFO]: Epoch 383 - training loss (MAE): 0.0523, validation MSE: -0.7262
2025-05-03 00:03:31 [INFO]: Epoch 384 - training loss (MAE): 0.0522, validation MSE: -0.7207
2025-05-03 00:03:46 [INFO]: Epoch 385 - training loss (MAE): 0.0527, validation MSE: -0.7297
2025-05-03 00:04:01 [INFO]: Epoch 386 - training loss (MAE): 0.0523, validation MSE: -0.7312
2025-05-03 00:04:16 [INFO]: Epoch 387 - training loss (MAE): 0.0524, validation MSE: -0.7647
2025-05-03 00:04:31 [INFO]: Epoch 388 - training loss (MAE): 0.0525, validation MSE: -0.7640
2025-05-03 00:04:46 [INFO]: Epoch 389 - training loss (MAE): 0.0528, validation MSE: -0.7182
2025-05-03 00:05:02 [INFO]: Epoch 390 - training loss (MAE): 0.0523, validation MSE: -0.6780
2025-05-03 00:05:17 [INFO]: Epoch 391 - training loss (MAE): 0.0517, validation MSE: -0.7452
2025-05-03 00:05:33 [INFO]: Epoch 392 - training loss (MAE): 0.0523, validation MSE: -0.7744
2025-05-03 00:05:48 [INFO]: Epoch 393 - training loss (MAE): 0.0519, validation MSE: -0.7599
2025-05-03 00:06:03 [INFO]: Epoch 394 - training loss (MAE): 0.0519, validation MSE: -0.7448
2025-05-03 00:06:18 [INFO]: Epoch 395 - training loss (MAE): 0.0516, validation MSE: -0.7316
2025-05-03 00:06:34 [INFO]: Epoch 396 - training loss (MAE): 0.0516, validation MSE: -0.7196
2025-05-03 00:06:49 [INFO]: Epoch 397 - training loss (MAE): 0.0521, validation MSE: -0.7266
2025-05-03 00:07:04 [INFO]: Epoch 398 - training loss (MAE): 0.0520, validation MSE: -0.7396
2025-05-03 00:07:19 [INFO]: Epoch 399 - training loss (MAE): 0.0513, validation MSE: -0.7093
2025-05-03 00:07:34 [INFO]: Epoch 400 - training loss (MAE): 0.0521, validation MSE: -0.7645
2025-05-03 00:07:49 [INFO]: Epoch 401 - training loss (MAE): 0.0518, validation MSE: -0.7481
2025-05-03 00:08:04 [INFO]: Epoch 402 - training loss (MAE): 0.0522, validation MSE: -0.7522
2025-05-03 00:08:19 [INFO]: Epoch 403 - training loss (MAE): 0.0513, validation MSE: -0.7375
2025-05-03 00:08:35 [INFO]: Epoch 404 - training loss (MAE): 0.0518, validation MSE: -0.7007
2025-05-03 00:08:50 [INFO]: Epoch 405 - training loss (MAE): 0.0515, validation MSE: -0.7501
2025-05-03 00:09:05 [INFO]: Epoch 406 - training loss (MAE): 0.0507, validation MSE: -0.7493
2025-05-03 00:09:20 [INFO]: Epoch 407 - training loss (MAE): 0.0511, validation MSE: -0.7642
2025-05-03 00:09:35 [INFO]: Epoch 408 - training loss (MAE): 0.0513, validation MSE: -0.7452
2025-05-03 00:09:50 [INFO]: Epoch 409 - training loss (MAE): 0.0515, validation MSE: -0.7562
2025-05-03 00:10:05 [INFO]: Epoch 410 - training loss (MAE): 0.0518, validation MSE: -0.7586
2025-05-03 00:10:20 [INFO]: Epoch 411 - training loss (MAE): 0.0513, validation MSE: -0.7596
2025-05-03 00:10:35 [INFO]: Epoch 412 - training loss (MAE): 0.0507, validation MSE: -0.7799
2025-05-03 00:10:50 [INFO]: Epoch 413 - training loss (MAE): 0.0510, validation MSE: -0.7382
2025-05-03 00:11:05 [INFO]: Epoch 414 - training loss (MAE): 0.0512, validation MSE: -0.7274
2025-05-03 00:11:20 [INFO]: Epoch 415 - training loss (MAE): 0.0510, validation MSE: -0.7111
2025-05-03 00:11:36 [INFO]: Epoch 416 - training loss (MAE): 0.0503, validation MSE: -0.6743
2025-05-03 00:11:51 [INFO]: Epoch 417 - training loss (MAE): 0.0514, validation MSE: -0.7132
2025-05-03 00:12:06 [INFO]: Epoch 418 - training loss (MAE): 0.0510, validation MSE: -0.7056
2025-05-03 00:12:21 [INFO]: Epoch 419 - training loss (MAE): 0.0506, validation MSE: -0.7078
2025-05-03 00:12:36 [INFO]: Epoch 420 - training loss (MAE): 0.0504, validation MSE: -0.7337
2025-05-03 00:12:51 [INFO]: Epoch 421 - training loss (MAE): 0.0510, validation MSE: -0.7292
2025-05-03 00:13:06 [INFO]: Epoch 422 - training loss (MAE): 0.0507, validation MSE: -0.7280
2025-05-03 00:13:21 [INFO]: Epoch 423 - training loss (MAE): 0.0517, validation MSE: -0.6894
2025-05-03 00:13:36 [INFO]: Epoch 424 - training loss (MAE): 0.0509, validation MSE: -0.7041
2025-05-03 00:13:51 [INFO]: Epoch 425 - training loss (MAE): 0.0509, validation MSE: -0.7320
2025-05-03 00:14:06 [INFO]: Epoch 426 - training loss (MAE): 0.0498, validation MSE: -0.7216
2025-05-03 00:14:21 [INFO]: Epoch 427 - training loss (MAE): 0.0501, validation MSE: -0.7133
2025-05-03 00:14:37 [INFO]: Epoch 428 - training loss (MAE): 0.0503, validation MSE: -0.7490
2025-05-03 00:14:51 [INFO]: Epoch 429 - training loss (MAE): 0.0501, validation MSE: -0.7061
2025-05-03 00:15:07 [INFO]: Epoch 430 - training loss (MAE): 0.0502, validation MSE: -0.7267
2025-05-03 00:15:22 [INFO]: Epoch 431 - training loss (MAE): 0.0498, validation MSE: -0.7330
2025-05-03 00:15:37 [INFO]: Epoch 432 - training loss (MAE): 0.0503, validation MSE: -0.7216
2025-05-03 00:15:52 [INFO]: Epoch 433 - training loss (MAE): 0.0500, validation MSE: -0.6925
2025-05-03 00:16:07 [INFO]: Epoch 434 - training loss (MAE): 0.0499, validation MSE: -0.7386
2025-05-03 00:16:22 [INFO]: Epoch 435 - training loss (MAE): 0.0502, validation MSE: -0.7209
2025-05-03 00:16:37 [INFO]: Epoch 436 - training loss (MAE): 0.0501, validation MSE: -0.7103
2025-05-03 00:16:52 [INFO]: Epoch 437 - training loss (MAE): 0.0506, validation MSE: -0.6604
2025-05-03 00:17:07 [INFO]: Epoch 438 - training loss (MAE): 0.0505, validation MSE: -0.7328
2025-05-03 00:17:23 [INFO]: Epoch 439 - training loss (MAE): 0.0498, validation MSE: -0.7261
2025-05-03 00:17:38 [INFO]: Epoch 440 - training loss (MAE): 0.0496, validation MSE: -0.7301
2025-05-03 00:17:53 [INFO]: Epoch 441 - training loss (MAE): 0.0498, validation MSE: -0.7237
2025-05-03 00:18:08 [INFO]: Epoch 442 - training loss (MAE): 0.0494, validation MSE: -0.7307
2025-05-03 00:18:23 [INFO]: Epoch 443 - training loss (MAE): 0.0494, validation MSE: -0.7156
2025-05-03 00:18:38 [INFO]: Epoch 444 - training loss (MAE): 0.0505, validation MSE: -0.7108
2025-05-03 00:18:53 [INFO]: Epoch 445 - training loss (MAE): 0.0499, validation MSE: -0.7361
2025-05-03 00:19:08 [INFO]: Epoch 446 - training loss (MAE): 0.0498, validation MSE: -0.7224
2025-05-03 00:19:24 [INFO]: Epoch 447 - training loss (MAE): 0.0492, validation MSE: -0.7437
2025-05-03 00:19:39 [INFO]: Epoch 448 - training loss (MAE): 0.0499, validation MSE: -0.7325
2025-05-03 00:19:54 [INFO]: Epoch 449 - training loss (MAE): 0.0492, validation MSE: -0.6985
2025-05-03 00:20:09 [INFO]: Epoch 450 - training loss (MAE): 0.0500, validation MSE: -0.6484
2025-05-03 00:20:24 [INFO]: Epoch 451 - training loss (MAE): 0.0495, validation MSE: -0.7321
2025-05-03 00:20:39 [INFO]: Epoch 452 - training loss (MAE): 0.0495, validation MSE: -0.7297
2025-05-03 00:20:54 [INFO]: Epoch 453 - training loss (MAE): 0.0495, validation MSE: -0.7191
2025-05-03 00:21:09 [INFO]: Epoch 454 - training loss (MAE): 0.0488, validation MSE: -0.7131
2025-05-03 00:21:24 [INFO]: Epoch 455 - training loss (MAE): 0.0495, validation MSE: -0.7229
2025-05-03 00:21:40 [INFO]: Epoch 456 - training loss (MAE): 0.0492, validation MSE: -0.7253
2025-05-03 00:21:55 [INFO]: Epoch 457 - training loss (MAE): 0.0491, validation MSE: -0.6679
2025-05-03 00:22:10 [INFO]: Epoch 458 - training loss (MAE): 0.0492, validation MSE: -0.6859
2025-05-03 00:22:25 [INFO]: Epoch 459 - training loss (MAE): 0.0493, validation MSE: -0.6558
2025-05-03 00:22:40 [INFO]: Epoch 460 - training loss (MAE): 0.0490, validation MSE: -0.7444
2025-05-03 00:22:55 [INFO]: Epoch 461 - training loss (MAE): 0.0492, validation MSE: -0.7100
2025-05-03 00:23:11 [INFO]: Epoch 462 - training loss (MAE): 0.0495, validation MSE: -0.7134
2025-05-03 00:23:26 [INFO]: Epoch 463 - training loss (MAE): 0.0493, validation MSE: -0.7524
2025-05-03 00:23:41 [INFO]: Epoch 464 - training loss (MAE): 0.0493, validation MSE: -0.7068
2025-05-03 00:23:56 [INFO]: Epoch 465 - training loss (MAE): 0.0489, validation MSE: -0.7092
2025-05-03 00:24:11 [INFO]: Epoch 466 - training loss (MAE): 0.0490, validation MSE: -0.7263
2025-05-03 00:24:26 [INFO]: Epoch 467 - training loss (MAE): 0.0491, validation MSE: -0.7130
2025-05-03 00:24:41 [INFO]: Epoch 468 - training loss (MAE): 0.0489, validation MSE: -0.6780
2025-05-03 00:24:56 [INFO]: Epoch 469 - training loss (MAE): 0.0490, validation MSE: -0.7089
2025-05-03 00:25:11 [INFO]: Epoch 470 - training loss (MAE): 0.0488, validation MSE: -0.6910
2025-05-03 00:25:27 [INFO]: Epoch 471 - training loss (MAE): 0.0490, validation MSE: -0.6744
2025-05-03 00:25:42 [INFO]: Epoch 472 - training loss (MAE): 0.0484, validation MSE: -0.7158
2025-05-03 00:25:57 [INFO]: Epoch 473 - training loss (MAE): 0.0485, validation MSE: -0.7161
2025-05-03 00:26:12 [INFO]: Epoch 474 - training loss (MAE): 0.0483, validation MSE: -0.6985
2025-05-03 00:26:27 [INFO]: Epoch 475 - training loss (MAE): 0.0486, validation MSE: -0.7001
2025-05-03 00:26:42 [INFO]: Epoch 476 - training loss (MAE): 0.0486, validation MSE: -0.6683
2025-05-03 00:26:58 [INFO]: Epoch 477 - training loss (MAE): 0.0492, validation MSE: -0.7191
2025-05-03 00:27:13 [INFO]: Epoch 478 - training loss (MAE): 0.0481, validation MSE: -0.7428
2025-05-03 00:27:28 [INFO]: Epoch 479 - training loss (MAE): 0.0488, validation MSE: -0.7299
2025-05-03 00:27:43 [INFO]: Epoch 480 - training loss (MAE): 0.0488, validation MSE: -0.6993
2025-05-03 00:27:58 [INFO]: Epoch 481 - training loss (MAE): 0.0481, validation MSE: -0.7013
2025-05-03 00:28:13 [INFO]: Epoch 482 - training loss (MAE): 0.0483, validation MSE: -0.7239
2025-05-03 00:28:29 [INFO]: Epoch 483 - training loss (MAE): 0.0484, validation MSE: -0.6706
2025-05-03 00:28:44 [INFO]: Epoch 484 - training loss (MAE): 0.0488, validation MSE: -0.6948
2025-05-03 00:28:59 [INFO]: Epoch 485 - training loss (MAE): 0.0483, validation MSE: -0.7273
2025-05-03 00:29:14 [INFO]: Epoch 486 - training loss (MAE): 0.0488, validation MSE: -0.7242
2025-05-03 00:29:29 [INFO]: Epoch 487 - training loss (MAE): 0.0485, validation MSE: -0.6937
2025-05-03 00:29:44 [INFO]: Epoch 488 - training loss (MAE): 0.0480, validation MSE: -0.6712
2025-05-03 00:29:59 [INFO]: Epoch 489 - training loss (MAE): 0.0485, validation MSE: -0.7278
2025-05-03 00:30:15 [INFO]: Epoch 490 - training loss (MAE): 0.0490, validation MSE: -0.7269
2025-05-03 00:30:30 [INFO]: Epoch 491 - training loss (MAE): 0.0475, validation MSE: -0.7264
2025-05-03 00:30:45 [INFO]: Epoch 492 - training loss (MAE): 0.0479, validation MSE: -0.7025
2025-05-03 00:31:00 [INFO]: Epoch 493 - training loss (MAE): 0.0483, validation MSE: -0.6957
2025-05-03 00:31:15 [INFO]: Epoch 494 - training loss (MAE): 0.0477, validation MSE: -0.6868
2025-05-03 00:31:30 [INFO]: Epoch 495 - training loss (MAE): 0.0488, validation MSE: -0.7014
2025-05-03 00:31:46 [INFO]: Epoch 496 - training loss (MAE): 0.0477, validation MSE: -0.6994
2025-05-03 00:32:01 [INFO]: Epoch 497 - training loss (MAE): 0.0479, validation MSE: -0.7246
2025-05-03 00:32:16 [INFO]: Epoch 498 - training loss (MAE): 0.0482, validation MSE: -0.7106
2025-05-03 00:32:31 [INFO]: Epoch 499 - training loss (MAE): 0.0480, validation MSE: -0.7236
2025-05-03 00:32:46 [INFO]: Epoch 500 - training loss (MAE): 0.0479, validation MSE: -0.7161
2025-05-03 00:32:46 [INFO]: Finished training. The best model is from epoch#255.
2025-05-03 00:32:47 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_4.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 00:32:47 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_4.pypots
Fold 4 metrics: MAE: 0.341, MSE: 0.441, MRE: 0.460
Fold 4 metrics: MAE: 0.341, MSE: 0.441, MRE: 0.460
Training fold 5/10
2025-05-03 00:32:47 [INFO]: No given device, using default device: cuda
2025-05-03 00:32:47 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-03 00:32:47 [INFO]: Using customized MAE as the training loss function.
2025-05-03 00:32:47 [INFO]: Using customized MSE as the validation metric function.
2025-05-03 00:32:47 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-03 00:33:02 [INFO]: Epoch 001 - training loss (MAE): 1.3272, validation MSE: -0.1066
2025-05-03 00:33:17 [INFO]: Epoch 002 - training loss (MAE): 0.8096, validation MSE: -0.2927
2025-05-03 00:33:32 [INFO]: Epoch 003 - training loss (MAE): 0.7292, validation MSE: -0.3884
2025-05-03 00:33:47 [INFO]: Epoch 004 - training loss (MAE): 0.6831, validation MSE: -0.5353
2025-05-03 00:34:03 [INFO]: Epoch 005 - training loss (MAE): 0.6442, validation MSE: -0.5001
2025-05-03 00:34:18 [INFO]: Epoch 006 - training loss (MAE): 0.6078, validation MSE: -0.6067
2025-05-03 00:34:33 [INFO]: Epoch 007 - training loss (MAE): 0.5714, validation MSE: -0.6361
2025-05-03 00:34:48 [INFO]: Epoch 008 - training loss (MAE): 0.5399, validation MSE: -0.7013
2025-05-03 00:35:03 [INFO]: Epoch 009 - training loss (MAE): 0.5168, validation MSE: -0.7441
2025-05-03 00:35:18 [INFO]: Epoch 010 - training loss (MAE): 0.4977, validation MSE: -0.7957
2025-05-03 00:35:33 [INFO]: Epoch 011 - training loss (MAE): 0.4810, validation MSE: -0.7786
2025-05-03 00:35:48 [INFO]: Epoch 012 - training loss (MAE): 0.4653, validation MSE: -0.8106
2025-05-03 00:36:03 [INFO]: Epoch 013 - training loss (MAE): 0.4530, validation MSE: -0.8030
2025-05-03 00:36:19 [INFO]: Epoch 014 - training loss (MAE): 0.4413, validation MSE: -0.8085
2025-05-03 00:36:34 [INFO]: Epoch 015 - training loss (MAE): 0.4323, validation MSE: -0.8266
2025-05-03 00:36:49 [INFO]: Epoch 016 - training loss (MAE): 0.4244, validation MSE: -0.7828
2025-05-03 00:37:03 [INFO]: Epoch 017 - training loss (MAE): 0.4177, validation MSE: -0.8167
2025-05-03 00:37:19 [INFO]: Epoch 018 - training loss (MAE): 0.4116, validation MSE: -0.8542
2025-05-03 00:37:34 [INFO]: Epoch 019 - training loss (MAE): 0.4058, validation MSE: -0.8665
2025-05-03 00:37:49 [INFO]: Epoch 020 - training loss (MAE): 0.4078, validation MSE: -0.7763
2025-05-03 00:38:04 [INFO]: Epoch 021 - training loss (MAE): 0.3980, validation MSE: -0.8422
2025-05-03 00:38:19 [INFO]: Epoch 022 - training loss (MAE): 0.3936, validation MSE: -0.8659
2025-05-03 00:38:34 [INFO]: Epoch 023 - training loss (MAE): 0.3900, validation MSE: -0.8892
2025-05-03 00:38:49 [INFO]: Epoch 024 - training loss (MAE): 0.3882, validation MSE: -0.8427
2025-05-03 00:39:05 [INFO]: Epoch 025 - training loss (MAE): 0.3854, validation MSE: -0.8488
2025-05-03 00:39:20 [INFO]: Epoch 026 - training loss (MAE): 0.3822, validation MSE: -0.8949
2025-05-03 00:39:35 [INFO]: Epoch 027 - training loss (MAE): 0.3809, validation MSE: -0.8736
2025-05-03 00:39:50 [INFO]: Epoch 028 - training loss (MAE): 0.3792, validation MSE: -0.8596
2025-05-03 00:40:05 [INFO]: Epoch 029 - training loss (MAE): 0.3781, validation MSE: -0.8664
2025-05-03 00:40:21 [INFO]: Epoch 030 - training loss (MAE): 0.3762, validation MSE: -0.8653
2025-05-03 00:40:36 [INFO]: Epoch 031 - training loss (MAE): 0.3759, validation MSE: -0.8765
2025-05-03 00:40:51 [INFO]: Epoch 032 - training loss (MAE): 0.3736, validation MSE: -0.8573
2025-05-03 00:41:07 [INFO]: Epoch 033 - training loss (MAE): 0.3727, validation MSE: -0.8631
2025-05-03 00:41:22 [INFO]: Epoch 034 - training loss (MAE): 0.3727, validation MSE: -0.8809
2025-05-03 00:41:37 [INFO]: Epoch 035 - training loss (MAE): 0.3708, validation MSE: -0.8712
2025-05-03 00:41:52 [INFO]: Epoch 036 - training loss (MAE): 0.3697, validation MSE: -0.8953
2025-05-03 00:42:07 [INFO]: Epoch 037 - training loss (MAE): 0.3703, validation MSE: -0.9255
2025-05-03 00:42:22 [INFO]: Epoch 038 - training loss (MAE): 0.3688, validation MSE: -0.8780
2025-05-03 00:42:38 [INFO]: Epoch 039 - training loss (MAE): 0.3669, validation MSE: -0.8858
2025-05-03 00:42:53 [INFO]: Epoch 040 - training loss (MAE): 0.3664, validation MSE: -0.8783
2025-05-03 00:43:08 [INFO]: Epoch 041 - training loss (MAE): 0.3657, validation MSE: -0.8765
2025-05-03 00:43:23 [INFO]: Epoch 042 - training loss (MAE): 0.3650, validation MSE: -0.8894
2025-05-03 00:43:38 [INFO]: Epoch 043 - training loss (MAE): 0.3645, validation MSE: -0.8626
2025-05-03 00:43:53 [INFO]: Epoch 044 - training loss (MAE): 0.3637, validation MSE: -0.8970
2025-05-03 00:44:08 [INFO]: Epoch 045 - training loss (MAE): 0.3632, validation MSE: -0.8928
2025-05-03 00:44:23 [INFO]: Epoch 046 - training loss (MAE): 0.3629, validation MSE: -0.8800
2025-05-03 00:44:38 [INFO]: Epoch 047 - training loss (MAE): 0.3623, validation MSE: -0.8699
2025-05-03 00:44:53 [INFO]: Epoch 048 - training loss (MAE): 0.3621, validation MSE: -0.8910
2025-05-03 00:45:08 [INFO]: Epoch 049 - training loss (MAE): 0.3297, validation MSE: -0.7668
2025-05-03 00:45:24 [INFO]: Epoch 050 - training loss (MAE): 0.2264, validation MSE: -0.8820
2025-05-03 00:45:39 [INFO]: Epoch 051 - training loss (MAE): 0.2000, validation MSE: -0.8745
2025-05-03 00:45:54 [INFO]: Epoch 052 - training loss (MAE): 0.1829, validation MSE: -0.8868
2025-05-03 00:46:09 [INFO]: Epoch 053 - training loss (MAE): 0.1729, validation MSE: -0.8802
2025-05-03 00:46:24 [INFO]: Epoch 054 - training loss (MAE): 0.1632, validation MSE: -0.8756
2025-05-03 00:46:39 [INFO]: Epoch 055 - training loss (MAE): 0.1557, validation MSE: -0.8728
2025-05-03 00:46:54 [INFO]: Epoch 056 - training loss (MAE): 0.1501, validation MSE: -0.8869
2025-05-03 00:47:09 [INFO]: Epoch 057 - training loss (MAE): 0.1429, validation MSE: -0.8878
2025-05-03 00:47:24 [INFO]: Epoch 058 - training loss (MAE): 0.1373, validation MSE: -0.8858
2025-05-03 00:47:40 [INFO]: Epoch 059 - training loss (MAE): 0.1323, validation MSE: -0.8856
2025-05-03 00:47:55 [INFO]: Epoch 060 - training loss (MAE): 0.1282, validation MSE: -0.8955
2025-05-03 00:48:10 [INFO]: Epoch 061 - training loss (MAE): 0.1244, validation MSE: -0.8845
2025-05-03 00:48:26 [INFO]: Epoch 062 - training loss (MAE): 0.1197, validation MSE: -0.8843
2025-05-03 00:48:41 [INFO]: Epoch 063 - training loss (MAE): 0.1168, validation MSE: -0.8797
2025-05-03 00:48:56 [INFO]: Epoch 064 - training loss (MAE): 0.1160, validation MSE: -0.8945
2025-05-03 00:49:11 [INFO]: Epoch 065 - training loss (MAE): 0.1111, validation MSE: -0.8714
2025-05-03 00:49:27 [INFO]: Epoch 066 - training loss (MAE): 0.1076, validation MSE: -0.8784
2025-05-03 00:49:42 [INFO]: Epoch 067 - training loss (MAE): 0.1062, validation MSE: -0.9325
2025-05-03 00:49:57 [INFO]: Epoch 068 - training loss (MAE): 0.1056, validation MSE: -0.9276
2025-05-03 00:50:12 [INFO]: Epoch 069 - training loss (MAE): 0.1018, validation MSE: -0.8755
2025-05-03 00:50:28 [INFO]: Epoch 070 - training loss (MAE): 0.0998, validation MSE: -0.8970
2025-05-03 00:50:43 [INFO]: Epoch 071 - training loss (MAE): 0.0990, validation MSE: -0.8833
2025-05-03 00:50:58 [INFO]: Epoch 072 - training loss (MAE): 0.0966, validation MSE: -0.9009
2025-05-03 00:51:13 [INFO]: Epoch 073 - training loss (MAE): 0.0955, validation MSE: -0.8842
2025-05-03 00:51:28 [INFO]: Epoch 074 - training loss (MAE): 0.0944, validation MSE: -0.9059
2025-05-03 00:51:43 [INFO]: Epoch 075 - training loss (MAE): 0.0940, validation MSE: -0.8795
2025-05-03 00:51:58 [INFO]: Epoch 076 - training loss (MAE): 0.0924, validation MSE: -0.8977
2025-05-03 00:52:14 [INFO]: Epoch 077 - training loss (MAE): 0.0906, validation MSE: -0.8875
2025-05-03 00:52:29 [INFO]: Epoch 078 - training loss (MAE): 0.0908, validation MSE: -0.8187
2025-05-03 00:52:44 [INFO]: Epoch 079 - training loss (MAE): 0.0888, validation MSE: -0.8800
2025-05-03 00:53:00 [INFO]: Epoch 080 - training loss (MAE): 0.0882, validation MSE: -0.8887
2025-05-03 00:53:15 [INFO]: Epoch 081 - training loss (MAE): 0.0873, validation MSE: -0.8862
2025-05-03 00:53:30 [INFO]: Epoch 082 - training loss (MAE): 0.0860, validation MSE: -0.8648
2025-05-03 00:53:45 [INFO]: Epoch 083 - training loss (MAE): 0.0851, validation MSE: -0.8881
2025-05-03 00:54:00 [INFO]: Epoch 084 - training loss (MAE): 0.0852, validation MSE: -0.8849
2025-05-03 00:54:15 [INFO]: Epoch 085 - training loss (MAE): 0.0836, validation MSE: -0.8264
2025-05-03 00:54:30 [INFO]: Epoch 086 - training loss (MAE): 0.0832, validation MSE: -0.8793
2025-05-03 00:54:46 [INFO]: Epoch 087 - training loss (MAE): 0.0832, validation MSE: -0.9013
2025-05-03 00:55:01 [INFO]: Epoch 088 - training loss (MAE): 0.0829, validation MSE: -0.8860
2025-05-03 00:55:16 [INFO]: Epoch 089 - training loss (MAE): 0.0814, validation MSE: -0.8559
2025-05-03 00:55:31 [INFO]: Epoch 090 - training loss (MAE): 0.0818, validation MSE: -0.8525
2025-05-03 00:55:46 [INFO]: Epoch 091 - training loss (MAE): 0.0802, validation MSE: -0.8667
2025-05-03 00:56:01 [INFO]: Epoch 092 - training loss (MAE): 0.0801, validation MSE: -0.8680
2025-05-03 00:56:16 [INFO]: Epoch 093 - training loss (MAE): 0.0797, validation MSE: -0.8663
2025-05-03 00:56:32 [INFO]: Epoch 094 - training loss (MAE): 0.0788, validation MSE: -0.8579
2025-05-03 00:56:47 [INFO]: Epoch 095 - training loss (MAE): 0.0787, validation MSE: -0.8613
2025-05-03 00:57:02 [INFO]: Epoch 096 - training loss (MAE): 0.0782, validation MSE: -0.8433
2025-05-03 00:57:17 [INFO]: Epoch 097 - training loss (MAE): 0.0774, validation MSE: -0.8775
2025-05-03 00:57:32 [INFO]: Epoch 098 - training loss (MAE): 0.0771, validation MSE: -0.8921
2025-05-03 00:57:47 [INFO]: Epoch 099 - training loss (MAE): 0.0774, validation MSE: -0.8222
2025-05-03 00:58:03 [INFO]: Epoch 100 - training loss (MAE): 0.0771, validation MSE: -0.8427
2025-05-03 00:58:18 [INFO]: Epoch 101 - training loss (MAE): 0.0769, validation MSE: -0.8945
2025-05-03 00:58:33 [INFO]: Epoch 102 - training loss (MAE): 0.0763, validation MSE: -0.8221
2025-05-03 00:58:48 [INFO]: Epoch 103 - training loss (MAE): 0.0766, validation MSE: -0.8795
2025-05-03 00:59:03 [INFO]: Epoch 104 - training loss (MAE): 0.0749, validation MSE: -0.8892
2025-05-03 00:59:18 [INFO]: Epoch 105 - training loss (MAE): 0.0748, validation MSE: -0.8865
2025-05-03 00:59:34 [INFO]: Epoch 106 - training loss (MAE): 0.0749, validation MSE: -0.8468
2025-05-03 00:59:49 [INFO]: Epoch 107 - training loss (MAE): 0.0754, validation MSE: -0.8798
2025-05-03 01:00:04 [INFO]: Epoch 108 - training loss (MAE): 0.0739, validation MSE: -0.8819
2025-05-03 01:00:19 [INFO]: Epoch 109 - training loss (MAE): 0.0736, validation MSE: -0.8685
2025-05-03 01:00:35 [INFO]: Epoch 110 - training loss (MAE): 0.0742, validation MSE: -0.8587
2025-05-03 01:00:50 [INFO]: Epoch 111 - training loss (MAE): 0.0739, validation MSE: -0.9049
2025-05-03 01:01:05 [INFO]: Epoch 112 - training loss (MAE): 0.0733, validation MSE: -0.8666
2025-05-03 01:01:20 [INFO]: Epoch 113 - training loss (MAE): 0.0734, validation MSE: -0.8351
2025-05-03 01:01:35 [INFO]: Epoch 114 - training loss (MAE): 0.0725, validation MSE: -0.8836
2025-05-03 01:01:50 [INFO]: Epoch 115 - training loss (MAE): 0.0730, validation MSE: -0.8473
2025-05-03 01:02:05 [INFO]: Epoch 116 - training loss (MAE): 0.0726, validation MSE: -0.8690
2025-05-03 01:02:21 [INFO]: Epoch 117 - training loss (MAE): 0.0729, validation MSE: -0.9071
2025-05-03 01:02:36 [INFO]: Epoch 118 - training loss (MAE): 0.0727, validation MSE: -0.8765
2025-05-03 01:02:51 [INFO]: Epoch 119 - training loss (MAE): 0.0724, validation MSE: -0.8678
2025-05-03 01:03:07 [INFO]: Epoch 120 - training loss (MAE): 0.0720, validation MSE: -0.8897
2025-05-03 01:03:22 [INFO]: Epoch 121 - training loss (MAE): 0.0719, validation MSE: -0.8590
2025-05-03 01:03:37 [INFO]: Epoch 122 - training loss (MAE): 0.0716, validation MSE: -0.8957
2025-05-03 01:03:52 [INFO]: Epoch 123 - training loss (MAE): 0.0713, validation MSE: -0.8907
2025-05-03 01:04:08 [INFO]: Epoch 124 - training loss (MAE): 0.0712, validation MSE: -0.8717
2025-05-03 01:04:23 [INFO]: Epoch 125 - training loss (MAE): 0.0718, validation MSE: -0.9089
2025-05-03 01:04:38 [INFO]: Epoch 126 - training loss (MAE): 0.0712, validation MSE: -0.9011
2025-05-03 01:04:53 [INFO]: Epoch 127 - training loss (MAE): 0.0719, validation MSE: -0.8707
2025-05-03 01:05:09 [INFO]: Epoch 128 - training loss (MAE): 0.0706, validation MSE: -0.8687
2025-05-03 01:05:24 [INFO]: Epoch 129 - training loss (MAE): 0.0706, validation MSE: -0.8736
2025-05-03 01:05:39 [INFO]: Epoch 130 - training loss (MAE): 0.0706, validation MSE: -0.9020
2025-05-03 01:05:54 [INFO]: Epoch 131 - training loss (MAE): 0.0706, validation MSE: -0.8581
2025-05-03 01:06:09 [INFO]: Epoch 132 - training loss (MAE): 0.0708, validation MSE: -0.9173
2025-05-03 01:06:25 [INFO]: Epoch 133 - training loss (MAE): 0.0703, validation MSE: -0.9191
2025-05-03 01:06:40 [INFO]: Epoch 134 - training loss (MAE): 0.0701, validation MSE: -0.8745
2025-05-03 01:06:55 [INFO]: Epoch 135 - training loss (MAE): 0.0698, validation MSE: -0.8590
2025-05-03 01:07:10 [INFO]: Epoch 136 - training loss (MAE): 0.0713, validation MSE: -0.9083
2025-05-03 01:07:25 [INFO]: Epoch 137 - training loss (MAE): 0.0692, validation MSE: -0.8816
2025-05-03 01:07:41 [INFO]: Epoch 138 - training loss (MAE): 0.0704, validation MSE: -0.8741
2025-05-03 01:07:56 [INFO]: Epoch 139 - training loss (MAE): 0.0691, validation MSE: -0.8523
2025-05-03 01:08:11 [INFO]: Epoch 140 - training loss (MAE): 0.0696, validation MSE: -0.8714
2025-05-03 01:08:26 [INFO]: Epoch 141 - training loss (MAE): 0.0698, validation MSE: -0.8382
2025-05-03 01:08:42 [INFO]: Epoch 142 - training loss (MAE): 0.0688, validation MSE: -0.8665
2025-05-03 01:08:57 [INFO]: Epoch 143 - training loss (MAE): 0.0688, validation MSE: -0.9045
2025-05-03 01:09:12 [INFO]: Epoch 144 - training loss (MAE): 0.0703, validation MSE: -0.8676
2025-05-03 01:09:27 [INFO]: Epoch 145 - training loss (MAE): 0.0688, validation MSE: -0.8822
2025-05-03 01:09:42 [INFO]: Epoch 146 - training loss (MAE): 0.0696, validation MSE: -0.8554
2025-05-03 01:09:57 [INFO]: Epoch 147 - training loss (MAE): 0.0690, validation MSE: -0.8588
2025-05-03 01:10:12 [INFO]: Epoch 148 - training loss (MAE): 0.0688, validation MSE: -0.8666
2025-05-03 01:10:28 [INFO]: Epoch 149 - training loss (MAE): 0.0683, validation MSE: -0.8751
2025-05-03 01:10:43 [INFO]: Epoch 150 - training loss (MAE): 0.0682, validation MSE: -0.8496
2025-05-03 01:10:58 [INFO]: Epoch 151 - training loss (MAE): 0.0680, validation MSE: -0.8484
2025-05-03 01:11:14 [INFO]: Epoch 152 - training loss (MAE): 0.0685, validation MSE: -0.8726
2025-05-03 01:11:29 [INFO]: Epoch 153 - training loss (MAE): 0.0682, validation MSE: -0.9013
2025-05-03 01:11:44 [INFO]: Epoch 154 - training loss (MAE): 0.0675, validation MSE: -0.8510
2025-05-03 01:11:59 [INFO]: Epoch 155 - training loss (MAE): 0.0681, validation MSE: -0.9019
2025-05-03 01:12:14 [INFO]: Epoch 156 - training loss (MAE): 0.0679, validation MSE: -0.8666
2025-05-03 01:12:30 [INFO]: Epoch 157 - training loss (MAE): 0.0682, validation MSE: -0.8739
2025-05-03 01:12:45 [INFO]: Epoch 158 - training loss (MAE): 0.0690, validation MSE: -0.8582
2025-05-03 01:13:00 [INFO]: Epoch 159 - training loss (MAE): 0.0677, validation MSE: -0.8501
2025-05-03 01:13:15 [INFO]: Epoch 160 - training loss (MAE): 0.0684, validation MSE: -0.8878
2025-05-03 01:13:31 [INFO]: Epoch 161 - training loss (MAE): 0.0671, validation MSE: -0.9079
2025-05-03 01:13:46 [INFO]: Epoch 162 - training loss (MAE): 0.0666, validation MSE: -0.8825
2025-05-03 01:14:01 [INFO]: Epoch 163 - training loss (MAE): 0.0672, validation MSE: -0.9139
2025-05-03 01:14:16 [INFO]: Epoch 164 - training loss (MAE): 0.0669, validation MSE: -0.8747
2025-05-03 01:14:32 [INFO]: Epoch 165 - training loss (MAE): 0.0669, validation MSE: -0.9133
2025-05-03 01:14:47 [INFO]: Epoch 166 - training loss (MAE): 0.0669, validation MSE: -0.8875
2025-05-03 01:15:02 [INFO]: Epoch 167 - training loss (MAE): 0.0677, validation MSE: -0.8473
2025-05-03 01:15:18 [INFO]: Epoch 168 - training loss (MAE): 0.0670, validation MSE: -0.9134
2025-05-03 01:15:33 [INFO]: Epoch 169 - training loss (MAE): 0.0667, validation MSE: -0.8541
2025-05-03 01:15:49 [INFO]: Epoch 170 - training loss (MAE): 0.0665, validation MSE: -0.9038
2025-05-03 01:16:04 [INFO]: Epoch 171 - training loss (MAE): 0.0668, validation MSE: -0.8938
2025-05-03 01:16:19 [INFO]: Epoch 172 - training loss (MAE): 0.0664, validation MSE: -0.8842
2025-05-03 01:16:34 [INFO]: Epoch 173 - training loss (MAE): 0.0664, validation MSE: -0.8859
2025-05-03 01:16:50 [INFO]: Epoch 174 - training loss (MAE): 0.0664, validation MSE: -0.8236
2025-05-03 01:17:05 [INFO]: Epoch 175 - training loss (MAE): 0.0664, validation MSE: -0.8841
2025-05-03 01:17:20 [INFO]: Epoch 176 - training loss (MAE): 0.0668, validation MSE: -0.8813
2025-05-03 01:17:35 [INFO]: Epoch 177 - training loss (MAE): 0.0663, validation MSE: -0.8586
2025-05-03 01:17:51 [INFO]: Epoch 178 - training loss (MAE): 0.0658, validation MSE: -0.8959
2025-05-03 01:18:05 [INFO]: Epoch 179 - training loss (MAE): 0.0665, validation MSE: -0.8505
2025-05-03 01:18:21 [INFO]: Epoch 180 - training loss (MAE): 0.0656, validation MSE: -0.8403
2025-05-03 01:18:36 [INFO]: Epoch 181 - training loss (MAE): 0.0658, validation MSE: -0.8802
2025-05-03 01:18:51 [INFO]: Epoch 182 - training loss (MAE): 0.0670, validation MSE: -0.9192
2025-05-03 01:19:06 [INFO]: Epoch 183 - training loss (MAE): 0.0657, validation MSE: -0.8960
2025-05-03 01:19:21 [INFO]: Epoch 184 - training loss (MAE): 0.0661, validation MSE: -0.8052
2025-05-03 01:19:36 [INFO]: Epoch 185 - training loss (MAE): 0.0667, validation MSE: -0.8733
2025-05-03 01:19:52 [INFO]: Epoch 186 - training loss (MAE): 0.0651, validation MSE: -0.8755
2025-05-03 01:20:07 [INFO]: Epoch 187 - training loss (MAE): 0.0656, validation MSE: -0.8521
2025-05-03 01:20:22 [INFO]: Epoch 188 - training loss (MAE): 0.0654, validation MSE: -0.9201
2025-05-03 01:20:37 [INFO]: Epoch 189 - training loss (MAE): 0.0661, validation MSE: -0.8256
2025-05-03 01:20:52 [INFO]: Epoch 190 - training loss (MAE): 0.0657, validation MSE: -0.9336
2025-05-03 01:21:07 [INFO]: Epoch 191 - training loss (MAE): 0.0648, validation MSE: -0.8799
2025-05-03 01:21:23 [INFO]: Epoch 192 - training loss (MAE): 0.0654, validation MSE: -0.9192
2025-05-03 01:21:38 [INFO]: Epoch 193 - training loss (MAE): 0.0650, validation MSE: -0.9657
2025-05-03 01:21:53 [INFO]: Epoch 194 - training loss (MAE): 0.0652, validation MSE: -0.9085
2025-05-03 01:22:08 [INFO]: Epoch 195 - training loss (MAE): 0.0649, validation MSE: -0.8905
2025-05-03 01:22:23 [INFO]: Epoch 196 - training loss (MAE): 0.0653, validation MSE: -0.8823
2025-05-03 01:22:38 [INFO]: Epoch 197 - training loss (MAE): 0.0652, validation MSE: -0.8752
2025-05-03 01:22:54 [INFO]: Epoch 198 - training loss (MAE): 0.0651, validation MSE: -0.9029
2025-05-03 01:23:09 [INFO]: Epoch 199 - training loss (MAE): 0.0650, validation MSE: -0.9189
2025-05-03 01:23:24 [INFO]: Epoch 200 - training loss (MAE): 0.0651, validation MSE: -0.8778
2025-05-03 01:23:39 [INFO]: Epoch 201 - training loss (MAE): 0.0647, validation MSE: -0.9212
2025-05-03 01:23:54 [INFO]: Epoch 202 - training loss (MAE): 0.0654, validation MSE: -0.8835
2025-05-03 01:24:09 [INFO]: Epoch 203 - training loss (MAE): 0.0642, validation MSE: -0.8793
2025-05-03 01:24:24 [INFO]: Epoch 204 - training loss (MAE): 0.0647, validation MSE: -0.9023
2025-05-03 01:24:40 [INFO]: Epoch 205 - training loss (MAE): 0.0648, validation MSE: -0.8530
2025-05-03 01:24:55 [INFO]: Epoch 206 - training loss (MAE): 0.0643, validation MSE: -0.9034
2025-05-03 01:25:10 [INFO]: Epoch 207 - training loss (MAE): 0.0641, validation MSE: -0.8493
2025-05-03 01:25:25 [INFO]: Epoch 208 - training loss (MAE): 0.0641, validation MSE: -0.9039
2025-05-03 01:25:40 [INFO]: Epoch 209 - training loss (MAE): 0.0640, validation MSE: -0.8626
2025-05-03 01:25:55 [INFO]: Epoch 210 - training loss (MAE): 0.0647, validation MSE: -0.8781
2025-05-03 01:26:10 [INFO]: Epoch 211 - training loss (MAE): 0.0644, validation MSE: -0.8669
2025-05-03 01:26:25 [INFO]: Epoch 212 - training loss (MAE): 0.0644, validation MSE: -0.8883
2025-05-03 01:26:41 [INFO]: Epoch 213 - training loss (MAE): 0.0639, validation MSE: -0.8633
2025-05-03 01:26:56 [INFO]: Epoch 214 - training loss (MAE): 0.0645, validation MSE: -0.8943
2025-05-03 01:27:11 [INFO]: Epoch 215 - training loss (MAE): 0.0638, validation MSE: -0.9014
2025-05-03 01:27:26 [INFO]: Epoch 216 - training loss (MAE): 0.0635, validation MSE: -0.9170
2025-05-03 01:27:42 [INFO]: Epoch 217 - training loss (MAE): 0.0637, validation MSE: -0.9010
2025-05-03 01:27:57 [INFO]: Epoch 218 - training loss (MAE): 0.0642, validation MSE: -0.8760
2025-05-03 01:28:12 [INFO]: Epoch 219 - training loss (MAE): 0.0640, validation MSE: -0.8752
2025-05-03 01:28:27 [INFO]: Epoch 220 - training loss (MAE): 0.0642, validation MSE: -0.9229
2025-05-03 01:28:42 [INFO]: Epoch 221 - training loss (MAE): 0.0637, validation MSE: -0.9229
2025-05-03 01:28:58 [INFO]: Epoch 222 - training loss (MAE): 0.0646, validation MSE: -0.8515
2025-05-03 01:29:13 [INFO]: Epoch 223 - training loss (MAE): 0.0636, validation MSE: -0.8961
2025-05-03 01:29:28 [INFO]: Epoch 224 - training loss (MAE): 0.0637, validation MSE: -0.8825
2025-05-03 01:29:43 [INFO]: Epoch 225 - training loss (MAE): 0.0637, validation MSE: -0.9315
2025-05-03 01:29:59 [INFO]: Epoch 226 - training loss (MAE): 0.0637, validation MSE: -0.9496
2025-05-03 01:30:14 [INFO]: Epoch 227 - training loss (MAE): 0.0646, validation MSE: -0.8800
2025-05-03 01:30:29 [INFO]: Epoch 228 - training loss (MAE): 0.0637, validation MSE: -0.8506
2025-05-03 01:30:44 [INFO]: Epoch 229 - training loss (MAE): 0.0643, validation MSE: -0.8936
2025-05-03 01:31:00 [INFO]: Epoch 230 - training loss (MAE): 0.0632, validation MSE: -0.8794
2025-05-03 01:31:15 [INFO]: Epoch 231 - training loss (MAE): 0.0632, validation MSE: -0.9138
2025-05-03 01:31:30 [INFO]: Epoch 232 - training loss (MAE): 0.0631, validation MSE: -0.9097
2025-05-03 01:31:45 [INFO]: Epoch 233 - training loss (MAE): 0.0633, validation MSE: -0.8910
2025-05-03 01:32:01 [INFO]: Epoch 234 - training loss (MAE): 0.0635, validation MSE: -0.9072
2025-05-03 01:32:16 [INFO]: Epoch 235 - training loss (MAE): 0.0634, validation MSE: -0.9276
2025-05-03 01:32:31 [INFO]: Epoch 236 - training loss (MAE): 0.0638, validation MSE: -0.9351
2025-05-03 01:32:46 [INFO]: Epoch 237 - training loss (MAE): 0.0631, validation MSE: -0.9437
2025-05-03 01:33:01 [INFO]: Epoch 238 - training loss (MAE): 0.0633, validation MSE: -0.9084
2025-05-03 01:33:16 [INFO]: Epoch 239 - training loss (MAE): 0.0626, validation MSE: -0.8780
2025-05-03 01:33:32 [INFO]: Epoch 240 - training loss (MAE): 0.0631, validation MSE: -0.9307
2025-05-03 01:33:47 [INFO]: Epoch 241 - training loss (MAE): 0.0625, validation MSE: -0.8529
2025-05-03 01:34:02 [INFO]: Epoch 242 - training loss (MAE): 0.0624, validation MSE: -0.8911
2025-05-03 01:34:17 [INFO]: Epoch 243 - training loss (MAE): 0.0632, validation MSE: -0.8738
2025-05-03 01:34:32 [INFO]: Epoch 244 - training loss (MAE): 0.0625, validation MSE: -0.9461
2025-05-03 01:34:48 [INFO]: Epoch 245 - training loss (MAE): 0.0632, validation MSE: -0.8844
2025-05-03 01:35:03 [INFO]: Epoch 246 - training loss (MAE): 0.0628, validation MSE: -0.9418
2025-05-03 01:35:18 [INFO]: Epoch 247 - training loss (MAE): 0.0634, validation MSE: -0.9244
2025-05-03 01:35:33 [INFO]: Epoch 248 - training loss (MAE): 0.0628, validation MSE: -0.8929
2025-05-03 01:35:48 [INFO]: Epoch 249 - training loss (MAE): 0.0624, validation MSE: -0.8749
2025-05-03 01:36:03 [INFO]: Epoch 250 - training loss (MAE): 0.0633, validation MSE: -0.8716
2025-05-03 01:36:18 [INFO]: Epoch 251 - training loss (MAE): 0.0625, validation MSE: -0.8853
2025-05-03 01:36:33 [INFO]: Epoch 252 - training loss (MAE): 0.0624, validation MSE: -0.8665
2025-05-03 01:36:49 [INFO]: Epoch 253 - training loss (MAE): 0.0627, validation MSE: -0.9146
2025-05-03 01:37:04 [INFO]: Epoch 254 - training loss (MAE): 0.0622, validation MSE: -0.8513
2025-05-03 01:37:19 [INFO]: Epoch 255 - training loss (MAE): 0.0626, validation MSE: -0.9319
2025-05-03 01:37:34 [INFO]: Epoch 256 - training loss (MAE): 0.0624, validation MSE: -0.9768
2025-05-03 01:37:49 [INFO]: Epoch 257 - training loss (MAE): 0.0629, validation MSE: -0.9049
2025-05-03 01:38:04 [INFO]: Epoch 258 - training loss (MAE): 0.0624, validation MSE: -0.9188
2025-05-03 01:38:19 [INFO]: Epoch 259 - training loss (MAE): 0.0619, validation MSE: -0.9404
2025-05-03 01:38:35 [INFO]: Epoch 260 - training loss (MAE): 0.0618, validation MSE: -0.9039
2025-05-03 01:38:50 [INFO]: Epoch 261 - training loss (MAE): 0.0621, validation MSE: -0.9187
2025-05-03 01:39:05 [INFO]: Epoch 262 - training loss (MAE): 0.0628, validation MSE: -0.9433
2025-05-03 01:39:20 [INFO]: Epoch 263 - training loss (MAE): 0.0619, validation MSE: -0.9475
2025-05-03 01:39:36 [INFO]: Epoch 264 - training loss (MAE): 0.0618, validation MSE: -0.9129
2025-05-03 01:39:51 [INFO]: Epoch 265 - training loss (MAE): 0.0629, validation MSE: -0.9076
2025-05-03 01:40:06 [INFO]: Epoch 266 - training loss (MAE): 0.0622, validation MSE: -0.9179
2025-05-03 01:40:22 [INFO]: Epoch 267 - training loss (MAE): 0.0620, validation MSE: -0.9271
2025-05-03 01:40:37 [INFO]: Epoch 268 - training loss (MAE): 0.0617, validation MSE: -0.8850
2025-05-03 01:40:52 [INFO]: Epoch 269 - training loss (MAE): 0.0623, validation MSE: -0.9435
2025-05-03 01:41:07 [INFO]: Epoch 270 - training loss (MAE): 0.0616, validation MSE: -0.9026
2025-05-03 01:41:22 [INFO]: Epoch 271 - training loss (MAE): 0.0631, validation MSE: -0.9533
2025-05-03 01:41:37 [INFO]: Epoch 272 - training loss (MAE): 0.0626, validation MSE: -0.9318
2025-05-03 01:41:53 [INFO]: Epoch 273 - training loss (MAE): 0.0616, validation MSE: -0.9289
2025-05-03 01:42:08 [INFO]: Epoch 274 - training loss (MAE): 0.0609, validation MSE: -0.9318
2025-05-03 01:42:23 [INFO]: Epoch 275 - training loss (MAE): 0.0615, validation MSE: -0.8814
2025-05-03 01:42:38 [INFO]: Epoch 276 - training loss (MAE): 0.0614, validation MSE: -0.9436
2025-05-03 01:42:54 [INFO]: Epoch 277 - training loss (MAE): 0.0624, validation MSE: -0.8992
2025-05-03 01:43:09 [INFO]: Epoch 278 - training loss (MAE): 0.0612, validation MSE: -0.8886
2025-05-03 01:43:24 [INFO]: Epoch 279 - training loss (MAE): 0.0629, validation MSE: -0.9014
2025-05-03 01:43:39 [INFO]: Epoch 280 - training loss (MAE): 0.0610, validation MSE: -0.8940
2025-05-03 01:43:54 [INFO]: Epoch 281 - training loss (MAE): 0.0614, validation MSE: -0.8718
2025-05-03 01:44:09 [INFO]: Epoch 282 - training loss (MAE): 0.0611, validation MSE: -0.9358
2025-05-03 01:44:25 [INFO]: Epoch 283 - training loss (MAE): 0.0611, validation MSE: -0.9362
2025-05-03 01:44:40 [INFO]: Epoch 284 - training loss (MAE): 0.0619, validation MSE: -0.8881
2025-05-03 01:44:55 [INFO]: Epoch 285 - training loss (MAE): 0.0612, validation MSE: -0.8905
2025-05-03 01:45:10 [INFO]: Epoch 286 - training loss (MAE): 0.0612, validation MSE: -0.9401
2025-05-03 01:45:25 [INFO]: Epoch 287 - training loss (MAE): 0.0622, validation MSE: -0.9156
2025-05-03 01:45:41 [INFO]: Epoch 288 - training loss (MAE): 0.0622, validation MSE: -0.9089
2025-05-03 01:45:56 [INFO]: Epoch 289 - training loss (MAE): 0.0615, validation MSE: -0.9407
2025-05-03 01:46:11 [INFO]: Epoch 290 - training loss (MAE): 0.0614, validation MSE: -0.9492
2025-05-03 01:46:26 [INFO]: Epoch 291 - training loss (MAE): 0.0608, validation MSE: -0.9022
2025-05-03 01:46:41 [INFO]: Epoch 292 - training loss (MAE): 0.0612, validation MSE: -0.9110
2025-05-03 01:46:56 [INFO]: Epoch 293 - training loss (MAE): 0.0604, validation MSE: -0.8873
2025-05-03 01:47:11 [INFO]: Epoch 294 - training loss (MAE): 0.0604, validation MSE: -0.9443
2025-05-03 01:47:26 [INFO]: Epoch 295 - training loss (MAE): 0.0607, validation MSE: -0.9938
2025-05-03 01:47:42 [INFO]: Epoch 296 - training loss (MAE): 0.0610, validation MSE: -0.9825
2025-05-03 01:47:57 [INFO]: Epoch 297 - training loss (MAE): 0.0607, validation MSE: -1.0117
2025-05-03 01:48:12 [INFO]: Epoch 298 - training loss (MAE): 0.0615, validation MSE: -0.9525
2025-05-03 01:48:27 [INFO]: Epoch 299 - training loss (MAE): 0.0610, validation MSE: -0.9912
2025-05-03 01:48:43 [INFO]: Epoch 300 - training loss (MAE): 0.0613, validation MSE: -0.8939
2025-05-03 01:48:58 [INFO]: Epoch 301 - training loss (MAE): 0.0609, validation MSE: -0.9553
2025-05-03 01:49:13 [INFO]: Epoch 302 - training loss (MAE): 0.0610, validation MSE: -0.8982
2025-05-03 01:49:28 [INFO]: Epoch 303 - training loss (MAE): 0.0606, validation MSE: -0.9342
2025-05-03 01:49:43 [INFO]: Epoch 304 - training loss (MAE): 0.0612, validation MSE: -0.9346
2025-05-03 01:49:58 [INFO]: Epoch 305 - training loss (MAE): 0.0611, validation MSE: -0.9250
2025-05-03 01:50:13 [INFO]: Epoch 306 - training loss (MAE): 0.0607, validation MSE: -0.9238
2025-05-03 01:50:28 [INFO]: Epoch 307 - training loss (MAE): 0.0606, validation MSE: -0.8952
2025-05-03 01:50:43 [INFO]: Epoch 308 - training loss (MAE): 0.0604, validation MSE: -0.9076
2025-05-03 01:50:59 [INFO]: Epoch 309 - training loss (MAE): 0.0608, validation MSE: -0.9658
2025-05-03 01:51:14 [INFO]: Epoch 310 - training loss (MAE): 0.0603, validation MSE: -0.9125
2025-05-03 01:51:29 [INFO]: Epoch 311 - training loss (MAE): 0.0603, validation MSE: -0.9718
2025-05-03 01:51:44 [INFO]: Epoch 312 - training loss (MAE): 0.0609, validation MSE: -0.9521
2025-05-03 01:51:59 [INFO]: Epoch 313 - training loss (MAE): 0.0603, validation MSE: -0.9049
2025-05-03 01:52:14 [INFO]: Epoch 314 - training loss (MAE): 0.0608, validation MSE: -0.9541
2025-05-03 01:52:30 [INFO]: Epoch 315 - training loss (MAE): 0.0603, validation MSE: -0.9296
2025-05-03 01:52:45 [INFO]: Epoch 316 - training loss (MAE): 0.0602, validation MSE: -0.9051
2025-05-03 01:53:00 [INFO]: Epoch 317 - training loss (MAE): 0.0607, validation MSE: -0.9281
2025-05-03 01:53:16 [INFO]: Epoch 318 - training loss (MAE): 0.0603, validation MSE: -0.9470
2025-05-03 01:53:31 [INFO]: Epoch 319 - training loss (MAE): 0.0602, validation MSE: -0.9920
2025-05-03 01:53:46 [INFO]: Epoch 320 - training loss (MAE): 0.0603, validation MSE: -0.9832
2025-05-03 01:54:01 [INFO]: Epoch 321 - training loss (MAE): 0.0608, validation MSE: -0.9634
2025-05-03 01:54:16 [INFO]: Epoch 322 - training loss (MAE): 0.0601, validation MSE: -0.9633
2025-05-03 01:54:32 [INFO]: Epoch 323 - training loss (MAE): 0.0600, validation MSE: -0.9733
2025-05-03 01:54:47 [INFO]: Epoch 324 - training loss (MAE): 0.0603, validation MSE: -0.9332
2025-05-03 01:55:02 [INFO]: Epoch 325 - training loss (MAE): 0.0600, validation MSE: -0.9183
2025-05-03 01:55:17 [INFO]: Epoch 326 - training loss (MAE): 0.0604, validation MSE: -0.9600
2025-05-03 01:55:33 [INFO]: Epoch 327 - training loss (MAE): 0.0600, validation MSE: -0.9169
2025-05-03 01:55:48 [INFO]: Epoch 328 - training loss (MAE): 0.0597, validation MSE: -0.9541
2025-05-03 01:56:03 [INFO]: Epoch 329 - training loss (MAE): 0.0594, validation MSE: -0.9120
2025-05-03 01:56:18 [INFO]: Epoch 330 - training loss (MAE): 0.0602, validation MSE: -0.9528
2025-05-03 01:56:33 [INFO]: Epoch 331 - training loss (MAE): 0.0603, validation MSE: -0.9167
2025-05-03 01:56:48 [INFO]: Epoch 332 - training loss (MAE): 0.0599, validation MSE: -0.9530
2025-05-03 01:57:04 [INFO]: Epoch 333 - training loss (MAE): 0.0600, validation MSE: -0.9770
2025-05-03 01:57:19 [INFO]: Epoch 334 - training loss (MAE): 0.0594, validation MSE: -0.9742
2025-05-03 01:57:34 [INFO]: Epoch 335 - training loss (MAE): 0.0595, validation MSE: -0.9450
2025-05-03 01:57:49 [INFO]: Epoch 336 - training loss (MAE): 0.0602, validation MSE: -0.9703
2025-05-03 01:58:05 [INFO]: Epoch 337 - training loss (MAE): 0.0596, validation MSE: -0.9875
2025-05-03 01:58:20 [INFO]: Epoch 338 - training loss (MAE): 0.0606, validation MSE: -0.9010
2025-05-03 01:58:35 [INFO]: Epoch 339 - training loss (MAE): 0.0594, validation MSE: -0.9528
2025-05-03 01:58:50 [INFO]: Epoch 340 - training loss (MAE): 0.0598, validation MSE: -1.0080
2025-05-03 01:59:06 [INFO]: Epoch 341 - training loss (MAE): 0.0596, validation MSE: -1.0024
2025-05-03 01:59:21 [INFO]: Epoch 342 - training loss (MAE): 0.0591, validation MSE: -0.9997
2025-05-03 01:59:36 [INFO]: Epoch 343 - training loss (MAE): 0.0593, validation MSE: -0.9982
2025-05-03 01:59:51 [INFO]: Epoch 344 - training loss (MAE): 0.0591, validation MSE: -0.9613
2025-05-03 02:00:06 [INFO]: Epoch 345 - training loss (MAE): 0.0593, validation MSE: -0.9791
2025-05-03 02:00:21 [INFO]: Epoch 346 - training loss (MAE): 0.0595, validation MSE: -0.9978
2025-05-03 02:00:37 [INFO]: Epoch 347 - training loss (MAE): 0.0603, validation MSE: -0.9389
2025-05-03 02:00:52 [INFO]: Epoch 348 - training loss (MAE): 0.0587, validation MSE: -0.9971
2025-05-03 02:01:07 [INFO]: Epoch 349 - training loss (MAE): 0.0596, validation MSE: -0.9988
2025-05-03 02:01:22 [INFO]: Epoch 350 - training loss (MAE): 0.0596, validation MSE: -1.0145
2025-05-03 02:01:37 [INFO]: Epoch 351 - training loss (MAE): 0.0592, validation MSE: -0.9300
2025-05-03 02:01:52 [INFO]: Epoch 352 - training loss (MAE): 0.0594, validation MSE: -1.0048
2025-05-03 02:02:08 [INFO]: Epoch 353 - training loss (MAE): 0.0594, validation MSE: -1.0006
2025-05-03 02:02:23 [INFO]: Epoch 354 - training loss (MAE): 0.0586, validation MSE: -0.9841
2025-05-03 02:02:38 [INFO]: Epoch 355 - training loss (MAE): 0.0591, validation MSE: -0.9571
2025-05-03 02:02:54 [INFO]: Epoch 356 - training loss (MAE): 0.0591, validation MSE: -0.9605
2025-05-03 02:03:09 [INFO]: Epoch 357 - training loss (MAE): 0.0592, validation MSE: -0.9902
2025-05-03 02:03:24 [INFO]: Epoch 358 - training loss (MAE): 0.0590, validation MSE: -0.9303
2025-05-03 02:03:39 [INFO]: Epoch 359 - training loss (MAE): 0.0589, validation MSE: -0.9425
2025-05-03 02:03:54 [INFO]: Epoch 360 - training loss (MAE): 0.0595, validation MSE: -0.9691
2025-05-03 02:04:09 [INFO]: Epoch 361 - training loss (MAE): 0.0591, validation MSE: -0.9807
2025-05-03 02:04:24 [INFO]: Epoch 362 - training loss (MAE): 0.0593, validation MSE: -0.9229
2025-05-03 02:04:40 [INFO]: Epoch 363 - training loss (MAE): 0.0592, validation MSE: -0.9324
2025-05-03 02:04:55 [INFO]: Epoch 364 - training loss (MAE): 0.0588, validation MSE: -0.9332
2025-05-03 02:05:10 [INFO]: Epoch 365 - training loss (MAE): 0.0583, validation MSE: -0.9400
2025-05-03 02:05:25 [INFO]: Epoch 366 - training loss (MAE): 0.0590, validation MSE: -0.9604
2025-05-03 02:05:40 [INFO]: Epoch 367 - training loss (MAE): 0.0588, validation MSE: -0.9761
2025-05-03 02:05:55 [INFO]: Epoch 368 - training loss (MAE): 0.0585, validation MSE: -0.9571
2025-05-03 02:06:10 [INFO]: Epoch 369 - training loss (MAE): 0.0596, validation MSE: -0.9343
2025-05-03 02:06:25 [INFO]: Epoch 370 - training loss (MAE): 0.0585, validation MSE: -0.9391
2025-05-03 02:06:41 [INFO]: Epoch 371 - training loss (MAE): 0.0591, validation MSE: -0.9602
2025-05-03 02:06:56 [INFO]: Epoch 372 - training loss (MAE): 0.0589, validation MSE: -0.9384
2025-05-03 02:07:11 [INFO]: Epoch 373 - training loss (MAE): 0.0591, validation MSE: -0.9565
2025-05-03 02:07:26 [INFO]: Epoch 374 - training loss (MAE): 0.0580, validation MSE: -0.9724
2025-05-03 02:07:41 [INFO]: Epoch 375 - training loss (MAE): 0.0586, validation MSE: -0.8683
2025-05-03 02:07:56 [INFO]: Epoch 376 - training loss (MAE): 0.0586, validation MSE: -0.9777
2025-05-03 02:08:12 [INFO]: Epoch 377 - training loss (MAE): 0.0581, validation MSE: -0.9460
2025-05-03 02:08:27 [INFO]: Epoch 378 - training loss (MAE): 0.0581, validation MSE: -0.9137
2025-05-03 02:08:42 [INFO]: Epoch 379 - training loss (MAE): 0.0578, validation MSE: -0.9495
2025-05-03 02:08:57 [INFO]: Epoch 380 - training loss (MAE): 0.0587, validation MSE: -0.9993
2025-05-03 02:09:12 [INFO]: Epoch 381 - training loss (MAE): 0.0587, validation MSE: -1.0131
2025-05-03 02:09:28 [INFO]: Epoch 382 - training loss (MAE): 0.0580, validation MSE: -0.9646
2025-05-03 02:09:43 [INFO]: Epoch 383 - training loss (MAE): 0.0584, validation MSE: -0.9416
2025-05-03 02:09:58 [INFO]: Epoch 384 - training loss (MAE): 0.0583, validation MSE: -0.9285
2025-05-03 02:10:13 [INFO]: Epoch 385 - training loss (MAE): 0.0580, validation MSE: -0.9597
2025-05-03 02:10:28 [INFO]: Epoch 386 - training loss (MAE): 0.0581, validation MSE: -1.0291
2025-05-03 02:10:44 [INFO]: Epoch 387 - training loss (MAE): 0.0575, validation MSE: -0.9793
2025-05-03 02:10:59 [INFO]: Epoch 388 - training loss (MAE): 0.0579, validation MSE: -0.9764
2025-05-03 02:11:14 [INFO]: Epoch 389 - training loss (MAE): 0.0580, validation MSE: -0.9444
2025-05-03 02:11:29 [INFO]: Epoch 390 - training loss (MAE): 0.0583, validation MSE: -0.9564
2025-05-03 02:11:44 [INFO]: Epoch 391 - training loss (MAE): 0.0584, validation MSE: -0.9198
2025-05-03 02:12:00 [INFO]: Epoch 392 - training loss (MAE): 0.0582, validation MSE: -0.9343
2025-05-03 02:12:15 [INFO]: Epoch 393 - training loss (MAE): 0.0580, validation MSE: -0.9486
2025-05-03 02:12:30 [INFO]: Epoch 394 - training loss (MAE): 0.0579, validation MSE: -0.9698
2025-05-03 02:12:45 [INFO]: Epoch 395 - training loss (MAE): 0.0581, validation MSE: -0.9047
2025-05-03 02:13:00 [INFO]: Epoch 396 - training loss (MAE): 0.0577, validation MSE: -0.9728
2025-05-03 02:13:16 [INFO]: Epoch 397 - training loss (MAE): 0.0580, validation MSE: -0.8828
2025-05-03 02:13:31 [INFO]: Epoch 398 - training loss (MAE): 0.0577, validation MSE: -0.9411
2025-05-03 02:13:47 [INFO]: Epoch 399 - training loss (MAE): 0.0578, validation MSE: -0.9823
2025-05-03 02:14:02 [INFO]: Epoch 400 - training loss (MAE): 0.0579, validation MSE: -0.9619
2025-05-03 02:14:17 [INFO]: Epoch 401 - training loss (MAE): 0.0573, validation MSE: -0.9496
2025-05-03 02:14:32 [INFO]: Epoch 402 - training loss (MAE): 0.0576, validation MSE: -0.9904
2025-05-03 02:14:47 [INFO]: Epoch 403 - training loss (MAE): 0.0577, validation MSE: -0.9391
2025-05-03 02:15:03 [INFO]: Epoch 404 - training loss (MAE): 0.0577, validation MSE: -0.9494
2025-05-03 02:15:18 [INFO]: Epoch 405 - training loss (MAE): 0.0571, validation MSE: -0.9515
2025-05-03 02:15:33 [INFO]: Epoch 406 - training loss (MAE): 0.0579, validation MSE: -1.0282
2025-05-03 02:15:48 [INFO]: Epoch 407 - training loss (MAE): 0.0571, validation MSE: -0.9985
2025-05-03 02:16:03 [INFO]: Epoch 408 - training loss (MAE): 0.0569, validation MSE: -0.9877
2025-05-03 02:16:18 [INFO]: Epoch 409 - training loss (MAE): 0.0573, validation MSE: -0.9633
2025-05-03 02:16:33 [INFO]: Epoch 410 - training loss (MAE): 0.0576, validation MSE: -0.9659
2025-05-03 02:16:48 [INFO]: Epoch 411 - training loss (MAE): 0.0570, validation MSE: -0.8749
2025-05-03 02:17:04 [INFO]: Epoch 412 - training loss (MAE): 0.0569, validation MSE: -0.9159
2025-05-03 02:17:19 [INFO]: Epoch 413 - training loss (MAE): 0.0569, validation MSE: -0.9665
2025-05-03 02:17:34 [INFO]: Epoch 414 - training loss (MAE): 0.0573, validation MSE: -0.9610
2025-05-03 02:17:49 [INFO]: Epoch 415 - training loss (MAE): 0.0575, validation MSE: -0.9369
2025-05-03 02:18:04 [INFO]: Epoch 416 - training loss (MAE): 0.0570, validation MSE: -0.9680
2025-05-03 02:18:19 [INFO]: Epoch 417 - training loss (MAE): 0.0567, validation MSE: -0.9229
2025-05-03 02:18:35 [INFO]: Epoch 418 - training loss (MAE): 0.0568, validation MSE: -0.9630
2025-05-03 02:18:50 [INFO]: Epoch 419 - training loss (MAE): 0.0574, validation MSE: -0.9972
2025-05-03 02:19:05 [INFO]: Epoch 420 - training loss (MAE): 0.0568, validation MSE: -0.9907
2025-05-03 02:19:20 [INFO]: Epoch 421 - training loss (MAE): 0.0573, validation MSE: -0.8517
2025-05-03 02:19:36 [INFO]: Epoch 422 - training loss (MAE): 0.0566, validation MSE: -0.9862
2025-05-03 02:19:51 [INFO]: Epoch 423 - training loss (MAE): 0.0560, validation MSE: -0.8729
2025-05-03 02:20:06 [INFO]: Epoch 424 - training loss (MAE): 0.0576, validation MSE: -0.9952
2025-05-03 02:20:21 [INFO]: Epoch 425 - training loss (MAE): 0.0591, validation MSE: -0.9460
2025-05-03 02:20:36 [INFO]: Epoch 426 - training loss (MAE): 0.0566, validation MSE: -0.9347
2025-05-03 02:20:51 [INFO]: Epoch 427 - training loss (MAE): 0.0570, validation MSE: -0.9587
2025-05-03 02:21:06 [INFO]: Epoch 428 - training loss (MAE): 0.0564, validation MSE: -0.9910
2025-05-03 02:21:22 [INFO]: Epoch 429 - training loss (MAE): 0.0565, validation MSE: -0.9809
2025-05-03 02:21:37 [INFO]: Epoch 430 - training loss (MAE): 0.0572, validation MSE: -0.8743
2025-05-03 02:21:52 [INFO]: Epoch 431 - training loss (MAE): 0.0563, validation MSE: -0.9592
2025-05-03 02:22:07 [INFO]: Epoch 432 - training loss (MAE): 0.0560, validation MSE: -0.9810
2025-05-03 02:22:22 [INFO]: Epoch 433 - training loss (MAE): 0.0558, validation MSE: -0.9832
2025-05-03 02:22:37 [INFO]: Epoch 434 - training loss (MAE): 0.0565, validation MSE: -0.9927
2025-05-03 02:22:52 [INFO]: Epoch 435 - training loss (MAE): 0.0565, validation MSE: -0.8872
2025-05-03 02:23:08 [INFO]: Epoch 436 - training loss (MAE): 0.0565, validation MSE: -0.9645
2025-05-03 02:23:23 [INFO]: Epoch 437 - training loss (MAE): 0.0563, validation MSE: -0.9400
2025-05-03 02:23:38 [INFO]: Epoch 438 - training loss (MAE): 0.0566, validation MSE: -0.9852
2025-05-03 02:23:53 [INFO]: Epoch 439 - training loss (MAE): 0.0562, validation MSE: -0.9549
2025-05-03 02:24:09 [INFO]: Epoch 440 - training loss (MAE): 0.0562, validation MSE: -0.9133
2025-05-03 02:24:24 [INFO]: Epoch 441 - training loss (MAE): 0.0565, validation MSE: -0.9284
2025-05-03 02:24:38 [INFO]: Epoch 442 - training loss (MAE): 0.0560, validation MSE: -0.8941
2025-05-03 02:24:54 [INFO]: Epoch 443 - training loss (MAE): 0.0557, validation MSE: -0.8658
2025-05-03 02:25:09 [INFO]: Epoch 444 - training loss (MAE): 0.0558, validation MSE: -0.9989
2025-05-03 02:25:24 [INFO]: Epoch 445 - training loss (MAE): 0.0553, validation MSE: -1.0057
2025-05-03 02:25:40 [INFO]: Epoch 446 - training loss (MAE): 0.0557, validation MSE: -0.8510
2025-05-03 02:25:55 [INFO]: Epoch 447 - training loss (MAE): 0.0559, validation MSE: -0.9927
2025-05-03 02:26:10 [INFO]: Epoch 448 - training loss (MAE): 0.0556, validation MSE: -0.9800
2025-05-03 02:26:25 [INFO]: Epoch 449 - training loss (MAE): 0.0552, validation MSE: -1.0062
2025-05-03 02:26:40 [INFO]: Epoch 450 - training loss (MAE): 0.0557, validation MSE: -0.9894
2025-05-03 02:26:56 [INFO]: Epoch 451 - training loss (MAE): 0.0559, validation MSE: -0.9840
2025-05-03 02:27:11 [INFO]: Epoch 452 - training loss (MAE): 0.0555, validation MSE: -1.0222
2025-05-03 02:27:26 [INFO]: Epoch 453 - training loss (MAE): 0.0569, validation MSE: -0.9976
2025-05-03 02:27:41 [INFO]: Epoch 454 - training loss (MAE): 0.0550, validation MSE: -1.0066
2025-05-03 02:27:56 [INFO]: Epoch 455 - training loss (MAE): 0.0553, validation MSE: -1.0011
2025-05-03 02:28:11 [INFO]: Epoch 456 - training loss (MAE): 0.0550, validation MSE: -0.9871
2025-05-03 02:28:26 [INFO]: Epoch 457 - training loss (MAE): 0.0553, validation MSE: -1.0156
2025-05-03 02:28:41 [INFO]: Epoch 458 - training loss (MAE): 0.0563, validation MSE: -0.9935
2025-05-03 02:28:57 [INFO]: Epoch 459 - training loss (MAE): 0.0553, validation MSE: -0.9747
2025-05-03 02:29:12 [INFO]: Epoch 460 - training loss (MAE): 0.0551, validation MSE: -0.9678
2025-05-03 02:29:27 [INFO]: Epoch 461 - training loss (MAE): 0.0554, validation MSE: -1.0150
2025-05-03 02:29:42 [INFO]: Epoch 462 - training loss (MAE): 0.0552, validation MSE: -1.0036
2025-05-03 02:29:57 [INFO]: Epoch 463 - training loss (MAE): 0.0550, validation MSE: -0.8142
2025-05-03 02:30:12 [INFO]: Epoch 464 - training loss (MAE): 0.0552, validation MSE: -0.8444
2025-05-03 02:30:27 [INFO]: Epoch 465 - training loss (MAE): 0.0551, validation MSE: -1.0059
2025-05-03 02:30:43 [INFO]: Epoch 466 - training loss (MAE): 0.0550, validation MSE: -0.9439
2025-05-03 02:30:58 [INFO]: Epoch 467 - training loss (MAE): 0.0551, validation MSE: -0.9942
2025-05-03 02:31:13 [INFO]: Epoch 468 - training loss (MAE): 0.0546, validation MSE: -1.0220
2025-05-03 02:31:29 [INFO]: Epoch 469 - training loss (MAE): 0.0545, validation MSE: -1.0013
2025-05-03 02:31:44 [INFO]: Epoch 470 - training loss (MAE): 0.0544, validation MSE: -1.0228
2025-05-03 02:31:59 [INFO]: Epoch 471 - training loss (MAE): 0.0554, validation MSE: -1.0117
2025-05-03 02:32:15 [INFO]: Epoch 472 - training loss (MAE): 0.0549, validation MSE: -0.9795
2025-05-03 02:32:30 [INFO]: Epoch 473 - training loss (MAE): 0.0546, validation MSE: -1.0103
2025-05-03 02:32:45 [INFO]: Epoch 474 - training loss (MAE): 0.0547, validation MSE: -1.0088
2025-05-03 02:33:00 [INFO]: Epoch 475 - training loss (MAE): 0.0548, validation MSE: -0.8134
2025-05-03 02:33:15 [INFO]: Epoch 476 - training loss (MAE): 0.0555, validation MSE: -0.9375
2025-05-03 02:33:31 [INFO]: Epoch 477 - training loss (MAE): 0.0543, validation MSE: -0.9510
2025-05-03 02:33:46 [INFO]: Epoch 478 - training loss (MAE): 0.0550, validation MSE: -0.9563
2025-05-03 02:34:01 [INFO]: Epoch 479 - training loss (MAE): 0.0546, validation MSE: -0.9483
2025-05-03 02:34:16 [INFO]: Epoch 480 - training loss (MAE): 0.0545, validation MSE: -1.0151
2025-05-03 02:34:32 [INFO]: Epoch 481 - training loss (MAE): 0.0547, validation MSE: -0.9843
2025-05-03 02:34:47 [INFO]: Epoch 482 - training loss (MAE): 0.0545, validation MSE: -0.9841
2025-05-03 02:35:02 [INFO]: Epoch 483 - training loss (MAE): 0.0551, validation MSE: -0.8934
2025-05-03 02:35:17 [INFO]: Epoch 484 - training loss (MAE): 0.0541, validation MSE: -0.9847
2025-05-03 02:35:32 [INFO]: Epoch 485 - training loss (MAE): 0.0543, validation MSE: -0.9498
2025-05-03 02:35:47 [INFO]: Epoch 486 - training loss (MAE): 0.0547, validation MSE: -1.0048
2025-05-03 02:36:03 [INFO]: Epoch 487 - training loss (MAE): 0.0542, validation MSE: -0.9848
2025-05-03 02:36:18 [INFO]: Epoch 488 - training loss (MAE): 0.0547, validation MSE: -0.9907
2025-05-03 02:36:33 [INFO]: Epoch 489 - training loss (MAE): 0.0541, validation MSE: -0.9953
2025-05-03 02:36:48 [INFO]: Epoch 490 - training loss (MAE): 0.0543, validation MSE: -1.0257
2025-05-03 02:37:03 [INFO]: Epoch 491 - training loss (MAE): 0.0544, validation MSE: -0.9065
2025-05-03 02:37:18 [INFO]: Epoch 492 - training loss (MAE): 0.0544, validation MSE: -0.9293
2025-05-03 02:37:33 [INFO]: Epoch 493 - training loss (MAE): 0.0542, validation MSE: -0.9940
2025-05-03 02:37:48 [INFO]: Epoch 494 - training loss (MAE): 0.0537, validation MSE: -1.0321
2025-05-03 02:38:03 [INFO]: Epoch 495 - training loss (MAE): 0.0542, validation MSE: -0.9649
2025-05-03 02:38:19 [INFO]: Epoch 496 - training loss (MAE): 0.0543, validation MSE: -0.9490
2025-05-03 02:38:34 [INFO]: Epoch 497 - training loss (MAE): 0.0534, validation MSE: -0.9833
2025-05-03 02:38:49 [INFO]: Epoch 498 - training loss (MAE): 0.0543, validation MSE: -0.9406
2025-05-03 02:39:04 [INFO]: Epoch 499 - training loss (MAE): 0.0540, validation MSE: -0.9638
2025-05-03 02:39:19 [INFO]: Epoch 500 - training loss (MAE): 0.0537, validation MSE: -0.9174
2025-05-03 02:39:19 [INFO]: Finished training. The best model is from epoch#494.
2025-05-03 02:39:20 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_5.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 02:39:20 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_5.pypots
Fold 5 metrics: MAE: 0.416, MSE: 0.557, MRE: 0.621
Fold 5 metrics: MAE: 0.416, MSE: 0.557, MRE: 0.621
Training fold 6/10
2025-05-03 02:39:20 [INFO]: No given device, using default device: cuda
2025-05-03 02:39:20 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-03 02:39:20 [INFO]: Using customized MAE as the training loss function.
2025-05-03 02:39:20 [INFO]: Using customized MSE as the validation metric function.
2025-05-03 02:39:20 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-03 02:39:35 [INFO]: Epoch 001 - training loss (MAE): 1.0498, validation MSE: -0.1353
2025-05-03 02:39:51 [INFO]: Epoch 002 - training loss (MAE): 0.4502, validation MSE: -0.2399
2025-05-03 02:40:06 [INFO]: Epoch 003 - training loss (MAE): 0.3777, validation MSE: -0.3200
2025-05-03 02:40:21 [INFO]: Epoch 004 - training loss (MAE): 0.3372, validation MSE: -0.3385
2025-05-03 02:40:36 [INFO]: Epoch 005 - training loss (MAE): 0.3087, validation MSE: -0.3502
2025-05-03 02:40:51 [INFO]: Epoch 006 - training loss (MAE): 0.2862, validation MSE: -0.3703
2025-05-03 02:41:06 [INFO]: Epoch 007 - training loss (MAE): 0.2629, validation MSE: -0.3921
2025-05-03 02:41:22 [INFO]: Epoch 008 - training loss (MAE): 0.2433, validation MSE: -0.4062
2025-05-03 02:41:37 [INFO]: Epoch 009 - training loss (MAE): 0.2260, validation MSE: -0.4015
2025-05-03 02:41:52 [INFO]: Epoch 010 - training loss (MAE): 0.2094, validation MSE: -0.4113
2025-05-03 02:42:07 [INFO]: Epoch 011 - training loss (MAE): 0.1957, validation MSE: -0.3919
2025-05-03 02:42:22 [INFO]: Epoch 012 - training loss (MAE): 0.1851, validation MSE: -0.3939
2025-05-03 02:42:37 [INFO]: Epoch 013 - training loss (MAE): 0.1727, validation MSE: -0.3828
2025-05-03 02:42:52 [INFO]: Epoch 014 - training loss (MAE): 0.1647, validation MSE: -0.3686
2025-05-03 02:43:07 [INFO]: Epoch 015 - training loss (MAE): 0.1549, validation MSE: -0.3466
2025-05-03 02:43:22 [INFO]: Epoch 016 - training loss (MAE): 0.1483, validation MSE: -0.3361
2025-05-03 02:43:37 [INFO]: Epoch 017 - training loss (MAE): 0.1419, validation MSE: -0.3441
2025-05-03 02:43:52 [INFO]: Epoch 018 - training loss (MAE): 0.1376, validation MSE: -0.3568
2025-05-03 02:44:08 [INFO]: Epoch 019 - training loss (MAE): 0.1332, validation MSE: -0.3472
2025-05-03 02:44:23 [INFO]: Epoch 020 - training loss (MAE): 0.1285, validation MSE: -0.3606
2025-05-03 02:44:38 [INFO]: Epoch 021 - training loss (MAE): 0.1258, validation MSE: -0.3714
2025-05-03 02:44:53 [INFO]: Epoch 022 - training loss (MAE): 0.1225, validation MSE: -0.3602
2025-05-03 02:45:08 [INFO]: Epoch 023 - training loss (MAE): 0.1203, validation MSE: -0.3703
2025-05-03 02:45:23 [INFO]: Epoch 024 - training loss (MAE): 0.1167, validation MSE: -0.3623
2025-05-03 02:45:39 [INFO]: Epoch 025 - training loss (MAE): 0.1153, validation MSE: -0.3499
2025-05-03 02:45:54 [INFO]: Epoch 026 - training loss (MAE): 0.1129, validation MSE: -0.3676
2025-05-03 02:46:09 [INFO]: Epoch 027 - training loss (MAE): 0.1097, validation MSE: -0.3522
2025-05-03 02:46:24 [INFO]: Epoch 028 - training loss (MAE): 0.1082, validation MSE: -0.3676
2025-05-03 02:46:39 [INFO]: Epoch 029 - training loss (MAE): 0.1061, validation MSE: -0.3707
2025-05-03 02:46:54 [INFO]: Epoch 030 - training loss (MAE): 0.1038, validation MSE: -0.3648
2025-05-03 02:47:10 [INFO]: Epoch 031 - training loss (MAE): 0.1021, validation MSE: -0.3864
2025-05-03 02:47:25 [INFO]: Epoch 032 - training loss (MAE): 0.1011, validation MSE: -0.3710
2025-05-03 02:47:40 [INFO]: Epoch 033 - training loss (MAE): 0.0991, validation MSE: -0.3769
2025-05-03 02:47:55 [INFO]: Epoch 034 - training loss (MAE): 0.0970, validation MSE: -0.3718
2025-05-03 02:48:10 [INFO]: Epoch 035 - training loss (MAE): 0.0969, validation MSE: -0.3942
2025-05-03 02:48:25 [INFO]: Epoch 036 - training loss (MAE): 0.0943, validation MSE: -0.3975
2025-05-03 02:48:40 [INFO]: Epoch 037 - training loss (MAE): 0.0935, validation MSE: -0.3957
2025-05-03 02:48:55 [INFO]: Epoch 038 - training loss (MAE): 0.0918, validation MSE: -0.4121
2025-05-03 02:49:10 [INFO]: Epoch 039 - training loss (MAE): 0.0913, validation MSE: -0.4158
2025-05-03 02:49:26 [INFO]: Epoch 040 - training loss (MAE): 0.0900, validation MSE: -0.4042
2025-05-03 02:49:41 [INFO]: Epoch 041 - training loss (MAE): 0.0883, validation MSE: -0.4263
2025-05-03 02:49:56 [INFO]: Epoch 042 - training loss (MAE): 0.0870, validation MSE: -0.4165
2025-05-03 02:50:11 [INFO]: Epoch 043 - training loss (MAE): 0.0863, validation MSE: -0.4193
2025-05-03 02:50:26 [INFO]: Epoch 044 - training loss (MAE): 0.0850, validation MSE: -0.4285
2025-05-03 02:50:41 [INFO]: Epoch 045 - training loss (MAE): 0.0853, validation MSE: -0.4220
2025-05-03 02:50:57 [INFO]: Epoch 046 - training loss (MAE): 0.0849, validation MSE: -0.4439
2025-05-03 02:51:12 [INFO]: Epoch 047 - training loss (MAE): 0.0831, validation MSE: -0.4218
2025-05-03 02:51:27 [INFO]: Epoch 048 - training loss (MAE): 0.0821, validation MSE: -0.4385
2025-05-03 02:51:42 [INFO]: Epoch 049 - training loss (MAE): 0.0814, validation MSE: -0.4244
2025-05-03 02:51:57 [INFO]: Epoch 050 - training loss (MAE): 0.0808, validation MSE: -0.4284
2025-05-03 02:52:12 [INFO]: Epoch 051 - training loss (MAE): 0.0805, validation MSE: -0.4207
2025-05-03 02:52:27 [INFO]: Epoch 052 - training loss (MAE): 0.0793, validation MSE: -0.4462
2025-05-03 02:52:42 [INFO]: Epoch 053 - training loss (MAE): 0.0789, validation MSE: -0.4470
2025-05-03 02:52:58 [INFO]: Epoch 054 - training loss (MAE): 0.0779, validation MSE: -0.4624
2025-05-03 02:53:13 [INFO]: Epoch 055 - training loss (MAE): 0.0782, validation MSE: -0.4510
2025-05-03 02:53:28 [INFO]: Epoch 056 - training loss (MAE): 0.0773, validation MSE: -0.4600
2025-05-03 02:53:43 [INFO]: Epoch 057 - training loss (MAE): 0.0771, validation MSE: -0.4338
2025-05-03 02:53:58 [INFO]: Epoch 058 - training loss (MAE): 0.0771, validation MSE: -0.4509
2025-05-03 02:54:13 [INFO]: Epoch 059 - training loss (MAE): 0.0759, validation MSE: -0.4424
2025-05-03 02:54:29 [INFO]: Epoch 060 - training loss (MAE): 0.0763, validation MSE: -0.4529
2025-05-03 02:54:44 [INFO]: Epoch 061 - training loss (MAE): 0.0751, validation MSE: -0.4493
2025-05-03 02:54:59 [INFO]: Epoch 062 - training loss (MAE): 0.0749, validation MSE: -0.4338
2025-05-03 02:55:14 [INFO]: Epoch 063 - training loss (MAE): 0.0746, validation MSE: -0.4442
2025-05-03 02:55:29 [INFO]: Epoch 064 - training loss (MAE): 0.0749, validation MSE: -0.4394
2025-05-03 02:55:44 [INFO]: Epoch 065 - training loss (MAE): 0.0742, validation MSE: -0.4404
2025-05-03 02:56:00 [INFO]: Epoch 066 - training loss (MAE): 0.0729, validation MSE: -0.4614
2025-05-03 02:56:15 [INFO]: Epoch 067 - training loss (MAE): 0.0734, validation MSE: -0.4361
2025-05-03 02:56:30 [INFO]: Epoch 068 - training loss (MAE): 0.0734, validation MSE: -0.4218
2025-05-03 02:56:45 [INFO]: Epoch 069 - training loss (MAE): 0.0722, validation MSE: -0.4355
2025-05-03 02:57:00 [INFO]: Epoch 070 - training loss (MAE): 0.0721, validation MSE: -0.4415
2025-05-03 02:57:15 [INFO]: Epoch 071 - training loss (MAE): 0.0726, validation MSE: -0.4485
2025-05-03 02:57:30 [INFO]: Epoch 072 - training loss (MAE): 0.0720, validation MSE: -0.4544
2025-05-03 02:57:46 [INFO]: Epoch 073 - training loss (MAE): 0.0714, validation MSE: -0.4471
2025-05-03 02:58:01 [INFO]: Epoch 074 - training loss (MAE): 0.0717, validation MSE: -0.4283
2025-05-03 02:58:16 [INFO]: Epoch 075 - training loss (MAE): 0.0713, validation MSE: -0.4371
2025-05-03 02:58:31 [INFO]: Epoch 076 - training loss (MAE): 0.0707, validation MSE: -0.4314
2025-05-03 02:58:46 [INFO]: Epoch 077 - training loss (MAE): 0.0706, validation MSE: -0.4435
2025-05-03 02:59:01 [INFO]: Epoch 078 - training loss (MAE): 0.0699, validation MSE: -0.4441
2025-05-03 02:59:16 [INFO]: Epoch 079 - training loss (MAE): 0.0703, validation MSE: -0.4388
2025-05-03 02:59:31 [INFO]: Epoch 080 - training loss (MAE): 0.0702, validation MSE: -0.4374
2025-05-03 02:59:46 [INFO]: Epoch 081 - training loss (MAE): 0.0697, validation MSE: -0.4422
2025-05-03 03:00:01 [INFO]: Epoch 082 - training loss (MAE): 0.0696, validation MSE: -0.4460
2025-05-03 03:00:16 [INFO]: Epoch 083 - training loss (MAE): 0.0693, validation MSE: -0.4315
2025-05-03 03:00:32 [INFO]: Epoch 084 - training loss (MAE): 0.0692, validation MSE: -0.4354
2025-05-03 03:00:47 [INFO]: Epoch 085 - training loss (MAE): 0.0686, validation MSE: -0.4410
2025-05-03 03:01:02 [INFO]: Epoch 086 - training loss (MAE): 0.0691, validation MSE: -0.4520
2025-05-03 03:01:17 [INFO]: Epoch 087 - training loss (MAE): 0.0680, validation MSE: -0.4463
2025-05-03 03:01:32 [INFO]: Epoch 088 - training loss (MAE): 0.0683, validation MSE: -0.4355
2025-05-03 03:01:48 [INFO]: Epoch 089 - training loss (MAE): 0.0678, validation MSE: -0.4303
2025-05-03 03:02:03 [INFO]: Epoch 090 - training loss (MAE): 0.0681, validation MSE: -0.4426
2025-05-03 03:02:18 [INFO]: Epoch 091 - training loss (MAE): 0.0680, validation MSE: -0.4238
2025-05-03 03:02:33 [INFO]: Epoch 092 - training loss (MAE): 0.0678, validation MSE: -0.4283
2025-05-03 03:02:48 [INFO]: Epoch 093 - training loss (MAE): 0.0672, validation MSE: -0.4308
2025-05-03 03:03:03 [INFO]: Epoch 094 - training loss (MAE): 0.0675, validation MSE: -0.4271
2025-05-03 03:03:19 [INFO]: Epoch 095 - training loss (MAE): 0.0670, validation MSE: -0.3763
2025-05-03 03:03:34 [INFO]: Epoch 096 - training loss (MAE): 0.0675, validation MSE: -0.4281
2025-05-03 03:03:49 [INFO]: Epoch 097 - training loss (MAE): 0.0675, validation MSE: -0.4428
2025-05-03 03:04:04 [INFO]: Epoch 098 - training loss (MAE): 0.0675, validation MSE: -0.3885
2025-05-03 03:04:19 [INFO]: Epoch 099 - training loss (MAE): 0.0671, validation MSE: -0.4209
2025-05-03 03:04:34 [INFO]: Epoch 100 - training loss (MAE): 0.0670, validation MSE: -0.4175
2025-05-03 03:04:49 [INFO]: Epoch 101 - training loss (MAE): 0.0663, validation MSE: -0.4371
2025-05-03 03:05:04 [INFO]: Epoch 102 - training loss (MAE): 0.0667, validation MSE: -0.4473
2025-05-03 03:05:19 [INFO]: Epoch 103 - training loss (MAE): 0.0661, validation MSE: -0.4418
2025-05-03 03:05:34 [INFO]: Epoch 104 - training loss (MAE): 0.0662, validation MSE: -0.3972
2025-05-03 03:05:49 [INFO]: Epoch 105 - training loss (MAE): 0.0658, validation MSE: -0.4693
2025-05-03 03:06:04 [INFO]: Epoch 106 - training loss (MAE): 0.0657, validation MSE: -0.4226
2025-05-03 03:06:20 [INFO]: Epoch 107 - training loss (MAE): 0.0661, validation MSE: -0.4461
2025-05-03 03:06:35 [INFO]: Epoch 108 - training loss (MAE): 0.0654, validation MSE: -0.4576
2025-05-03 03:06:50 [INFO]: Epoch 109 - training loss (MAE): 0.0654, validation MSE: -0.4830
2025-05-03 03:07:05 [INFO]: Epoch 110 - training loss (MAE): 0.0650, validation MSE: -0.4357
2025-05-03 03:07:20 [INFO]: Epoch 111 - training loss (MAE): 0.0649, validation MSE: -0.4609
2025-05-03 03:07:35 [INFO]: Epoch 112 - training loss (MAE): 0.0650, validation MSE: -0.4371
2025-05-03 03:07:50 [INFO]: Epoch 113 - training loss (MAE): 0.0646, validation MSE: -0.4489
2025-05-03 03:08:06 [INFO]: Epoch 114 - training loss (MAE): 0.0648, validation MSE: -0.4224
2025-05-03 03:08:21 [INFO]: Epoch 115 - training loss (MAE): 0.0653, validation MSE: -0.4646
2025-05-03 03:08:36 [INFO]: Epoch 116 - training loss (MAE): 0.0643, validation MSE: -0.4455
2025-05-03 03:08:51 [INFO]: Epoch 117 - training loss (MAE): 0.0647, validation MSE: -0.4449
2025-05-03 03:09:06 [INFO]: Epoch 118 - training loss (MAE): 0.0646, validation MSE: -0.3767
2025-05-03 03:09:21 [INFO]: Epoch 119 - training loss (MAE): 0.0649, validation MSE: -0.4429
2025-05-03 03:09:37 [INFO]: Epoch 120 - training loss (MAE): 0.0639, validation MSE: -0.4496
2025-05-03 03:09:52 [INFO]: Epoch 121 - training loss (MAE): 0.0635, validation MSE: -0.4440
2025-05-03 03:10:07 [INFO]: Epoch 122 - training loss (MAE): 0.0640, validation MSE: -0.4495
2025-05-03 03:10:22 [INFO]: Epoch 123 - training loss (MAE): 0.0647, validation MSE: -0.4360
2025-05-03 03:10:37 [INFO]: Epoch 124 - training loss (MAE): 0.0645, validation MSE: -0.4636
2025-05-03 03:10:52 [INFO]: Epoch 125 - training loss (MAE): 0.0639, validation MSE: -0.4255
2025-05-03 03:11:07 [INFO]: Epoch 126 - training loss (MAE): 0.0636, validation MSE: -0.4213
2025-05-03 03:11:23 [INFO]: Epoch 127 - training loss (MAE): 0.0637, validation MSE: -0.4419
2025-05-03 03:11:38 [INFO]: Epoch 128 - training loss (MAE): 0.0633, validation MSE: -0.4468
2025-05-03 03:11:53 [INFO]: Epoch 129 - training loss (MAE): 0.0633, validation MSE: -0.4052
2025-05-03 03:12:08 [INFO]: Epoch 130 - training loss (MAE): 0.0646, validation MSE: -0.4229
2025-05-03 03:12:23 [INFO]: Epoch 131 - training loss (MAE): 0.0636, validation MSE: -0.4118
2025-05-03 03:12:38 [INFO]: Epoch 132 - training loss (MAE): 0.0633, validation MSE: -0.4369
2025-05-03 03:12:53 [INFO]: Epoch 133 - training loss (MAE): 0.0623, validation MSE: -0.4686
2025-05-03 03:13:09 [INFO]: Epoch 134 - training loss (MAE): 0.0630, validation MSE: -0.4107
2025-05-03 03:13:24 [INFO]: Epoch 135 - training loss (MAE): 0.0627, validation MSE: -0.3970
2025-05-03 03:13:39 [INFO]: Epoch 136 - training loss (MAE): 0.0624, validation MSE: -0.4447
2025-05-03 03:13:54 [INFO]: Epoch 137 - training loss (MAE): 0.0628, validation MSE: -0.4522
2025-05-03 03:14:09 [INFO]: Epoch 138 - training loss (MAE): 0.0624, validation MSE: -0.4356
2025-05-03 03:14:24 [INFO]: Epoch 139 - training loss (MAE): 0.0624, validation MSE: -0.4536
2025-05-03 03:14:40 [INFO]: Epoch 140 - training loss (MAE): 0.0625, validation MSE: -0.3961
2025-05-03 03:14:54 [INFO]: Epoch 141 - training loss (MAE): 0.0622, validation MSE: -0.4431
2025-05-03 03:15:09 [INFO]: Epoch 142 - training loss (MAE): 0.0628, validation MSE: -0.4430
2025-05-03 03:15:25 [INFO]: Epoch 143 - training loss (MAE): 0.0639, validation MSE: -0.4244
2025-05-03 03:15:40 [INFO]: Epoch 144 - training loss (MAE): 0.0633, validation MSE: -0.4050
2025-05-03 03:15:55 [INFO]: Epoch 145 - training loss (MAE): 0.0625, validation MSE: -0.4199
2025-05-03 03:16:10 [INFO]: Epoch 146 - training loss (MAE): 0.0621, validation MSE: -0.4453
2025-05-03 03:16:25 [INFO]: Epoch 147 - training loss (MAE): 0.0621, validation MSE: -0.4336
2025-05-03 03:16:40 [INFO]: Epoch 148 - training loss (MAE): 0.0616, validation MSE: -0.3754
2025-05-03 03:16:56 [INFO]: Epoch 149 - training loss (MAE): 0.0620, validation MSE: -0.4121
2025-05-03 03:17:11 [INFO]: Epoch 150 - training loss (MAE): 0.0621, validation MSE: -0.4403
2025-05-03 03:17:26 [INFO]: Epoch 151 - training loss (MAE): 0.0615, validation MSE: -0.4605
2025-05-03 03:17:41 [INFO]: Epoch 152 - training loss (MAE): 0.0615, validation MSE: -0.4392
2025-05-03 03:17:56 [INFO]: Epoch 153 - training loss (MAE): 0.0625, validation MSE: -0.3749
2025-05-03 03:18:11 [INFO]: Epoch 154 - training loss (MAE): 0.0620, validation MSE: -0.4294
2025-05-03 03:18:26 [INFO]: Epoch 155 - training loss (MAE): 0.0613, validation MSE: -0.4416
2025-05-03 03:18:42 [INFO]: Epoch 156 - training loss (MAE): 0.0611, validation MSE: -0.4564
2025-05-03 03:18:57 [INFO]: Epoch 157 - training loss (MAE): 0.0611, validation MSE: -0.4484
2025-05-03 03:19:12 [INFO]: Epoch 158 - training loss (MAE): 0.0613, validation MSE: -0.4641
2025-05-03 03:19:27 [INFO]: Epoch 159 - training loss (MAE): 0.0608, validation MSE: -0.4374
2025-05-03 03:19:42 [INFO]: Epoch 160 - training loss (MAE): 0.0612, validation MSE: -0.4319
2025-05-03 03:19:57 [INFO]: Epoch 161 - training loss (MAE): 0.0611, validation MSE: -0.4481
2025-05-03 03:20:13 [INFO]: Epoch 162 - training loss (MAE): 0.0613, validation MSE: -0.4419
2025-05-03 03:20:28 [INFO]: Epoch 163 - training loss (MAE): 0.0609, validation MSE: -0.4384
2025-05-03 03:20:43 [INFO]: Epoch 164 - training loss (MAE): 0.0613, validation MSE: -0.4254
2025-05-03 03:20:58 [INFO]: Epoch 165 - training loss (MAE): 0.0606, validation MSE: -0.4456
2025-05-03 03:21:13 [INFO]: Epoch 166 - training loss (MAE): 0.0611, validation MSE: -0.4230
2025-05-03 03:21:28 [INFO]: Epoch 167 - training loss (MAE): 0.0604, validation MSE: -0.4288
2025-05-03 03:21:43 [INFO]: Epoch 168 - training loss (MAE): 0.0606, validation MSE: -0.3952
2025-05-03 03:21:58 [INFO]: Epoch 169 - training loss (MAE): 0.0614, validation MSE: -0.3740
2025-05-03 03:22:13 [INFO]: Epoch 170 - training loss (MAE): 0.0606, validation MSE: -0.4691
2025-05-03 03:22:28 [INFO]: Epoch 171 - training loss (MAE): 0.0608, validation MSE: -0.4492
2025-05-03 03:22:43 [INFO]: Epoch 172 - training loss (MAE): 0.0602, validation MSE: -0.4394
2025-05-03 03:22:58 [INFO]: Epoch 173 - training loss (MAE): 0.0606, validation MSE: -0.4291
2025-05-03 03:23:13 [INFO]: Epoch 174 - training loss (MAE): 0.0608, validation MSE: -0.4275
2025-05-03 03:23:29 [INFO]: Epoch 175 - training loss (MAE): 0.0604, validation MSE: -0.4235
2025-05-03 03:23:44 [INFO]: Epoch 176 - training loss (MAE): 0.0600, validation MSE: -0.4350
2025-05-03 03:24:00 [INFO]: Epoch 177 - training loss (MAE): 0.0599, validation MSE: -0.4310
2025-05-03 03:24:15 [INFO]: Epoch 178 - training loss (MAE): 0.0602, validation MSE: -0.4254
2025-05-03 03:24:30 [INFO]: Epoch 179 - training loss (MAE): 0.0608, validation MSE: -0.4210
2025-05-03 03:24:45 [INFO]: Epoch 180 - training loss (MAE): 0.0600, validation MSE: -0.4582
2025-05-03 03:25:00 [INFO]: Epoch 181 - training loss (MAE): 0.0604, validation MSE: -0.3969
2025-05-03 03:25:15 [INFO]: Epoch 182 - training loss (MAE): 0.0598, validation MSE: -0.4329
2025-05-03 03:25:30 [INFO]: Epoch 183 - training loss (MAE): 0.0598, validation MSE: -0.4235
2025-05-03 03:25:46 [INFO]: Epoch 184 - training loss (MAE): 0.0599, validation MSE: -0.4392
2025-05-03 03:26:01 [INFO]: Epoch 185 - training loss (MAE): 0.0605, validation MSE: -0.4376
2025-05-03 03:26:17 [INFO]: Epoch 186 - training loss (MAE): 0.0602, validation MSE: -0.3994
2025-05-03 03:26:32 [INFO]: Epoch 187 - training loss (MAE): 0.0596, validation MSE: -0.4432
2025-05-03 03:26:47 [INFO]: Epoch 188 - training loss (MAE): 0.0605, validation MSE: -0.4228
2025-05-03 03:27:02 [INFO]: Epoch 189 - training loss (MAE): 0.0595, validation MSE: -0.4410
2025-05-03 03:27:17 [INFO]: Epoch 190 - training loss (MAE): 0.0597, validation MSE: -0.4278
2025-05-03 03:27:32 [INFO]: Epoch 191 - training loss (MAE): 0.0597, validation MSE: -0.3532
2025-05-03 03:27:47 [INFO]: Epoch 192 - training loss (MAE): 0.0592, validation MSE: -0.4259
2025-05-03 03:28:02 [INFO]: Epoch 193 - training loss (MAE): 0.0596, validation MSE: -0.3773
2025-05-03 03:28:17 [INFO]: Epoch 194 - training loss (MAE): 0.0595, validation MSE: -0.4241
2025-05-03 03:28:33 [INFO]: Epoch 195 - training loss (MAE): 0.0597, validation MSE: -0.4402
2025-05-03 03:28:48 [INFO]: Epoch 196 - training loss (MAE): 0.0593, validation MSE: -0.4115
2025-05-03 03:29:03 [INFO]: Epoch 197 - training loss (MAE): 0.0596, validation MSE: -0.4487
2025-05-03 03:29:18 [INFO]: Epoch 198 - training loss (MAE): 0.0599, validation MSE: -0.4121
2025-05-03 03:29:33 [INFO]: Epoch 199 - training loss (MAE): 0.0596, validation MSE: -0.4482
2025-05-03 03:29:48 [INFO]: Epoch 200 - training loss (MAE): 0.0593, validation MSE: -0.4088
2025-05-03 03:30:03 [INFO]: Epoch 201 - training loss (MAE): 0.0593, validation MSE: -0.3771
2025-05-03 03:30:19 [INFO]: Epoch 202 - training loss (MAE): 0.0590, validation MSE: -0.3188
2025-05-03 03:30:34 [INFO]: Epoch 203 - training loss (MAE): 0.0587, validation MSE: -0.3954
2025-05-03 03:30:49 [INFO]: Epoch 204 - training loss (MAE): 0.0589, validation MSE: -0.4211
2025-05-03 03:31:04 [INFO]: Epoch 205 - training loss (MAE): 0.0587, validation MSE: -0.4586
2025-05-03 03:31:20 [INFO]: Epoch 206 - training loss (MAE): 0.0595, validation MSE: -0.4289
2025-05-03 03:31:35 [INFO]: Epoch 207 - training loss (MAE): 0.0597, validation MSE: -0.4691
2025-05-03 03:31:50 [INFO]: Epoch 208 - training loss (MAE): 0.0591, validation MSE: -0.4309
2025-05-03 03:32:05 [INFO]: Epoch 209 - training loss (MAE): 0.0590, validation MSE: -0.3875
2025-05-03 03:32:21 [INFO]: Epoch 210 - training loss (MAE): 0.0588, validation MSE: -0.4419
2025-05-03 03:32:36 [INFO]: Epoch 211 - training loss (MAE): 0.0587, validation MSE: -0.4278
2025-05-03 03:32:51 [INFO]: Epoch 212 - training loss (MAE): 0.0588, validation MSE: -0.4273
2025-05-03 03:33:06 [INFO]: Epoch 213 - training loss (MAE): 0.0598, validation MSE: -0.3845
2025-05-03 03:33:22 [INFO]: Epoch 214 - training loss (MAE): 0.0590, validation MSE: -0.4357
2025-05-03 03:33:37 [INFO]: Epoch 215 - training loss (MAE): 0.0588, validation MSE: -0.4490
2025-05-03 03:33:52 [INFO]: Epoch 216 - training loss (MAE): 0.0587, validation MSE: -0.4147
2025-05-03 03:34:07 [INFO]: Epoch 217 - training loss (MAE): 0.0589, validation MSE: -0.4617
2025-05-03 03:34:22 [INFO]: Epoch 218 - training loss (MAE): 0.0581, validation MSE: -0.3962
2025-05-03 03:34:37 [INFO]: Epoch 219 - training loss (MAE): 0.0588, validation MSE: -0.4002
2025-05-03 03:34:53 [INFO]: Epoch 220 - training loss (MAE): 0.0582, validation MSE: -0.4431
2025-05-03 03:35:08 [INFO]: Epoch 221 - training loss (MAE): 0.0585, validation MSE: -0.4584
2025-05-03 03:35:23 [INFO]: Epoch 222 - training loss (MAE): 0.0581, validation MSE: -0.4168
2025-05-03 03:35:38 [INFO]: Epoch 223 - training loss (MAE): 0.0587, validation MSE: -0.4437
2025-05-03 03:35:53 [INFO]: Epoch 224 - training loss (MAE): 0.0586, validation MSE: -0.4179
2025-05-03 03:36:08 [INFO]: Epoch 225 - training loss (MAE): 0.0585, validation MSE: -0.4218
2025-05-03 03:36:23 [INFO]: Epoch 226 - training loss (MAE): 0.0582, validation MSE: -0.4415
2025-05-03 03:36:39 [INFO]: Epoch 227 - training loss (MAE): 0.0584, validation MSE: -0.4673
2025-05-03 03:36:54 [INFO]: Epoch 228 - training loss (MAE): 0.0583, validation MSE: -0.4460
2025-05-03 03:37:09 [INFO]: Epoch 229 - training loss (MAE): 0.0582, validation MSE: -0.4372
2025-05-03 03:37:24 [INFO]: Epoch 230 - training loss (MAE): 0.0577, validation MSE: -0.4465
2025-05-03 03:37:39 [INFO]: Epoch 231 - training loss (MAE): 0.0579, validation MSE: -0.4197
2025-05-03 03:37:54 [INFO]: Epoch 232 - training loss (MAE): 0.0584, validation MSE: -0.4621
2025-05-03 03:38:09 [INFO]: Epoch 233 - training loss (MAE): 0.0578, validation MSE: -0.4703
2025-05-03 03:38:24 [INFO]: Epoch 234 - training loss (MAE): 0.0580, validation MSE: -0.4109
2025-05-03 03:38:40 [INFO]: Epoch 235 - training loss (MAE): 0.0584, validation MSE: -0.4371
2025-05-03 03:38:55 [INFO]: Epoch 236 - training loss (MAE): 0.0582, validation MSE: -0.4382
2025-05-03 03:39:10 [INFO]: Epoch 237 - training loss (MAE): 0.0579, validation MSE: -0.4161
2025-05-03 03:39:25 [INFO]: Epoch 238 - training loss (MAE): 0.0589, validation MSE: -0.4167
2025-05-03 03:39:40 [INFO]: Epoch 239 - training loss (MAE): 0.0574, validation MSE: -0.4374
2025-05-03 03:39:55 [INFO]: Epoch 240 - training loss (MAE): 0.0579, validation MSE: -0.4455
2025-05-03 03:40:11 [INFO]: Epoch 241 - training loss (MAE): 0.0574, validation MSE: -0.4426
2025-05-03 03:40:26 [INFO]: Epoch 242 - training loss (MAE): 0.0578, validation MSE: -0.4159
2025-05-03 03:40:41 [INFO]: Epoch 243 - training loss (MAE): 0.0577, validation MSE: -0.4381
2025-05-03 03:40:56 [INFO]: Epoch 244 - training loss (MAE): 0.0579, validation MSE: -0.4322
2025-05-03 03:41:11 [INFO]: Epoch 245 - training loss (MAE): 0.0577, validation MSE: -0.3900
2025-05-03 03:41:26 [INFO]: Epoch 246 - training loss (MAE): 0.0578, validation MSE: -0.4488
2025-05-03 03:41:42 [INFO]: Epoch 247 - training loss (MAE): 0.0578, validation MSE: -0.4368
2025-05-03 03:41:57 [INFO]: Epoch 248 - training loss (MAE): 0.0577, validation MSE: -0.4608
2025-05-03 03:42:12 [INFO]: Epoch 249 - training loss (MAE): 0.0575, validation MSE: -0.4351
2025-05-03 03:42:27 [INFO]: Epoch 250 - training loss (MAE): 0.0574, validation MSE: -0.2232
2025-05-03 03:42:42 [INFO]: Epoch 251 - training loss (MAE): 0.0580, validation MSE: -0.4513
2025-05-03 03:42:57 [INFO]: Epoch 252 - training loss (MAE): 0.0573, validation MSE: -0.4641
2025-05-03 03:43:12 [INFO]: Epoch 253 - training loss (MAE): 0.0579, validation MSE: -0.4273
2025-05-03 03:43:27 [INFO]: Epoch 254 - training loss (MAE): 0.0579, validation MSE: -0.4008
2025-05-03 03:43:43 [INFO]: Epoch 255 - training loss (MAE): 0.0582, validation MSE: -0.4617
2025-05-03 03:43:58 [INFO]: Epoch 256 - training loss (MAE): 0.0571, validation MSE: -0.3921
2025-05-03 03:44:13 [INFO]: Epoch 257 - training loss (MAE): 0.0571, validation MSE: -0.4436
2025-05-03 03:44:28 [INFO]: Epoch 258 - training loss (MAE): 0.0571, validation MSE: -0.4638
2025-05-03 03:44:43 [INFO]: Epoch 259 - training loss (MAE): 0.0573, validation MSE: -0.3414
2025-05-03 03:44:58 [INFO]: Epoch 260 - training loss (MAE): 0.0566, validation MSE: -0.4506
2025-05-03 03:45:14 [INFO]: Epoch 261 - training loss (MAE): 0.0570, validation MSE: -0.4239
2025-05-03 03:45:29 [INFO]: Epoch 262 - training loss (MAE): 0.0577, validation MSE: -0.3449
2025-05-03 03:45:44 [INFO]: Epoch 263 - training loss (MAE): 0.0570, validation MSE: -0.3875
2025-05-03 03:45:59 [INFO]: Epoch 264 - training loss (MAE): 0.0572, validation MSE: -0.4157
2025-05-03 03:46:15 [INFO]: Epoch 265 - training loss (MAE): 0.0568, validation MSE: -0.4649
2025-05-03 03:46:30 [INFO]: Epoch 266 - training loss (MAE): 0.0568, validation MSE: -0.4689
2025-05-03 03:46:45 [INFO]: Epoch 267 - training loss (MAE): 0.0566, validation MSE: -0.4706
2025-05-03 03:47:00 [INFO]: Epoch 268 - training loss (MAE): 0.0572, validation MSE: -0.4475
2025-05-03 03:47:16 [INFO]: Epoch 269 - training loss (MAE): 0.0567, validation MSE: -0.4674
2025-05-03 03:47:31 [INFO]: Epoch 270 - training loss (MAE): 0.0566, validation MSE: -0.4642
2025-05-03 03:47:46 [INFO]: Epoch 271 - training loss (MAE): 0.0565, validation MSE: -0.4493
2025-05-03 03:48:01 [INFO]: Epoch 272 - training loss (MAE): 0.0568, validation MSE: -0.4534
2025-05-03 03:48:16 [INFO]: Epoch 273 - training loss (MAE): 0.0575, validation MSE: -0.4690
2025-05-03 03:48:32 [INFO]: Epoch 274 - training loss (MAE): 0.0568, validation MSE: -0.4138
2025-05-03 03:48:47 [INFO]: Epoch 275 - training loss (MAE): 0.0568, validation MSE: -0.4484
2025-05-03 03:49:02 [INFO]: Epoch 276 - training loss (MAE): 0.0568, validation MSE: -0.3887
2025-05-03 03:49:17 [INFO]: Epoch 277 - training loss (MAE): 0.0573, validation MSE: -0.4490
2025-05-03 03:49:32 [INFO]: Epoch 278 - training loss (MAE): 0.0563, validation MSE: -0.4337
2025-05-03 03:49:47 [INFO]: Epoch 279 - training loss (MAE): 0.0565, validation MSE: -0.4601
2025-05-03 03:50:03 [INFO]: Epoch 280 - training loss (MAE): 0.0571, validation MSE: -0.3895
2025-05-03 03:50:18 [INFO]: Epoch 281 - training loss (MAE): 0.0566, validation MSE: -0.3969
2025-05-03 03:50:33 [INFO]: Epoch 282 - training loss (MAE): 0.0563, validation MSE: -0.4401
2025-05-03 03:50:48 [INFO]: Epoch 283 - training loss (MAE): 0.0563, validation MSE: -0.4350
2025-05-03 03:51:03 [INFO]: Epoch 284 - training loss (MAE): 0.0562, validation MSE: -0.4373
2025-05-03 03:51:18 [INFO]: Epoch 285 - training loss (MAE): 0.0562, validation MSE: -0.3221
2025-05-03 03:51:33 [INFO]: Epoch 286 - training loss (MAE): 0.0566, validation MSE: -0.4588
2025-05-03 03:51:48 [INFO]: Epoch 287 - training loss (MAE): 0.0560, validation MSE: -0.4253
2025-05-03 03:52:03 [INFO]: Epoch 288 - training loss (MAE): 0.0568, validation MSE: -0.4644
2025-05-03 03:52:19 [INFO]: Epoch 289 - training loss (MAE): 0.0567, validation MSE: -0.4578
2025-05-03 03:52:34 [INFO]: Epoch 290 - training loss (MAE): 0.0565, validation MSE: -0.4327
2025-05-03 03:52:49 [INFO]: Epoch 291 - training loss (MAE): 0.0563, validation MSE: -0.4472
2025-05-03 03:53:04 [INFO]: Epoch 292 - training loss (MAE): 0.0561, validation MSE: -0.4486
2025-05-03 03:53:20 [INFO]: Epoch 293 - training loss (MAE): 0.0559, validation MSE: -0.4278
2025-05-03 03:53:35 [INFO]: Epoch 294 - training loss (MAE): 0.0568, validation MSE: -0.4553
2025-05-03 03:53:50 [INFO]: Epoch 295 - training loss (MAE): 0.0567, validation MSE: -0.4276
2025-05-03 03:54:05 [INFO]: Epoch 296 - training loss (MAE): 0.0561, validation MSE: -0.3273
2025-05-03 03:54:20 [INFO]: Epoch 297 - training loss (MAE): 0.0563, validation MSE: -0.4675
2025-05-03 03:54:35 [INFO]: Epoch 298 - training loss (MAE): 0.0559, validation MSE: -0.4682
2025-05-03 03:54:50 [INFO]: Epoch 299 - training loss (MAE): 0.0559, validation MSE: -0.4593
2025-05-03 03:55:05 [INFO]: Epoch 300 - training loss (MAE): 0.0567, validation MSE: -0.3388
2025-05-03 03:55:20 [INFO]: Epoch 301 - training loss (MAE): 0.0563, validation MSE: -0.3773
2025-05-03 03:55:35 [INFO]: Epoch 302 - training loss (MAE): 0.0555, validation MSE: -0.4386
2025-05-03 03:55:51 [INFO]: Epoch 303 - training loss (MAE): 0.0557, validation MSE: -0.4502
2025-05-03 03:56:06 [INFO]: Epoch 304 - training loss (MAE): 0.0559, validation MSE: -0.4575
2025-05-03 03:56:21 [INFO]: Epoch 305 - training loss (MAE): 0.0555, validation MSE: -0.4402
2025-05-03 03:56:36 [INFO]: Epoch 306 - training loss (MAE): 0.0557, validation MSE: -0.4145
2025-05-03 03:56:51 [INFO]: Epoch 307 - training loss (MAE): 0.0555, validation MSE: -0.4474
2025-05-03 03:57:06 [INFO]: Epoch 308 - training loss (MAE): 0.0554, validation MSE: -0.4142
2025-05-03 03:57:21 [INFO]: Epoch 309 - training loss (MAE): 0.0555, validation MSE: -0.4530
2025-05-03 03:57:37 [INFO]: Epoch 310 - training loss (MAE): 0.0554, validation MSE: -0.3286
2025-05-03 03:57:52 [INFO]: Epoch 311 - training loss (MAE): 0.0555, validation MSE: -0.2942
2025-05-03 03:58:07 [INFO]: Epoch 312 - training loss (MAE): 0.0557, validation MSE: -0.3727
2025-05-03 03:58:22 [INFO]: Epoch 313 - training loss (MAE): 0.0560, validation MSE: -0.4115
2025-05-03 03:58:38 [INFO]: Epoch 314 - training loss (MAE): 0.0564, validation MSE: -0.4720
2025-05-03 03:58:53 [INFO]: Epoch 315 - training loss (MAE): 0.0558, validation MSE: -0.3542
2025-05-03 03:59:08 [INFO]: Epoch 316 - training loss (MAE): 0.0557, validation MSE: -0.4705
2025-05-03 03:59:23 [INFO]: Epoch 317 - training loss (MAE): 0.0551, validation MSE: -0.4118
2025-05-03 03:59:38 [INFO]: Epoch 318 - training loss (MAE): 0.0550, validation MSE: -0.4552
2025-05-03 03:59:53 [INFO]: Epoch 319 - training loss (MAE): 0.0555, validation MSE: -0.3994
2025-05-03 04:00:08 [INFO]: Epoch 320 - training loss (MAE): 0.0549, validation MSE: -0.2972
2025-05-03 04:00:23 [INFO]: Epoch 321 - training loss (MAE): 0.0550, validation MSE: -0.4027
2025-05-03 04:00:39 [INFO]: Epoch 322 - training loss (MAE): 0.0550, validation MSE: -0.4375
2025-05-03 04:00:54 [INFO]: Epoch 323 - training loss (MAE): 0.0556, validation MSE: -0.4329
2025-05-03 04:01:09 [INFO]: Epoch 324 - training loss (MAE): 0.0550, validation MSE: -0.4596
2025-05-03 04:01:24 [INFO]: Epoch 325 - training loss (MAE): 0.0550, validation MSE: -0.3712
2025-05-03 04:01:39 [INFO]: Epoch 326 - training loss (MAE): 0.0552, validation MSE: -0.4494
2025-05-03 04:01:54 [INFO]: Epoch 327 - training loss (MAE): 0.0555, validation MSE: -0.3988
2025-05-03 04:02:09 [INFO]: Epoch 328 - training loss (MAE): 0.0547, validation MSE: -0.4143
2025-05-03 04:02:25 [INFO]: Epoch 329 - training loss (MAE): 0.0550, validation MSE: -0.3834
2025-05-03 04:02:40 [INFO]: Epoch 330 - training loss (MAE): 0.0549, validation MSE: -0.4590
2025-05-03 04:02:55 [INFO]: Epoch 331 - training loss (MAE): 0.0547, validation MSE: -0.4812
2025-05-03 04:03:10 [INFO]: Epoch 332 - training loss (MAE): 0.0554, validation MSE: -0.2869
2025-05-03 04:03:25 [INFO]: Epoch 333 - training loss (MAE): 0.0545, validation MSE: -0.4657
2025-05-03 04:03:40 [INFO]: Epoch 334 - training loss (MAE): 0.0551, validation MSE: -0.3192
2025-05-03 04:03:55 [INFO]: Epoch 335 - training loss (MAE): 0.0548, validation MSE: -0.4420
2025-05-03 04:04:10 [INFO]: Epoch 336 - training loss (MAE): 0.0543, validation MSE: -0.4627
2025-05-03 04:04:26 [INFO]: Epoch 337 - training loss (MAE): 0.0546, validation MSE: -0.4459
2025-05-03 04:04:41 [INFO]: Epoch 338 - training loss (MAE): 0.0542, validation MSE: -0.4142
2025-05-03 04:04:57 [INFO]: Epoch 339 - training loss (MAE): 0.0545, validation MSE: -0.4164
2025-05-03 04:05:12 [INFO]: Epoch 340 - training loss (MAE): 0.0547, validation MSE: -0.4427
2025-05-03 04:05:27 [INFO]: Epoch 341 - training loss (MAE): 0.0544, validation MSE: -0.4421
2025-05-03 04:05:42 [INFO]: Epoch 342 - training loss (MAE): 0.0540, validation MSE: -0.4365
2025-05-03 04:05:57 [INFO]: Epoch 343 - training loss (MAE): 0.0546, validation MSE: -0.4307
2025-05-03 04:06:12 [INFO]: Epoch 344 - training loss (MAE): 0.0544, validation MSE: -0.4686
2025-05-03 04:06:28 [INFO]: Epoch 345 - training loss (MAE): 0.0544, validation MSE: -0.4059
2025-05-03 04:06:43 [INFO]: Epoch 346 - training loss (MAE): 0.0545, validation MSE: -0.4342
2025-05-03 04:06:58 [INFO]: Epoch 347 - training loss (MAE): 0.0538, validation MSE: -0.4261
2025-05-03 04:07:13 [INFO]: Epoch 348 - training loss (MAE): 0.0541, validation MSE: -0.3993
2025-05-03 04:07:29 [INFO]: Epoch 349 - training loss (MAE): 0.0533, validation MSE: -0.4479
2025-05-03 04:07:44 [INFO]: Epoch 350 - training loss (MAE): 0.0542, validation MSE: -0.4554
2025-05-03 04:07:59 [INFO]: Epoch 351 - training loss (MAE): 0.0538, validation MSE: -0.3630
2025-05-03 04:08:14 [INFO]: Epoch 352 - training loss (MAE): 0.0537, validation MSE: -0.3663
2025-05-03 04:08:29 [INFO]: Epoch 353 - training loss (MAE): 0.0541, validation MSE: -0.4188
2025-05-03 04:08:44 [INFO]: Epoch 354 - training loss (MAE): 0.0542, validation MSE: -0.3114
2025-05-03 04:08:59 [INFO]: Epoch 355 - training loss (MAE): 0.0539, validation MSE: -0.4402
2025-05-03 04:09:14 [INFO]: Epoch 356 - training loss (MAE): 0.0541, validation MSE: -0.4131
2025-05-03 04:09:29 [INFO]: Epoch 357 - training loss (MAE): 0.0541, validation MSE: -0.4358
2025-05-03 04:09:45 [INFO]: Epoch 358 - training loss (MAE): 0.0537, validation MSE: -0.3888
2025-05-03 04:10:00 [INFO]: Epoch 359 - training loss (MAE): 0.0539, validation MSE: -0.4530
2025-05-03 04:10:15 [INFO]: Epoch 360 - training loss (MAE): 0.0535, validation MSE: -0.4201
2025-05-03 04:10:30 [INFO]: Epoch 361 - training loss (MAE): 0.0538, validation MSE: -0.3855
2025-05-03 04:10:45 [INFO]: Epoch 362 - training loss (MAE): 0.0534, validation MSE: -0.3819
2025-05-03 04:11:00 [INFO]: Epoch 363 - training loss (MAE): 0.0536, validation MSE: -0.4085
2025-05-03 04:11:15 [INFO]: Epoch 364 - training loss (MAE): 0.0534, validation MSE: -0.4035
2025-05-03 04:11:31 [INFO]: Epoch 365 - training loss (MAE): 0.0534, validation MSE: -0.4530
2025-05-03 04:11:46 [INFO]: Epoch 366 - training loss (MAE): 0.0532, validation MSE: -0.3960
2025-05-03 04:12:01 [INFO]: Epoch 367 - training loss (MAE): 0.0535, validation MSE: -0.4124
2025-05-03 04:12:16 [INFO]: Epoch 368 - training loss (MAE): 0.0538, validation MSE: -0.4262
2025-05-03 04:12:31 [INFO]: Epoch 369 - training loss (MAE): 0.0534, validation MSE: -0.3678
2025-05-03 04:12:47 [INFO]: Epoch 370 - training loss (MAE): 0.0532, validation MSE: -0.3805
2025-05-03 04:13:02 [INFO]: Epoch 371 - training loss (MAE): 0.0532, validation MSE: -0.4556
2025-05-03 04:13:16 [INFO]: Epoch 372 - training loss (MAE): 0.0526, validation MSE: -0.4105
2025-05-03 04:13:32 [INFO]: Epoch 373 - training loss (MAE): 0.0531, validation MSE: -0.2936
2025-05-03 04:13:47 [INFO]: Epoch 374 - training loss (MAE): 0.0527, validation MSE: -0.2431
2025-05-03 04:14:02 [INFO]: Epoch 375 - training loss (MAE): 0.0532, validation MSE: -0.4343
2025-05-03 04:14:17 [INFO]: Epoch 376 - training loss (MAE): 0.0528, validation MSE: -0.4500
2025-05-03 04:14:32 [INFO]: Epoch 377 - training loss (MAE): 0.0528, validation MSE: -0.4098
2025-05-03 04:14:48 [INFO]: Epoch 378 - training loss (MAE): 0.0534, validation MSE: -0.3896
2025-05-03 04:15:03 [INFO]: Epoch 379 - training loss (MAE): 0.0534, validation MSE: -0.4127
2025-05-03 04:15:18 [INFO]: Epoch 380 - training loss (MAE): 0.0530, validation MSE: -0.4317
2025-05-03 04:15:33 [INFO]: Epoch 381 - training loss (MAE): 0.0534, validation MSE: -0.4412
2025-05-03 04:15:48 [INFO]: Epoch 382 - training loss (MAE): 0.0526, validation MSE: -0.4253
2025-05-03 04:16:03 [INFO]: Epoch 383 - training loss (MAE): 0.0524, validation MSE: -0.4300
2025-05-03 04:16:18 [INFO]: Epoch 384 - training loss (MAE): 0.0527, validation MSE: -0.3691
2025-05-03 04:16:34 [INFO]: Epoch 385 - training loss (MAE): 0.0524, validation MSE: -0.4162
2025-05-03 04:16:49 [INFO]: Epoch 386 - training loss (MAE): 0.0525, validation MSE: -0.3987
2025-05-03 04:17:04 [INFO]: Epoch 387 - training loss (MAE): 0.0523, validation MSE: -0.3865
2025-05-03 04:17:19 [INFO]: Epoch 388 - training loss (MAE): 0.0524, validation MSE: -0.3222
2025-05-03 04:17:34 [INFO]: Epoch 389 - training loss (MAE): 0.0526, validation MSE: -0.3944
2025-05-03 04:17:49 [INFO]: Epoch 390 - training loss (MAE): 0.0522, validation MSE: -0.4278
2025-05-03 04:18:05 [INFO]: Epoch 391 - training loss (MAE): 0.0523, validation MSE: -0.4303
2025-05-03 04:18:20 [INFO]: Epoch 392 - training loss (MAE): 0.0524, validation MSE: -0.4056
2025-05-03 04:18:35 [INFO]: Epoch 393 - training loss (MAE): 0.0525, validation MSE: -0.3943
2025-05-03 04:18:50 [INFO]: Epoch 394 - training loss (MAE): 0.0522, validation MSE: -0.4397
2025-05-03 04:19:05 [INFO]: Epoch 395 - training loss (MAE): 0.0516, validation MSE: -0.3666
2025-05-03 04:19:21 [INFO]: Epoch 396 - training loss (MAE): 0.0522, validation MSE: -0.4101
2025-05-03 04:19:36 [INFO]: Epoch 397 - training loss (MAE): 0.0519, validation MSE: -0.3868
2025-05-03 04:19:51 [INFO]: Epoch 398 - training loss (MAE): 0.0524, validation MSE: -0.3730
2025-05-03 04:20:06 [INFO]: Epoch 399 - training loss (MAE): 0.0525, validation MSE: -0.4226
2025-05-03 04:20:21 [INFO]: Epoch 400 - training loss (MAE): 0.0536, validation MSE: -0.3782
2025-05-03 04:20:36 [INFO]: Epoch 401 - training loss (MAE): 0.0519, validation MSE: -0.3637
2025-05-03 04:20:51 [INFO]: Epoch 402 - training loss (MAE): 0.0521, validation MSE: -0.3571
2025-05-03 04:21:06 [INFO]: Epoch 403 - training loss (MAE): 0.0518, validation MSE: -0.4017
2025-05-03 04:21:21 [INFO]: Epoch 404 - training loss (MAE): 0.0533, validation MSE: -0.4058
2025-05-03 04:21:37 [INFO]: Epoch 405 - training loss (MAE): 0.0518, validation MSE: -0.4157
2025-05-03 04:21:52 [INFO]: Epoch 406 - training loss (MAE): 0.0516, validation MSE: -0.4269
2025-05-03 04:22:07 [INFO]: Epoch 407 - training loss (MAE): 0.0516, validation MSE: -0.4261
2025-05-03 04:22:22 [INFO]: Epoch 408 - training loss (MAE): 0.0521, validation MSE: -0.3956
2025-05-03 04:22:37 [INFO]: Epoch 409 - training loss (MAE): 0.0530, validation MSE: -0.3924
2025-05-03 04:22:52 [INFO]: Epoch 410 - training loss (MAE): 0.0516, validation MSE: -0.4256
2025-05-03 04:23:07 [INFO]: Epoch 411 - training loss (MAE): 0.0521, validation MSE: -0.3734
2025-05-03 04:23:22 [INFO]: Epoch 412 - training loss (MAE): 0.0515, validation MSE: -0.3852
2025-05-03 04:23:38 [INFO]: Epoch 413 - training loss (MAE): 0.0513, validation MSE: -0.4191
2025-05-03 04:23:53 [INFO]: Epoch 414 - training loss (MAE): 0.0523, validation MSE: -0.4398
2025-05-03 04:24:08 [INFO]: Epoch 415 - training loss (MAE): 0.0515, validation MSE: -0.4312
2025-05-03 04:24:23 [INFO]: Epoch 416 - training loss (MAE): 0.0511, validation MSE: -0.3832
2025-05-03 04:24:38 [INFO]: Epoch 417 - training loss (MAE): 0.0519, validation MSE: -0.3907
2025-05-03 04:24:53 [INFO]: Epoch 418 - training loss (MAE): 0.0514, validation MSE: -0.4065
2025-05-03 04:25:09 [INFO]: Epoch 419 - training loss (MAE): 0.0513, validation MSE: -0.4218
2025-05-03 04:25:24 [INFO]: Epoch 420 - training loss (MAE): 0.0510, validation MSE: -0.4129
2025-05-03 04:25:39 [INFO]: Epoch 421 - training loss (MAE): 0.0514, validation MSE: -0.4294
2025-05-03 04:25:54 [INFO]: Epoch 422 - training loss (MAE): 0.0514, validation MSE: -0.4624
2025-05-03 04:26:09 [INFO]: Epoch 423 - training loss (MAE): 0.0513, validation MSE: -0.4224
2025-05-03 04:26:25 [INFO]: Epoch 424 - training loss (MAE): 0.0512, validation MSE: -0.3994
2025-05-03 04:26:40 [INFO]: Epoch 425 - training loss (MAE): 0.0511, validation MSE: -0.3705
2025-05-03 04:26:55 [INFO]: Epoch 426 - training loss (MAE): 0.0511, validation MSE: -0.4280
2025-05-03 04:27:10 [INFO]: Epoch 427 - training loss (MAE): 0.0516, validation MSE: -0.4301
2025-05-03 04:27:26 [INFO]: Epoch 428 - training loss (MAE): 0.0507, validation MSE: -0.4430
2025-05-03 04:27:41 [INFO]: Epoch 429 - training loss (MAE): 0.0508, validation MSE: -0.4326
2025-05-03 04:27:56 [INFO]: Epoch 430 - training loss (MAE): 0.0508, validation MSE: -0.4108
2025-05-03 04:28:11 [INFO]: Epoch 431 - training loss (MAE): 0.0514, validation MSE: -0.3930
2025-05-03 04:28:26 [INFO]: Epoch 432 - training loss (MAE): 0.0505, validation MSE: -0.4245
2025-05-03 04:28:41 [INFO]: Epoch 433 - training loss (MAE): 0.0506, validation MSE: -0.4021
2025-05-03 04:28:57 [INFO]: Epoch 434 - training loss (MAE): 0.0506, validation MSE: -0.4307
2025-05-03 04:29:12 [INFO]: Epoch 435 - training loss (MAE): 0.0509, validation MSE: -0.4210
2025-05-03 04:29:27 [INFO]: Epoch 436 - training loss (MAE): 0.0509, validation MSE: -0.4153
2025-05-03 04:29:42 [INFO]: Epoch 437 - training loss (MAE): 0.0506, validation MSE: -0.3955
2025-05-03 04:29:57 [INFO]: Epoch 438 - training loss (MAE): 0.0507, validation MSE: -0.4266
2025-05-03 04:30:12 [INFO]: Epoch 439 - training loss (MAE): 0.0506, validation MSE: -0.4415
2025-05-03 04:30:27 [INFO]: Epoch 440 - training loss (MAE): 0.0503, validation MSE: -0.4631
2025-05-03 04:30:43 [INFO]: Epoch 441 - training loss (MAE): 0.0504, validation MSE: -0.4475
2025-05-03 04:30:58 [INFO]: Epoch 442 - training loss (MAE): 0.0503, validation MSE: -0.4208
2025-05-03 04:31:13 [INFO]: Epoch 443 - training loss (MAE): 0.0506, validation MSE: -0.4026
2025-05-03 04:31:28 [INFO]: Epoch 444 - training loss (MAE): 0.0507, validation MSE: -0.4563
2025-05-03 04:31:43 [INFO]: Epoch 445 - training loss (MAE): 0.0501, validation MSE: -0.4556
2025-05-03 04:31:58 [INFO]: Epoch 446 - training loss (MAE): 0.0502, validation MSE: -0.4350
2025-05-03 04:32:13 [INFO]: Epoch 447 - training loss (MAE): 0.0505, validation MSE: -0.4089
2025-05-03 04:32:28 [INFO]: Epoch 448 - training loss (MAE): 0.0504, validation MSE: -0.4240
2025-05-03 04:32:44 [INFO]: Epoch 449 - training loss (MAE): 0.0497, validation MSE: -0.4065
2025-05-03 04:32:59 [INFO]: Epoch 450 - training loss (MAE): 0.0500, validation MSE: -0.3585
2025-05-03 04:33:14 [INFO]: Epoch 451 - training loss (MAE): 0.0510, validation MSE: -0.4289
2025-05-03 04:33:29 [INFO]: Epoch 452 - training loss (MAE): 0.0497, validation MSE: -0.4254
2025-05-03 04:33:44 [INFO]: Epoch 453 - training loss (MAE): 0.0501, validation MSE: -0.4330
2025-05-03 04:33:59 [INFO]: Epoch 454 - training loss (MAE): 0.0501, validation MSE: -0.4148
2025-05-03 04:34:14 [INFO]: Epoch 455 - training loss (MAE): 0.0504, validation MSE: -0.3843
2025-05-03 04:34:30 [INFO]: Epoch 456 - training loss (MAE): 0.0497, validation MSE: -0.4277
2025-05-03 04:34:45 [INFO]: Epoch 457 - training loss (MAE): 0.0503, validation MSE: -0.4291
2025-05-03 04:35:00 [INFO]: Epoch 458 - training loss (MAE): 0.0498, validation MSE: -0.3734
2025-05-03 04:35:15 [INFO]: Epoch 459 - training loss (MAE): 0.0498, validation MSE: -0.4057
2025-05-03 04:35:30 [INFO]: Epoch 460 - training loss (MAE): 0.0495, validation MSE: -0.3817
2025-05-03 04:35:46 [INFO]: Epoch 461 - training loss (MAE): 0.0495, validation MSE: -0.3902
2025-05-03 04:36:01 [INFO]: Epoch 462 - training loss (MAE): 0.0498, validation MSE: -0.3423
2025-05-03 04:36:16 [INFO]: Epoch 463 - training loss (MAE): 0.0496, validation MSE: -0.2794
2025-05-03 04:36:31 [INFO]: Epoch 464 - training loss (MAE): 0.0499, validation MSE: -0.2908
2025-05-03 04:36:46 [INFO]: Epoch 465 - training loss (MAE): 0.0492, validation MSE: -0.4165
2025-05-03 04:37:01 [INFO]: Epoch 466 - training loss (MAE): 0.0505, validation MSE: -0.3693
2025-05-03 04:37:17 [INFO]: Epoch 467 - training loss (MAE): 0.0495, validation MSE: -0.4451
2025-05-03 04:37:32 [INFO]: Epoch 468 - training loss (MAE): 0.0494, validation MSE: -0.4403
2025-05-03 04:37:47 [INFO]: Epoch 469 - training loss (MAE): 0.0494, validation MSE: -0.4473
2025-05-03 04:38:02 [INFO]: Epoch 470 - training loss (MAE): 0.0491, validation MSE: -0.3993
2025-05-03 04:38:17 [INFO]: Epoch 471 - training loss (MAE): 0.0491, validation MSE: -0.4383
2025-05-03 04:38:32 [INFO]: Epoch 472 - training loss (MAE): 0.0493, validation MSE: -0.3860
2025-05-03 04:38:48 [INFO]: Epoch 473 - training loss (MAE): 0.0493, validation MSE: -0.4133
2025-05-03 04:39:03 [INFO]: Epoch 474 - training loss (MAE): 0.0488, validation MSE: -0.4420
2025-05-03 04:39:18 [INFO]: Epoch 475 - training loss (MAE): 0.0496, validation MSE: -0.4121
2025-05-03 04:39:33 [INFO]: Epoch 476 - training loss (MAE): 0.0500, validation MSE: -0.4253
2025-05-03 04:39:48 [INFO]: Epoch 477 - training loss (MAE): 0.0490, validation MSE: -0.4039
2025-05-03 04:40:03 [INFO]: Epoch 478 - training loss (MAE): 0.0488, validation MSE: -0.4122
2025-05-03 04:40:18 [INFO]: Epoch 479 - training loss (MAE): 0.0494, validation MSE: -0.4000
2025-05-03 04:40:34 [INFO]: Epoch 480 - training loss (MAE): 0.0485, validation MSE: -0.4132
2025-05-03 04:40:49 [INFO]: Epoch 481 - training loss (MAE): 0.0489, validation MSE: -0.4088
2025-05-03 04:41:04 [INFO]: Epoch 482 - training loss (MAE): 0.0491, validation MSE: -0.4400
2025-05-03 04:41:19 [INFO]: Epoch 483 - training loss (MAE): 0.0486, validation MSE: -0.3569
2025-05-03 04:41:34 [INFO]: Epoch 484 - training loss (MAE): 0.0488, validation MSE: -0.4133
2025-05-03 04:41:50 [INFO]: Epoch 485 - training loss (MAE): 0.0490, validation MSE: -0.4216
2025-05-03 04:42:05 [INFO]: Epoch 486 - training loss (MAE): 0.0481, validation MSE: -0.4180
2025-05-03 04:42:20 [INFO]: Epoch 487 - training loss (MAE): 0.0485, validation MSE: -0.4410
2025-05-03 04:42:35 [INFO]: Epoch 488 - training loss (MAE): 0.0484, validation MSE: -0.3742
2025-05-03 04:42:50 [INFO]: Epoch 489 - training loss (MAE): 0.0483, validation MSE: -0.4313
2025-05-03 04:43:05 [INFO]: Epoch 490 - training loss (MAE): 0.0483, validation MSE: -0.4256
2025-05-03 04:43:20 [INFO]: Epoch 491 - training loss (MAE): 0.0488, validation MSE: -0.3919
2025-05-03 04:43:35 [INFO]: Epoch 492 - training loss (MAE): 0.0485, validation MSE: -0.4231
2025-05-03 04:43:51 [INFO]: Epoch 493 - training loss (MAE): 0.0483, validation MSE: -0.3337
2025-05-03 04:44:06 [INFO]: Epoch 494 - training loss (MAE): 0.0492, validation MSE: -0.3987
2025-05-03 04:44:21 [INFO]: Epoch 495 - training loss (MAE): 0.0482, validation MSE: -0.4266
2025-05-03 04:44:37 [INFO]: Epoch 496 - training loss (MAE): 0.0484, validation MSE: -0.4082
2025-05-03 04:44:52 [INFO]: Epoch 497 - training loss (MAE): 0.0489, validation MSE: -0.4037
2025-05-03 04:45:07 [INFO]: Epoch 498 - training loss (MAE): 0.0487, validation MSE: -0.4324
2025-05-03 04:45:22 [INFO]: Epoch 499 - training loss (MAE): 0.0486, validation MSE: -0.4184
2025-05-03 04:45:37 [INFO]: Epoch 500 - training loss (MAE): 0.0481, validation MSE: -0.4206
2025-05-03 04:45:37 [INFO]: Finished training. The best model is from epoch#109.
2025-05-03 04:45:38 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_6.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 04:45:38 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_6.pypots
Fold 6 metrics: MAE: 0.283, MSE: 0.335, MRE: 0.388
Fold 6 metrics: MAE: 0.283, MSE: 0.335, MRE: 0.388
Training fold 7/10
2025-05-03 04:45:38 [INFO]: No given device, using default device: cuda
2025-05-03 04:45:38 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-03 04:45:38 [INFO]: Using customized MAE as the training loss function.
2025-05-03 04:45:38 [INFO]: Using customized MSE as the validation metric function.
2025-05-03 04:45:38 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-03 04:45:53 [INFO]: Epoch 001 - training loss (MAE): 1.3434, validation MSE: 0.0727
2025-05-03 04:46:08 [INFO]: Epoch 002 - training loss (MAE): 0.7854, validation MSE: -0.0096
2025-05-03 04:46:23 [INFO]: Epoch 003 - training loss (MAE): 0.7058, validation MSE: -0.0566
2025-05-03 04:46:38 [INFO]: Epoch 004 - training loss (MAE): 0.6519, validation MSE: -0.0847
2025-05-03 04:46:54 [INFO]: Epoch 005 - training loss (MAE): 0.6076, validation MSE: -0.1176
2025-05-03 04:47:09 [INFO]: Epoch 006 - training loss (MAE): 0.5708, validation MSE: -0.1629
2025-05-03 04:47:24 [INFO]: Epoch 007 - training loss (MAE): 0.5411, validation MSE: -0.1989
2025-05-03 04:47:39 [INFO]: Epoch 008 - training loss (MAE): 0.5178, validation MSE: -0.2091
2025-05-03 04:47:54 [INFO]: Epoch 009 - training loss (MAE): 0.4972, validation MSE: -0.2238
2025-05-03 04:48:09 [INFO]: Epoch 010 - training loss (MAE): 0.4784, validation MSE: -0.2195
2025-05-03 04:48:25 [INFO]: Epoch 011 - training loss (MAE): 0.4641, validation MSE: -0.2264
2025-05-03 04:48:40 [INFO]: Epoch 012 - training loss (MAE): 0.4517, validation MSE: -0.2290
2025-05-03 04:48:56 [INFO]: Epoch 013 - training loss (MAE): 0.4422, validation MSE: -0.2449
2025-05-03 04:49:11 [INFO]: Epoch 014 - training loss (MAE): 0.4324, validation MSE: -0.2304
2025-05-03 04:49:26 [INFO]: Epoch 015 - training loss (MAE): 0.4250, validation MSE: -0.2493
2025-05-03 04:49:41 [INFO]: Epoch 016 - training loss (MAE): 0.4182, validation MSE: -0.2731
2025-05-03 04:49:56 [INFO]: Epoch 017 - training loss (MAE): 0.4117, validation MSE: -0.2599
2025-05-03 04:50:11 [INFO]: Epoch 018 - training loss (MAE): 0.4072, validation MSE: -0.2558
2025-05-03 04:50:26 [INFO]: Epoch 019 - training loss (MAE): 0.4030, validation MSE: -0.2697
2025-05-03 04:50:41 [INFO]: Epoch 020 - training loss (MAE): 0.3984, validation MSE: -0.2788
2025-05-03 04:50:56 [INFO]: Epoch 021 - training loss (MAE): 0.3944, validation MSE: -0.2729
2025-05-03 04:51:12 [INFO]: Epoch 022 - training loss (MAE): 0.3912, validation MSE: -0.2686
2025-05-03 04:51:27 [INFO]: Epoch 023 - training loss (MAE): 0.3884, validation MSE: -0.2850
2025-05-03 04:51:42 [INFO]: Epoch 024 - training loss (MAE): 0.3863, validation MSE: -0.2822
2025-05-03 04:51:57 [INFO]: Epoch 025 - training loss (MAE): 0.3830, validation MSE: -0.2892
2025-05-03 04:52:12 [INFO]: Epoch 026 - training loss (MAE): 0.3823, validation MSE: -0.2877
2025-05-03 04:52:27 [INFO]: Epoch 027 - training loss (MAE): 0.3795, validation MSE: -0.2792
2025-05-03 04:52:42 [INFO]: Epoch 028 - training loss (MAE): 0.3775, validation MSE: -0.2989
2025-05-03 04:52:57 [INFO]: Epoch 029 - training loss (MAE): 0.3763, validation MSE: -0.2883
2025-05-03 04:53:12 [INFO]: Epoch 030 - training loss (MAE): 0.3741, validation MSE: -0.2894
2025-05-03 04:53:28 [INFO]: Epoch 031 - training loss (MAE): 0.3739, validation MSE: -0.3048
2025-05-03 04:53:43 [INFO]: Epoch 032 - training loss (MAE): 0.3726, validation MSE: -0.2894
2025-05-03 04:53:58 [INFO]: Epoch 033 - training loss (MAE): 0.3716, validation MSE: -0.3200
2025-05-03 04:54:13 [INFO]: Epoch 034 - training loss (MAE): 0.3713, validation MSE: -0.2741
2025-05-03 04:54:28 [INFO]: Epoch 035 - training loss (MAE): 0.3697, validation MSE: -0.2902
2025-05-03 04:54:43 [INFO]: Epoch 036 - training loss (MAE): 0.3690, validation MSE: -0.3161
2025-05-03 04:54:59 [INFO]: Epoch 037 - training loss (MAE): 0.3674, validation MSE: -0.3201
2025-05-03 04:55:14 [INFO]: Epoch 038 - training loss (MAE): 0.3667, validation MSE: -0.3213
2025-05-03 04:55:29 [INFO]: Epoch 039 - training loss (MAE): 0.3669, validation MSE: -0.3046
2025-05-03 04:55:44 [INFO]: Epoch 040 - training loss (MAE): 0.3654, validation MSE: -0.3112
2025-05-03 04:55:59 [INFO]: Epoch 041 - training loss (MAE): 0.3645, validation MSE: -0.3248
2025-05-03 04:56:14 [INFO]: Epoch 042 - training loss (MAE): 0.3638, validation MSE: -0.3330
2025-05-03 04:56:29 [INFO]: Epoch 043 - training loss (MAE): 0.3635, validation MSE: -0.3090
2025-05-03 04:56:44 [INFO]: Epoch 044 - training loss (MAE): 0.3636, validation MSE: -0.3052
2025-05-03 04:56:59 [INFO]: Epoch 045 - training loss (MAE): 0.3618, validation MSE: -0.3218
2025-05-03 04:57:14 [INFO]: Epoch 046 - training loss (MAE): 0.3626, validation MSE: -0.3044
2025-05-03 04:57:29 [INFO]: Epoch 047 - training loss (MAE): 0.3626, validation MSE: -0.3318
2025-05-03 04:57:45 [INFO]: Epoch 048 - training loss (MAE): 0.3612, validation MSE: -0.3238
2025-05-03 04:58:00 [INFO]: Epoch 049 - training loss (MAE): 0.3609, validation MSE: -0.3509
2025-05-03 04:58:15 [INFO]: Epoch 050 - training loss (MAE): 0.3598, validation MSE: -0.3329
2025-05-03 04:58:30 [INFO]: Epoch 051 - training loss (MAE): 0.3601, validation MSE: -0.3222
2025-05-03 04:58:45 [INFO]: Epoch 052 - training loss (MAE): 0.3596, validation MSE: -0.3443
2025-05-03 04:59:00 [INFO]: Epoch 053 - training loss (MAE): 0.3594, validation MSE: -0.3411
2025-05-03 04:59:16 [INFO]: Epoch 054 - training loss (MAE): 0.3589, validation MSE: -0.3406
2025-05-03 04:59:31 [INFO]: Epoch 055 - training loss (MAE): 0.3592, validation MSE: -0.3361
2025-05-03 04:59:46 [INFO]: Epoch 056 - training loss (MAE): 0.3585, validation MSE: -0.3335
2025-05-03 05:00:01 [INFO]: Epoch 057 - training loss (MAE): 0.3579, validation MSE: -0.3253
2025-05-03 05:00:17 [INFO]: Epoch 058 - training loss (MAE): 0.3576, validation MSE: -0.3470
2025-05-03 05:00:32 [INFO]: Epoch 059 - training loss (MAE): 0.3571, validation MSE: -0.3470
2025-05-03 05:00:48 [INFO]: Epoch 060 - training loss (MAE): 0.3573, validation MSE: -0.3384
2025-05-03 05:01:03 [INFO]: Epoch 061 - training loss (MAE): 0.3573, validation MSE: -0.3589
2025-05-03 05:01:18 [INFO]: Epoch 062 - training loss (MAE): 0.3563, validation MSE: -0.3379
2025-05-03 05:01:34 [INFO]: Epoch 063 - training loss (MAE): 0.3562, validation MSE: -0.3545
2025-05-03 05:01:49 [INFO]: Epoch 064 - training loss (MAE): 0.3559, validation MSE: -0.3461
2025-05-03 05:02:04 [INFO]: Epoch 065 - training loss (MAE): 0.3562, validation MSE: -0.3405
2025-05-03 05:02:19 [INFO]: Epoch 066 - training loss (MAE): 0.3554, validation MSE: -0.3450
2025-05-03 05:02:34 [INFO]: Epoch 067 - training loss (MAE): 0.3554, validation MSE: -0.3473
2025-05-03 05:02:49 [INFO]: Epoch 068 - training loss (MAE): 0.3553, validation MSE: -0.3498
2025-05-03 05:03:05 [INFO]: Epoch 069 - training loss (MAE): 0.3547, validation MSE: -0.3235
2025-05-03 05:03:20 [INFO]: Epoch 070 - training loss (MAE): 0.3543, validation MSE: -0.3448
2025-05-03 05:03:35 [INFO]: Epoch 071 - training loss (MAE): 0.3544, validation MSE: -0.3579
2025-05-03 05:03:50 [INFO]: Epoch 072 - training loss (MAE): 0.3547, validation MSE: -0.3548
2025-05-03 05:04:05 [INFO]: Epoch 073 - training loss (MAE): 0.3543, validation MSE: -0.3542
2025-05-03 05:04:21 [INFO]: Epoch 074 - training loss (MAE): 0.3538, validation MSE: -0.3566
2025-05-03 05:04:36 [INFO]: Epoch 075 - training loss (MAE): 0.3540, validation MSE: -0.3576
2025-05-03 05:04:51 [INFO]: Epoch 076 - training loss (MAE): 0.3539, validation MSE: -0.3423
2025-05-03 05:05:06 [INFO]: Epoch 077 - training loss (MAE): 0.3536, validation MSE: -0.3370
2025-05-03 05:05:21 [INFO]: Epoch 078 - training loss (MAE): 0.3524, validation MSE: -0.3472
2025-05-03 05:05:36 [INFO]: Epoch 079 - training loss (MAE): 0.3522, validation MSE: -0.3437
2025-05-03 05:05:51 [INFO]: Epoch 080 - training loss (MAE): 0.3534, validation MSE: -0.3688
2025-05-03 05:06:06 [INFO]: Epoch 081 - training loss (MAE): 0.3525, validation MSE: -0.3398
2025-05-03 05:06:21 [INFO]: Epoch 082 - training loss (MAE): 0.3528, validation MSE: -0.3378
2025-05-03 05:06:36 [INFO]: Epoch 083 - training loss (MAE): 0.3532, validation MSE: -0.3549
2025-05-03 05:06:52 [INFO]: Epoch 084 - training loss (MAE): 0.3521, validation MSE: -0.3564
2025-05-03 05:07:07 [INFO]: Epoch 085 - training loss (MAE): 0.3524, validation MSE: -0.3435
2025-05-03 05:07:22 [INFO]: Epoch 086 - training loss (MAE): 0.3527, validation MSE: -0.3566
2025-05-03 05:07:38 [INFO]: Epoch 087 - training loss (MAE): 0.3527, validation MSE: -0.3332
2025-05-03 05:07:53 [INFO]: Epoch 088 - training loss (MAE): 0.3525, validation MSE: -0.3491
2025-05-03 05:08:09 [INFO]: Epoch 089 - training loss (MAE): 0.3511, validation MSE: -0.3466
2025-05-03 05:08:24 [INFO]: Epoch 090 - training loss (MAE): 0.3513, validation MSE: -0.3430
2025-05-03 05:08:40 [INFO]: Epoch 091 - training loss (MAE): 0.3519, validation MSE: -0.3488
2025-05-03 05:08:55 [INFO]: Epoch 092 - training loss (MAE): 0.3511, validation MSE: -0.3668
2025-05-03 05:09:10 [INFO]: Epoch 093 - training loss (MAE): 0.3510, validation MSE: -0.3422
2025-05-03 05:09:25 [INFO]: Epoch 094 - training loss (MAE): 0.3507, validation MSE: -0.3445
2025-05-03 05:09:40 [INFO]: Epoch 095 - training loss (MAE): 0.3507, validation MSE: -0.3684
2025-05-03 05:09:55 [INFO]: Epoch 096 - training loss (MAE): 0.3508, validation MSE: -0.3443
2025-05-03 05:10:10 [INFO]: Epoch 097 - training loss (MAE): 0.3520, validation MSE: -0.3471
2025-05-03 05:10:26 [INFO]: Epoch 098 - training loss (MAE): 0.3503, validation MSE: -0.3401
2025-05-03 05:10:41 [INFO]: Epoch 099 - training loss (MAE): 0.3503, validation MSE: -0.3314
2025-05-03 05:10:56 [INFO]: Epoch 100 - training loss (MAE): 0.3498, validation MSE: -0.3575
2025-05-03 05:11:11 [INFO]: Epoch 101 - training loss (MAE): 0.3507, validation MSE: -0.3595
2025-05-03 05:11:27 [INFO]: Epoch 102 - training loss (MAE): 0.3497, validation MSE: -0.3619
2025-05-03 05:11:42 [INFO]: Epoch 103 - training loss (MAE): 0.3501, validation MSE: -0.3569
2025-05-03 05:11:57 [INFO]: Epoch 104 - training loss (MAE): 0.3499, validation MSE: -0.3521
2025-05-03 05:12:12 [INFO]: Epoch 105 - training loss (MAE): 0.3502, validation MSE: -0.3654
2025-05-03 05:12:27 [INFO]: Epoch 106 - training loss (MAE): 0.3498, validation MSE: -0.3583
2025-05-03 05:12:42 [INFO]: Epoch 107 - training loss (MAE): 0.3491, validation MSE: -0.3477
2025-05-03 05:12:57 [INFO]: Epoch 108 - training loss (MAE): 0.3493, validation MSE: -0.3371
2025-05-03 05:13:12 [INFO]: Epoch 109 - training loss (MAE): 0.3495, validation MSE: -0.3442
2025-05-03 05:13:28 [INFO]: Epoch 110 - training loss (MAE): 0.3495, validation MSE: -0.3453
2025-05-03 05:13:43 [INFO]: Epoch 111 - training loss (MAE): 0.3493, validation MSE: -0.3578
2025-05-03 05:13:58 [INFO]: Epoch 112 - training loss (MAE): 0.3486, validation MSE: -0.3536
2025-05-03 05:14:13 [INFO]: Epoch 113 - training loss (MAE): 0.3492, validation MSE: -0.3548
2025-05-03 05:14:28 [INFO]: Epoch 114 - training loss (MAE): 0.3492, validation MSE: -0.3423
2025-05-03 05:14:43 [INFO]: Epoch 115 - training loss (MAE): 0.3485, validation MSE: -0.3359
2025-05-03 05:14:59 [INFO]: Epoch 116 - training loss (MAE): 0.3483, validation MSE: -0.3598
2025-05-03 05:15:14 [INFO]: Epoch 117 - training loss (MAE): 0.3485, validation MSE: -0.3577
2025-05-03 05:15:29 [INFO]: Epoch 118 - training loss (MAE): 0.3482, validation MSE: -0.3501
2025-05-03 05:15:44 [INFO]: Epoch 119 - training loss (MAE): 0.3487, validation MSE: -0.3594
2025-05-03 05:15:59 [INFO]: Epoch 120 - training loss (MAE): 0.3482, validation MSE: -0.3553
2025-05-03 05:16:14 [INFO]: Epoch 121 - training loss (MAE): 0.3488, validation MSE: -0.3500
2025-05-03 05:16:29 [INFO]: Epoch 122 - training loss (MAE): 0.3483, validation MSE: -0.3493
2025-05-03 05:16:44 [INFO]: Epoch 123 - training loss (MAE): 0.3480, validation MSE: -0.3166
2025-05-03 05:16:59 [INFO]: Epoch 124 - training loss (MAE): 0.3480, validation MSE: -0.3523
2025-05-03 05:17:14 [INFO]: Epoch 125 - training loss (MAE): 0.3488, validation MSE: -0.3552
2025-05-03 05:17:29 [INFO]: Epoch 126 - training loss (MAE): 0.3481, validation MSE: -0.3324
2025-05-03 05:17:45 [INFO]: Epoch 127 - training loss (MAE): 0.3476, validation MSE: -0.3305
2025-05-03 05:18:00 [INFO]: Epoch 128 - training loss (MAE): 0.3478, validation MSE: -0.3375
2025-05-03 05:18:15 [INFO]: Epoch 129 - training loss (MAE): 0.3476, validation MSE: -0.3406
2025-05-03 05:18:30 [INFO]: Epoch 130 - training loss (MAE): 0.3477, validation MSE: -0.3467
2025-05-03 05:18:45 [INFO]: Epoch 131 - training loss (MAE): 0.3470, validation MSE: -0.3489
2025-05-03 05:19:00 [INFO]: Epoch 132 - training loss (MAE): 0.3475, validation MSE: -0.3347
2025-05-03 05:19:15 [INFO]: Epoch 133 - training loss (MAE): 0.3475, validation MSE: -0.3300
2025-05-03 05:19:30 [INFO]: Epoch 134 - training loss (MAE): 0.3476, validation MSE: -0.3252
2025-05-03 05:19:45 [INFO]: Epoch 135 - training loss (MAE): 0.3469, validation MSE: -0.3410
2025-05-03 05:20:01 [INFO]: Epoch 136 - training loss (MAE): 0.3466, validation MSE: -0.3218
2025-05-03 05:20:16 [INFO]: Epoch 137 - training loss (MAE): 0.3472, validation MSE: -0.3079
2025-05-03 05:20:31 [INFO]: Epoch 138 - training loss (MAE): 0.3474, validation MSE: -0.3590
2025-05-03 05:20:46 [INFO]: Epoch 139 - training loss (MAE): 0.3477, validation MSE: -0.3415
2025-05-03 05:21:01 [INFO]: Epoch 140 - training loss (MAE): 0.3461, validation MSE: -0.3405
2025-05-03 05:21:16 [INFO]: Epoch 141 - training loss (MAE): 0.3473, validation MSE: -0.3332
2025-05-03 05:21:31 [INFO]: Epoch 142 - training loss (MAE): 0.3473, validation MSE: -0.3560
2025-05-03 05:21:47 [INFO]: Epoch 143 - training loss (MAE): 0.3464, validation MSE: -0.3303
2025-05-03 05:22:02 [INFO]: Epoch 144 - training loss (MAE): 0.3460, validation MSE: -0.3444
2025-05-03 05:22:17 [INFO]: Epoch 145 - training loss (MAE): 0.3471, validation MSE: -0.3243
2025-05-03 05:22:32 [INFO]: Epoch 146 - training loss (MAE): 0.3467, validation MSE: -0.3329
2025-05-03 05:22:48 [INFO]: Epoch 147 - training loss (MAE): 0.3462, validation MSE: -0.3151
2025-05-03 05:23:03 [INFO]: Epoch 148 - training loss (MAE): 0.3468, validation MSE: -0.3276
2025-05-03 05:23:18 [INFO]: Epoch 149 - training loss (MAE): 0.3465, validation MSE: -0.3371
2025-05-03 05:23:33 [INFO]: Epoch 150 - training loss (MAE): 0.3464, validation MSE: -0.3303
2025-05-03 05:23:48 [INFO]: Epoch 151 - training loss (MAE): 0.3458, validation MSE: -0.3279
2025-05-03 05:24:04 [INFO]: Epoch 152 - training loss (MAE): 0.3455, validation MSE: -0.3298
2025-05-03 05:24:19 [INFO]: Epoch 153 - training loss (MAE): 0.3460, validation MSE: -0.3146
2025-05-03 05:24:34 [INFO]: Epoch 154 - training loss (MAE): 0.3464, validation MSE: -0.3454
2025-05-03 05:24:50 [INFO]: Epoch 155 - training loss (MAE): 0.3456, validation MSE: -0.3347
2025-05-03 05:25:05 [INFO]: Epoch 156 - training loss (MAE): 0.3458, validation MSE: -0.3336
2025-05-03 05:25:20 [INFO]: Epoch 157 - training loss (MAE): 0.3459, validation MSE: -0.3061
2025-05-03 05:25:35 [INFO]: Epoch 158 - training loss (MAE): 0.3457, validation MSE: -0.3341
2025-05-03 05:25:50 [INFO]: Epoch 159 - training loss (MAE): 0.3453, validation MSE: -0.3251
2025-05-03 05:26:06 [INFO]: Epoch 160 - training loss (MAE): 0.3466, validation MSE: -0.2874
2025-05-03 05:26:21 [INFO]: Epoch 161 - training loss (MAE): 0.3454, validation MSE: -0.3443
2025-05-03 05:26:36 [INFO]: Epoch 162 - training loss (MAE): 0.3458, validation MSE: -0.3335
2025-05-03 05:26:51 [INFO]: Epoch 163 - training loss (MAE): 0.3450, validation MSE: -0.3232
2025-05-03 05:27:06 [INFO]: Epoch 164 - training loss (MAE): 0.3453, validation MSE: -0.3284
2025-05-03 05:27:21 [INFO]: Epoch 165 - training loss (MAE): 0.3451, validation MSE: -0.3336
2025-05-03 05:27:36 [INFO]: Epoch 166 - training loss (MAE): 0.3460, validation MSE: -0.3223
2025-05-03 05:27:51 [INFO]: Epoch 167 - training loss (MAE): 0.3459, validation MSE: -0.3260
2025-05-03 05:28:06 [INFO]: Epoch 168 - training loss (MAE): 0.3451, validation MSE: -0.2875
2025-05-03 05:28:21 [INFO]: Epoch 169 - training loss (MAE): 0.3448, validation MSE: -0.3299
2025-05-03 05:28:36 [INFO]: Epoch 170 - training loss (MAE): 0.3447, validation MSE: -0.3159
2025-05-03 05:28:51 [INFO]: Epoch 171 - training loss (MAE): 0.3454, validation MSE: -0.3291
2025-05-03 05:29:06 [INFO]: Epoch 172 - training loss (MAE): 0.3449, validation MSE: -0.3171
2025-05-03 05:29:21 [INFO]: Epoch 173 - training loss (MAE): 0.3448, validation MSE: -0.3227
2025-05-03 05:29:36 [INFO]: Epoch 174 - training loss (MAE): 0.3442, validation MSE: -0.2955
2025-05-03 05:29:51 [INFO]: Epoch 175 - training loss (MAE): 0.3448, validation MSE: -0.3375
2025-05-03 05:30:07 [INFO]: Epoch 176 - training loss (MAE): 0.3448, validation MSE: -0.3401
2025-05-03 05:30:22 [INFO]: Epoch 177 - training loss (MAE): 0.3447, validation MSE: -0.3116
2025-05-03 05:30:37 [INFO]: Epoch 178 - training loss (MAE): 0.3451, validation MSE: -0.3411
2025-05-03 05:30:52 [INFO]: Epoch 179 - training loss (MAE): 0.3450, validation MSE: -0.3192
2025-05-03 05:31:07 [INFO]: Epoch 180 - training loss (MAE): 0.3446, validation MSE: -0.3171
2025-05-03 05:31:22 [INFO]: Epoch 181 - training loss (MAE): 0.3443, validation MSE: -0.3071
2025-05-03 05:31:37 [INFO]: Epoch 182 - training loss (MAE): 0.3445, validation MSE: -0.3070
2025-05-03 05:31:52 [INFO]: Epoch 183 - training loss (MAE): 0.3449, validation MSE: -0.3126
2025-05-03 05:32:07 [INFO]: Epoch 184 - training loss (MAE): 0.3443, validation MSE: -0.3220
2025-05-03 05:32:22 [INFO]: Epoch 185 - training loss (MAE): 0.3445, validation MSE: -0.3263
2025-05-03 05:32:38 [INFO]: Epoch 186 - training loss (MAE): 0.3442, validation MSE: -0.3275
2025-05-03 05:32:53 [INFO]: Epoch 187 - training loss (MAE): 0.3442, validation MSE: -0.3415
2025-05-03 05:33:08 [INFO]: Epoch 188 - training loss (MAE): 0.3445, validation MSE: -0.3490
2025-05-03 05:33:23 [INFO]: Epoch 189 - training loss (MAE): 0.3442, validation MSE: -0.3311
2025-05-03 05:33:38 [INFO]: Epoch 190 - training loss (MAE): 0.3439, validation MSE: -0.2688
2025-05-03 05:33:53 [INFO]: Epoch 191 - training loss (MAE): 0.3442, validation MSE: -0.3056
2025-05-03 05:34:08 [INFO]: Epoch 192 - training loss (MAE): 0.3448, validation MSE: -0.3112
2025-05-03 05:34:23 [INFO]: Epoch 193 - training loss (MAE): 0.3436, validation MSE: -0.3323
2025-05-03 05:34:39 [INFO]: Epoch 194 - training loss (MAE): 0.3435, validation MSE: -0.3327
2025-05-03 05:34:54 [INFO]: Epoch 195 - training loss (MAE): 0.3444, validation MSE: -0.3089
2025-05-03 05:35:09 [INFO]: Epoch 196 - training loss (MAE): 0.3436, validation MSE: -0.3041
2025-05-03 05:35:24 [INFO]: Epoch 197 - training loss (MAE): 0.3446, validation MSE: -0.2643
2025-05-03 05:35:39 [INFO]: Epoch 198 - training loss (MAE): 0.3435, validation MSE: -0.3123
2025-05-03 05:35:54 [INFO]: Epoch 199 - training loss (MAE): 0.3437, validation MSE: -0.3193
2025-05-03 05:36:09 [INFO]: Epoch 200 - training loss (MAE): 0.3442, validation MSE: -0.3275
2025-05-03 05:36:24 [INFO]: Epoch 201 - training loss (MAE): 0.3437, validation MSE: -0.3379
2025-05-03 05:36:39 [INFO]: Epoch 202 - training loss (MAE): 0.3435, validation MSE: -0.3076
2025-05-03 05:36:54 [INFO]: Epoch 203 - training loss (MAE): 0.3437, validation MSE: -0.3234
2025-05-03 05:37:09 [INFO]: Epoch 204 - training loss (MAE): 0.3443, validation MSE: -0.2758
2025-05-03 05:37:24 [INFO]: Epoch 205 - training loss (MAE): 0.3436, validation MSE: -0.3027
2025-05-03 05:37:39 [INFO]: Epoch 206 - training loss (MAE): 0.3430, validation MSE: -0.3034
2025-05-03 05:37:54 [INFO]: Epoch 207 - training loss (MAE): 0.3436, validation MSE: -0.3158
2025-05-03 05:38:10 [INFO]: Epoch 208 - training loss (MAE): 0.3438, validation MSE: -0.3031
2025-05-03 05:38:25 [INFO]: Epoch 209 - training loss (MAE): 0.3428, validation MSE: -0.3121
2025-05-03 05:38:40 [INFO]: Epoch 210 - training loss (MAE): 0.3430, validation MSE: -0.3235
2025-05-03 05:38:55 [INFO]: Epoch 211 - training loss (MAE): 0.3436, validation MSE: -0.2927
2025-05-03 05:39:10 [INFO]: Epoch 212 - training loss (MAE): 0.3435, validation MSE: -0.3331
2025-05-03 05:39:25 [INFO]: Epoch 213 - training loss (MAE): 0.3429, validation MSE: -0.3388
2025-05-03 05:39:40 [INFO]: Epoch 214 - training loss (MAE): 0.3427, validation MSE: -0.2735
2025-05-03 05:39:56 [INFO]: Epoch 215 - training loss (MAE): 0.3431, validation MSE: -0.3098
2025-05-03 05:40:11 [INFO]: Epoch 216 - training loss (MAE): 0.3430, validation MSE: -0.3416
2025-05-03 05:40:26 [INFO]: Epoch 217 - training loss (MAE): 0.3432, validation MSE: -0.3030
2025-05-03 05:40:41 [INFO]: Epoch 218 - training loss (MAE): 0.3431, validation MSE: -0.3197
2025-05-03 05:40:56 [INFO]: Epoch 219 - training loss (MAE): 0.3428, validation MSE: -0.3050
2025-05-03 05:41:11 [INFO]: Epoch 220 - training loss (MAE): 0.3431, validation MSE: -0.3055
2025-05-03 05:41:26 [INFO]: Epoch 221 - training loss (MAE): 0.3428, validation MSE: -0.3394
2025-05-03 05:41:41 [INFO]: Epoch 222 - training loss (MAE): 0.3425, validation MSE: -0.3427
2025-05-03 05:41:56 [INFO]: Epoch 223 - training loss (MAE): 0.3432, validation MSE: -0.3377
2025-05-03 05:42:11 [INFO]: Epoch 224 - training loss (MAE): 0.3426, validation MSE: -0.3512
2025-05-03 05:42:27 [INFO]: Epoch 225 - training loss (MAE): 0.3433, validation MSE: -0.3156
2025-05-03 05:42:41 [INFO]: Epoch 226 - training loss (MAE): 0.3427, validation MSE: -0.3310
2025-05-03 05:42:56 [INFO]: Epoch 227 - training loss (MAE): 0.3425, validation MSE: -0.3207
2025-05-03 05:43:12 [INFO]: Epoch 228 - training loss (MAE): 0.3430, validation MSE: -0.3240
2025-05-03 05:43:27 [INFO]: Epoch 229 - training loss (MAE): 0.3432, validation MSE: -0.3381
2025-05-03 05:43:42 [INFO]: Epoch 230 - training loss (MAE): 0.3428, validation MSE: -0.3154
2025-05-03 05:43:57 [INFO]: Epoch 231 - training loss (MAE): 0.3429, validation MSE: -0.3097
2025-05-03 05:44:12 [INFO]: Epoch 232 - training loss (MAE): 0.3426, validation MSE: -0.2975
2025-05-03 05:44:27 [INFO]: Epoch 233 - training loss (MAE): 0.3429, validation MSE: -0.2968
2025-05-03 05:44:42 [INFO]: Epoch 234 - training loss (MAE): 0.3422, validation MSE: -0.3049
2025-05-03 05:44:57 [INFO]: Epoch 235 - training loss (MAE): 0.3428, validation MSE: -0.3388
2025-05-03 05:45:13 [INFO]: Epoch 236 - training loss (MAE): 0.3430, validation MSE: -0.3148
2025-05-03 05:45:28 [INFO]: Epoch 237 - training loss (MAE): 0.3421, validation MSE: -0.2971
2025-05-03 05:45:43 [INFO]: Epoch 238 - training loss (MAE): 0.3423, validation MSE: -0.2926
2025-05-03 05:45:58 [INFO]: Epoch 239 - training loss (MAE): 0.3425, validation MSE: -0.3088
2025-05-03 05:46:13 [INFO]: Epoch 240 - training loss (MAE): 0.3430, validation MSE: -0.2997
2025-05-03 05:46:28 [INFO]: Epoch 241 - training loss (MAE): 0.3431, validation MSE: -0.3216
2025-05-03 05:46:43 [INFO]: Epoch 242 - training loss (MAE): 0.3426, validation MSE: -0.3002
2025-05-03 05:46:58 [INFO]: Epoch 243 - training loss (MAE): 0.3426, validation MSE: -0.2842
2025-05-03 05:47:14 [INFO]: Epoch 244 - training loss (MAE): 0.3422, validation MSE: -0.2894
2025-05-03 05:47:29 [INFO]: Epoch 245 - training loss (MAE): 0.3419, validation MSE: -0.3117
2025-05-03 05:47:44 [INFO]: Epoch 246 - training loss (MAE): 0.3438, validation MSE: -0.2998
2025-05-03 05:47:59 [INFO]: Epoch 247 - training loss (MAE): 0.3420, validation MSE: -0.3197
2025-05-03 05:48:14 [INFO]: Epoch 248 - training loss (MAE): 0.3416, validation MSE: -0.2744
2025-05-03 05:48:29 [INFO]: Epoch 249 - training loss (MAE): 0.3421, validation MSE: -0.3085
2025-05-03 05:48:44 [INFO]: Epoch 250 - training loss (MAE): 0.3412, validation MSE: -0.2903
2025-05-03 05:48:59 [INFO]: Epoch 251 - training loss (MAE): 0.3419, validation MSE: -0.3345
2025-05-03 05:49:15 [INFO]: Epoch 252 - training loss (MAE): 0.3416, validation MSE: -0.3206
2025-05-03 05:49:30 [INFO]: Epoch 253 - training loss (MAE): 0.3430, validation MSE: -0.3377
2025-05-03 05:49:45 [INFO]: Epoch 254 - training loss (MAE): 0.3420, validation MSE: -0.3079
2025-05-03 05:50:00 [INFO]: Epoch 255 - training loss (MAE): 0.3417, validation MSE: -0.3392
2025-05-03 05:50:15 [INFO]: Epoch 256 - training loss (MAE): 0.3413, validation MSE: -0.3288
2025-05-03 05:50:30 [INFO]: Epoch 257 - training loss (MAE): 0.3409, validation MSE: -0.3437
2025-05-03 05:50:45 [INFO]: Epoch 258 - training loss (MAE): 0.3425, validation MSE: -0.3051
2025-05-03 05:51:00 [INFO]: Epoch 259 - training loss (MAE): 0.3416, validation MSE: -0.3334
2025-05-03 05:51:15 [INFO]: Epoch 260 - training loss (MAE): 0.3417, validation MSE: -0.3320
2025-05-03 05:51:31 [INFO]: Epoch 261 - training loss (MAE): 0.3420, validation MSE: -0.3515
2025-05-03 05:51:46 [INFO]: Epoch 262 - training loss (MAE): 0.3412, validation MSE: -0.3301
2025-05-03 05:52:01 [INFO]: Epoch 263 - training loss (MAE): 0.3423, validation MSE: -0.3194
2025-05-03 05:52:16 [INFO]: Epoch 264 - training loss (MAE): 0.3412, validation MSE: -0.3330
2025-05-03 05:52:31 [INFO]: Epoch 265 - training loss (MAE): 0.3424, validation MSE: -0.3262
2025-05-03 05:52:46 [INFO]: Epoch 266 - training loss (MAE): 0.3418, validation MSE: -0.3037
2025-05-03 05:53:02 [INFO]: Epoch 267 - training loss (MAE): 0.3416, validation MSE: -0.3029
2025-05-03 05:53:17 [INFO]: Epoch 268 - training loss (MAE): 0.3414, validation MSE: -0.3105
2025-05-03 05:53:32 [INFO]: Epoch 269 - training loss (MAE): 0.3417, validation MSE: -0.2929
2025-05-03 05:53:47 [INFO]: Epoch 270 - training loss (MAE): 0.3415, validation MSE: -0.3507
2025-05-03 05:54:02 [INFO]: Epoch 271 - training loss (MAE): 0.3414, validation MSE: -0.3454
2025-05-03 05:54:17 [INFO]: Epoch 272 - training loss (MAE): 0.3413, validation MSE: -0.3260
2025-05-03 05:54:32 [INFO]: Epoch 273 - training loss (MAE): 0.3414, validation MSE: -0.3445
2025-05-03 05:54:47 [INFO]: Epoch 274 - training loss (MAE): 0.3417, validation MSE: -0.3253
2025-05-03 05:55:02 [INFO]: Epoch 275 - training loss (MAE): 0.3408, validation MSE: -0.3179
2025-05-03 05:55:17 [INFO]: Epoch 276 - training loss (MAE): 0.3425, validation MSE: -0.3306
2025-05-03 05:55:32 [INFO]: Epoch 277 - training loss (MAE): 0.3410, validation MSE: -0.2910
2025-05-03 05:55:47 [INFO]: Epoch 278 - training loss (MAE): 0.3413, validation MSE: -0.2757
2025-05-03 05:56:02 [INFO]: Epoch 279 - training loss (MAE): 0.3416, validation MSE: -0.3048
2025-05-03 05:56:17 [INFO]: Epoch 280 - training loss (MAE): 0.3417, validation MSE: -0.3067
2025-05-03 05:56:32 [INFO]: Epoch 281 - training loss (MAE): 0.3410, validation MSE: -0.2837
2025-05-03 05:56:47 [INFO]: Epoch 282 - training loss (MAE): 0.3424, validation MSE: -0.2334
2025-05-03 05:57:02 [INFO]: Epoch 283 - training loss (MAE): 0.3429, validation MSE: -0.3198
2025-05-03 05:57:17 [INFO]: Epoch 284 - training loss (MAE): 0.3420, validation MSE: -0.3097
2025-05-03 05:57:32 [INFO]: Epoch 285 - training loss (MAE): 0.3423, validation MSE: -0.3308
2025-05-03 05:57:47 [INFO]: Epoch 286 - training loss (MAE): 0.3415, validation MSE: -0.3110
2025-05-03 05:58:03 [INFO]: Epoch 287 - training loss (MAE): 0.3415, validation MSE: -0.3521
2025-05-03 05:58:18 [INFO]: Epoch 288 - training loss (MAE): 0.3415, validation MSE: -0.3517
2025-05-03 05:58:33 [INFO]: Epoch 289 - training loss (MAE): 0.3407, validation MSE: -0.3178
2025-05-03 05:58:48 [INFO]: Epoch 290 - training loss (MAE): 0.3412, validation MSE: -0.3520
2025-05-03 05:59:03 [INFO]: Epoch 291 - training loss (MAE): 0.3413, validation MSE: -0.3558
2025-05-03 05:59:18 [INFO]: Epoch 292 - training loss (MAE): 0.3409, validation MSE: -0.3532
2025-05-03 05:59:33 [INFO]: Epoch 293 - training loss (MAE): 0.3412, validation MSE: -0.3018
2025-05-03 05:59:48 [INFO]: Epoch 294 - training loss (MAE): 0.3413, validation MSE: -0.3377
2025-05-03 06:00:03 [INFO]: Epoch 295 - training loss (MAE): 0.3413, validation MSE: -0.3455
2025-05-03 06:00:19 [INFO]: Epoch 296 - training loss (MAE): 0.3410, validation MSE: -0.2777
2025-05-03 06:00:34 [INFO]: Epoch 297 - training loss (MAE): 0.3411, validation MSE: -0.3207
2025-05-03 06:00:49 [INFO]: Epoch 298 - training loss (MAE): 0.3408, validation MSE: -0.3316
2025-05-03 06:01:04 [INFO]: Epoch 299 - training loss (MAE): 0.3409, validation MSE: -0.3501
2025-05-03 06:01:19 [INFO]: Epoch 300 - training loss (MAE): 0.3408, validation MSE: -0.3348
2025-05-03 06:01:35 [INFO]: Epoch 301 - training loss (MAE): 0.3409, validation MSE: -0.2743
2025-05-03 06:01:50 [INFO]: Epoch 302 - training loss (MAE): 0.3405, validation MSE: -0.3013
2025-05-03 06:02:05 [INFO]: Epoch 303 - training loss (MAE): 0.3413, validation MSE: -0.2393
2025-05-03 06:02:20 [INFO]: Epoch 304 - training loss (MAE): 0.3415, validation MSE: -0.2830
2025-05-03 06:02:35 [INFO]: Epoch 305 - training loss (MAE): 0.3408, validation MSE: -0.3463
2025-05-03 06:02:50 [INFO]: Epoch 306 - training loss (MAE): 0.3409, validation MSE: -0.3240
2025-05-03 06:03:05 [INFO]: Epoch 307 - training loss (MAE): 0.3406, validation MSE: -0.3290
2025-05-03 06:03:20 [INFO]: Epoch 308 - training loss (MAE): 0.3419, validation MSE: -0.3365
2025-05-03 06:03:35 [INFO]: Epoch 309 - training loss (MAE): 0.3402, validation MSE: -0.3502
2025-05-03 06:03:50 [INFO]: Epoch 310 - training loss (MAE): 0.3405, validation MSE: -0.3074
2025-05-03 06:04:05 [INFO]: Epoch 311 - training loss (MAE): 0.3403, validation MSE: -0.3778
2025-05-03 06:04:21 [INFO]: Epoch 312 - training loss (MAE): 0.3410, validation MSE: -0.3523
2025-05-03 06:04:36 [INFO]: Epoch 313 - training loss (MAE): 0.3412, validation MSE: -0.3698
2025-05-03 06:04:51 [INFO]: Epoch 314 - training loss (MAE): 0.3410, validation MSE: -0.3540
2025-05-03 06:05:06 [INFO]: Epoch 315 - training loss (MAE): 0.3409, validation MSE: -0.3464
2025-05-03 06:05:22 [INFO]: Epoch 316 - training loss (MAE): 0.3403, validation MSE: -0.3458
2025-05-03 06:05:37 [INFO]: Epoch 317 - training loss (MAE): 0.3406, validation MSE: -0.3339
2025-05-03 06:05:52 [INFO]: Epoch 318 - training loss (MAE): 0.3406, validation MSE: -0.3295
2025-05-03 06:06:07 [INFO]: Epoch 319 - training loss (MAE): 0.3408, validation MSE: -0.3624
2025-05-03 06:06:22 [INFO]: Epoch 320 - training loss (MAE): 0.3400, validation MSE: -0.3565
2025-05-03 06:06:37 [INFO]: Epoch 321 - training loss (MAE): 0.3407, validation MSE: -0.3392
2025-05-03 06:06:52 [INFO]: Epoch 322 - training loss (MAE): 0.3400, validation MSE: -0.3619
2025-05-03 06:07:08 [INFO]: Epoch 323 - training loss (MAE): 0.3410, validation MSE: -0.3782
2025-05-03 06:07:23 [INFO]: Epoch 324 - training loss (MAE): 0.3403, validation MSE: -0.3564
2025-05-03 06:07:38 [INFO]: Epoch 325 - training loss (MAE): 0.3396, validation MSE: -0.3655
2025-05-03 06:07:53 [INFO]: Epoch 326 - training loss (MAE): 0.3409, validation MSE: -0.3525
2025-05-03 06:08:08 [INFO]: Epoch 327 - training loss (MAE): 0.3401, validation MSE: -0.2930
2025-05-03 06:08:23 [INFO]: Epoch 328 - training loss (MAE): 0.3402, validation MSE: -0.3223
2025-05-03 06:08:38 [INFO]: Epoch 329 - training loss (MAE): 0.3406, validation MSE: -0.3250
2025-05-03 06:08:53 [INFO]: Epoch 330 - training loss (MAE): 0.3396, validation MSE: -0.3582
2025-05-03 06:09:08 [INFO]: Epoch 331 - training loss (MAE): 0.3404, validation MSE: -0.2943
2025-05-03 06:09:23 [INFO]: Epoch 332 - training loss (MAE): 0.3403, validation MSE: -0.3558
2025-05-03 06:09:39 [INFO]: Epoch 333 - training loss (MAE): 0.3406, validation MSE: -0.3050
2025-05-03 06:09:54 [INFO]: Epoch 334 - training loss (MAE): 0.3400, validation MSE: -0.2438
2025-05-03 06:10:09 [INFO]: Epoch 335 - training loss (MAE): 0.3398, validation MSE: -0.3532
2025-05-03 06:10:24 [INFO]: Epoch 336 - training loss (MAE): 0.3404, validation MSE: -0.2537
2025-05-03 06:10:39 [INFO]: Epoch 337 - training loss (MAE): 0.3405, validation MSE: -0.3241
2025-05-03 06:10:54 [INFO]: Epoch 338 - training loss (MAE): 0.3406, validation MSE: -0.3151
2025-05-03 06:11:09 [INFO]: Epoch 339 - training loss (MAE): 0.3408, validation MSE: -0.3632
2025-05-03 06:11:24 [INFO]: Epoch 340 - training loss (MAE): 0.3399, validation MSE: -0.3638
2025-05-03 06:11:40 [INFO]: Epoch 341 - training loss (MAE): 0.3407, validation MSE: -0.3504
2025-05-03 06:11:55 [INFO]: Epoch 342 - training loss (MAE): 0.3401, validation MSE: -0.3412
2025-05-03 06:12:10 [INFO]: Epoch 343 - training loss (MAE): 0.3397, validation MSE: -0.2722
2025-05-03 06:12:25 [INFO]: Epoch 344 - training loss (MAE): 0.3404, validation MSE: -0.3205
2025-05-03 06:12:40 [INFO]: Epoch 345 - training loss (MAE): 0.3404, validation MSE: -0.3234
2025-05-03 06:12:56 [INFO]: Epoch 346 - training loss (MAE): 0.3401, validation MSE: -0.3122
2025-05-03 06:13:11 [INFO]: Epoch 347 - training loss (MAE): 0.3403, validation MSE: -0.3432
2025-05-03 06:13:26 [INFO]: Epoch 348 - training loss (MAE): 0.3405, validation MSE: -0.3306
2025-05-03 06:13:41 [INFO]: Epoch 349 - training loss (MAE): 0.3396, validation MSE: -0.3213
2025-05-03 06:13:57 [INFO]: Epoch 350 - training loss (MAE): 0.3396, validation MSE: -0.3336
2025-05-03 06:14:12 [INFO]: Epoch 351 - training loss (MAE): 0.3398, validation MSE: -0.3573
2025-05-03 06:14:27 [INFO]: Epoch 352 - training loss (MAE): 0.3402, validation MSE: -0.3074
2025-05-03 06:14:42 [INFO]: Epoch 353 - training loss (MAE): 0.3407, validation MSE: -0.3534
2025-05-03 06:14:57 [INFO]: Epoch 354 - training loss (MAE): 0.3404, validation MSE: -0.3586
2025-05-03 06:15:12 [INFO]: Epoch 355 - training loss (MAE): 0.3407, validation MSE: -0.3482
2025-05-03 06:15:27 [INFO]: Epoch 356 - training loss (MAE): 0.3400, validation MSE: -0.2954
2025-05-03 06:15:42 [INFO]: Epoch 357 - training loss (MAE): 0.3402, validation MSE: -0.3547
2025-05-03 06:15:57 [INFO]: Epoch 358 - training loss (MAE): 0.3397, validation MSE: -0.3441
2025-05-03 06:16:13 [INFO]: Epoch 359 - training loss (MAE): 0.3405, validation MSE: -0.3118
2025-05-03 06:16:28 [INFO]: Epoch 360 - training loss (MAE): 0.3401, validation MSE: -0.3427
2025-05-03 06:16:43 [INFO]: Epoch 361 - training loss (MAE): 0.3393, validation MSE: -0.3474
2025-05-03 06:16:58 [INFO]: Epoch 362 - training loss (MAE): 0.3397, validation MSE: -0.3368
2025-05-03 06:17:13 [INFO]: Epoch 363 - training loss (MAE): 0.3396, validation MSE: -0.3359
2025-05-03 06:17:29 [INFO]: Epoch 364 - training loss (MAE): 0.3396, validation MSE: -0.3074
2025-05-03 06:17:43 [INFO]: Epoch 365 - training loss (MAE): 0.3398, validation MSE: -0.3338
2025-05-03 06:17:58 [INFO]: Epoch 366 - training loss (MAE): 0.3402, validation MSE: -0.2542
2025-05-03 06:18:14 [INFO]: Epoch 367 - training loss (MAE): 0.3399, validation MSE: -0.3222
2025-05-03 06:18:29 [INFO]: Epoch 368 - training loss (MAE): 0.3395, validation MSE: -0.3521
2025-05-03 06:18:44 [INFO]: Epoch 369 - training loss (MAE): 0.3394, validation MSE: -0.3441
2025-05-03 06:18:59 [INFO]: Epoch 370 - training loss (MAE): 0.3394, validation MSE: -0.3576
2025-05-03 06:19:14 [INFO]: Epoch 371 - training loss (MAE): 0.3408, validation MSE: -0.2569
2025-05-03 06:19:30 [INFO]: Epoch 372 - training loss (MAE): 0.3397, validation MSE: -0.3502
2025-05-03 06:19:45 [INFO]: Epoch 373 - training loss (MAE): 0.3394, validation MSE: -0.3448
2025-05-03 06:20:00 [INFO]: Epoch 374 - training loss (MAE): 0.3388, validation MSE: -0.3760
2025-05-03 06:20:15 [INFO]: Epoch 375 - training loss (MAE): 0.3398, validation MSE: -0.3002
2025-05-03 06:20:30 [INFO]: Epoch 376 - training loss (MAE): 0.3400, validation MSE: -0.3538
2025-05-03 06:20:45 [INFO]: Epoch 377 - training loss (MAE): 0.3399, validation MSE: -0.3117
2025-05-03 06:21:00 [INFO]: Epoch 378 - training loss (MAE): 0.3394, validation MSE: -0.3316
2025-05-03 06:21:16 [INFO]: Epoch 379 - training loss (MAE): 0.3397, validation MSE: -0.3693
2025-05-03 06:21:31 [INFO]: Epoch 380 - training loss (MAE): 0.3397, validation MSE: -0.3672
2025-05-03 06:21:46 [INFO]: Epoch 381 - training loss (MAE): 0.3393, validation MSE: -0.3236
2025-05-03 06:22:01 [INFO]: Epoch 382 - training loss (MAE): 0.3393, validation MSE: -0.3379
2025-05-03 06:22:17 [INFO]: Epoch 383 - training loss (MAE): 0.3398, validation MSE: -0.3331
2025-05-03 06:22:32 [INFO]: Epoch 384 - training loss (MAE): 0.3394, validation MSE: -0.3774
2025-05-03 06:22:47 [INFO]: Epoch 385 - training loss (MAE): 0.3390, validation MSE: -0.3588
2025-05-03 06:23:02 [INFO]: Epoch 386 - training loss (MAE): 0.3396, validation MSE: -0.2819
2025-05-03 06:23:17 [INFO]: Epoch 387 - training loss (MAE): 0.3395, validation MSE: -0.3209
2025-05-03 06:23:32 [INFO]: Epoch 388 - training loss (MAE): 0.3397, validation MSE: -0.3108
2025-05-03 06:23:47 [INFO]: Epoch 389 - training loss (MAE): 0.3396, validation MSE: -0.3592
2025-05-03 06:24:02 [INFO]: Epoch 390 - training loss (MAE): 0.3389, validation MSE: -0.3677
2025-05-03 06:24:17 [INFO]: Epoch 391 - training loss (MAE): 0.3395, validation MSE: -0.3526
2025-05-03 06:24:33 [INFO]: Epoch 392 - training loss (MAE): 0.3401, validation MSE: -0.3501
2025-05-03 06:24:48 [INFO]: Epoch 393 - training loss (MAE): 0.3391, validation MSE: -0.3268
2025-05-03 06:25:03 [INFO]: Epoch 394 - training loss (MAE): 0.3395, validation MSE: -0.3474
2025-05-03 06:25:18 [INFO]: Epoch 395 - training loss (MAE): 0.3414, validation MSE: -0.3141
2025-05-03 06:25:33 [INFO]: Epoch 396 - training loss (MAE): 0.3397, validation MSE: -0.3332
2025-05-03 06:25:48 [INFO]: Epoch 397 - training loss (MAE): 0.3387, validation MSE: -0.3275
2025-05-03 06:26:03 [INFO]: Epoch 398 - training loss (MAE): 0.3396, validation MSE: -0.3073
2025-05-03 06:26:18 [INFO]: Epoch 399 - training loss (MAE): 0.3397, validation MSE: -0.2832
2025-05-03 06:26:33 [INFO]: Epoch 400 - training loss (MAE): 0.3395, validation MSE: -0.3856
2025-05-03 06:26:48 [INFO]: Epoch 401 - training loss (MAE): 0.3395, validation MSE: -0.3743
2025-05-03 06:27:04 [INFO]: Epoch 402 - training loss (MAE): 0.3394, validation MSE: -0.3900
2025-05-03 06:27:19 [INFO]: Epoch 403 - training loss (MAE): 0.3393, validation MSE: -0.3898
2025-05-03 06:27:34 [INFO]: Epoch 404 - training loss (MAE): 0.3396, validation MSE: -0.3522
2025-05-03 06:27:49 [INFO]: Epoch 405 - training loss (MAE): 0.3394, validation MSE: -0.3650
2025-05-03 06:28:04 [INFO]: Epoch 406 - training loss (MAE): 0.3388, validation MSE: -0.3693
2025-05-03 06:28:19 [INFO]: Epoch 407 - training loss (MAE): 0.3389, validation MSE: -0.3432
2025-05-03 06:28:34 [INFO]: Epoch 408 - training loss (MAE): 0.3392, validation MSE: -0.3531
2025-05-03 06:28:50 [INFO]: Epoch 409 - training loss (MAE): 0.3391, validation MSE: -0.3372
2025-05-03 06:29:05 [INFO]: Epoch 410 - training loss (MAE): 0.3390, validation MSE: -0.3467
2025-05-03 06:29:20 [INFO]: Epoch 411 - training loss (MAE): 0.3394, validation MSE: -0.3535
2025-05-03 06:29:35 [INFO]: Epoch 412 - training loss (MAE): 0.3390, validation MSE: -0.3729
2025-05-03 06:29:50 [INFO]: Epoch 413 - training loss (MAE): 0.3387, validation MSE: -0.3653
2025-05-03 06:30:06 [INFO]: Epoch 414 - training loss (MAE): 0.3386, validation MSE: -0.3489
2025-05-03 06:30:21 [INFO]: Epoch 415 - training loss (MAE): 0.3390, validation MSE: -0.3783
2025-05-03 06:30:36 [INFO]: Epoch 416 - training loss (MAE): 0.3395, validation MSE: -0.3670
2025-05-03 06:30:51 [INFO]: Epoch 417 - training loss (MAE): 0.3389, validation MSE: -0.3620
2025-05-03 06:31:06 [INFO]: Epoch 418 - training loss (MAE): 0.3391, validation MSE: -0.3707
2025-05-03 06:31:21 [INFO]: Epoch 419 - training loss (MAE): 0.3391, validation MSE: -0.3541
2025-05-03 06:31:37 [INFO]: Epoch 420 - training loss (MAE): 0.3391, validation MSE: -0.3357
2025-05-03 06:31:52 [INFO]: Epoch 421 - training loss (MAE): 0.3383, validation MSE: -0.3361
2025-05-03 06:32:07 [INFO]: Epoch 422 - training loss (MAE): 0.3390, validation MSE: -0.3586
2025-05-03 06:32:22 [INFO]: Epoch 423 - training loss (MAE): 0.3384, validation MSE: -0.3599
2025-05-03 06:32:38 [INFO]: Epoch 424 - training loss (MAE): 0.3388, validation MSE: -0.3486
2025-05-03 06:32:52 [INFO]: Epoch 425 - training loss (MAE): 0.3390, validation MSE: -0.3503
2025-05-03 06:33:08 [INFO]: Epoch 426 - training loss (MAE): 0.3393, validation MSE: -0.3621
2025-05-03 06:33:23 [INFO]: Epoch 427 - training loss (MAE): 0.3389, validation MSE: -0.3420
2025-05-03 06:33:38 [INFO]: Epoch 428 - training loss (MAE): 0.3396, validation MSE: -0.2948
2025-05-03 06:33:53 [INFO]: Epoch 429 - training loss (MAE): 0.3394, validation MSE: -0.3557
2025-05-03 06:34:08 [INFO]: Epoch 430 - training loss (MAE): 0.3390, validation MSE: -0.3616
2025-05-03 06:34:24 [INFO]: Epoch 431 - training loss (MAE): 0.3384, validation MSE: -0.3392
2025-05-03 06:34:39 [INFO]: Epoch 432 - training loss (MAE): 0.3386, validation MSE: -0.3840
2025-05-03 06:34:54 [INFO]: Epoch 433 - training loss (MAE): 0.3386, validation MSE: -0.3263
2025-05-03 06:35:09 [INFO]: Epoch 434 - training loss (MAE): 0.3389, validation MSE: -0.3577
2025-05-03 06:35:24 [INFO]: Epoch 435 - training loss (MAE): 0.3384, validation MSE: -0.3496
2025-05-03 06:35:40 [INFO]: Epoch 436 - training loss (MAE): 0.3385, validation MSE: -0.3771
2025-05-03 06:35:55 [INFO]: Epoch 437 - training loss (MAE): 0.3386, validation MSE: -0.2724
2025-05-03 06:36:10 [INFO]: Epoch 438 - training loss (MAE): 0.3388, validation MSE: -0.3879
2025-05-03 06:36:25 [INFO]: Epoch 439 - training loss (MAE): 0.3390, validation MSE: -0.3236
2025-05-03 06:36:40 [INFO]: Epoch 440 - training loss (MAE): 0.3403, validation MSE: -0.3389
2025-05-03 06:36:55 [INFO]: Epoch 441 - training loss (MAE): 0.3385, validation MSE: -0.3858
2025-05-03 06:37:10 [INFO]: Epoch 442 - training loss (MAE): 0.3384, validation MSE: -0.3362
2025-05-03 06:37:25 [INFO]: Epoch 443 - training loss (MAE): 0.3384, validation MSE: -0.3677
2025-05-03 06:37:41 [INFO]: Epoch 444 - training loss (MAE): 0.3385, validation MSE: -0.3571
2025-05-03 06:37:56 [INFO]: Epoch 445 - training loss (MAE): 0.3391, validation MSE: -0.3797
2025-05-03 06:38:11 [INFO]: Epoch 446 - training loss (MAE): 0.3393, validation MSE: -0.3823
2025-05-03 06:38:26 [INFO]: Epoch 447 - training loss (MAE): 0.3386, validation MSE: -0.3769
2025-05-03 06:38:41 [INFO]: Epoch 448 - training loss (MAE): 0.3384, validation MSE: -0.3698
2025-05-03 06:38:56 [INFO]: Epoch 449 - training loss (MAE): 0.3386, validation MSE: -0.3525
2025-05-03 06:39:12 [INFO]: Epoch 450 - training loss (MAE): 0.3382, validation MSE: -0.3684
2025-05-03 06:39:27 [INFO]: Epoch 451 - training loss (MAE): 0.3381, validation MSE: -0.3788
2025-05-03 06:39:42 [INFO]: Epoch 452 - training loss (MAE): 0.3378, validation MSE: -0.3907
2025-05-03 06:39:57 [INFO]: Epoch 453 - training loss (MAE): 0.3388, validation MSE: -0.3546
2025-05-03 06:40:12 [INFO]: Epoch 454 - training loss (MAE): 0.3388, validation MSE: -0.3802
2025-05-03 06:40:27 [INFO]: Epoch 455 - training loss (MAE): 0.3383, validation MSE: -0.3442
2025-05-03 06:40:42 [INFO]: Epoch 456 - training loss (MAE): 0.3383, validation MSE: -0.3381
2025-05-03 06:40:57 [INFO]: Epoch 457 - training loss (MAE): 0.3379, validation MSE: -0.3199
2025-05-03 06:41:12 [INFO]: Epoch 458 - training loss (MAE): 0.3386, validation MSE: -0.3567
2025-05-03 06:41:27 [INFO]: Epoch 459 - training loss (MAE): 0.3385, validation MSE: -0.3543
2025-05-03 06:41:43 [INFO]: Epoch 460 - training loss (MAE): 0.3385, validation MSE: -0.3492
2025-05-03 06:41:57 [INFO]: Epoch 461 - training loss (MAE): 0.3381, validation MSE: -0.3611
2025-05-03 06:42:12 [INFO]: Epoch 462 - training loss (MAE): 0.3381, validation MSE: -0.3645
2025-05-03 06:42:27 [INFO]: Epoch 463 - training loss (MAE): 0.3389, validation MSE: -0.3750
2025-05-03 06:42:43 [INFO]: Epoch 464 - training loss (MAE): 0.3385, validation MSE: -0.3022
2025-05-03 06:42:58 [INFO]: Epoch 465 - training loss (MAE): 0.3378, validation MSE: -0.3519
2025-05-03 06:43:13 [INFO]: Epoch 466 - training loss (MAE): 0.3390, validation MSE: -0.3099
2025-05-03 06:43:28 [INFO]: Epoch 467 - training loss (MAE): 0.3384, validation MSE: -0.3383
2025-05-03 06:43:43 [INFO]: Epoch 468 - training loss (MAE): 0.3383, validation MSE: -0.3618
2025-05-03 06:43:58 [INFO]: Epoch 469 - training loss (MAE): 0.3377, validation MSE: -0.3746
2025-05-03 06:44:13 [INFO]: Epoch 470 - training loss (MAE): 0.3383, validation MSE: -0.3651
2025-05-03 06:44:28 [INFO]: Epoch 471 - training loss (MAE): 0.3381, validation MSE: -0.3640
2025-05-03 06:44:43 [INFO]: Epoch 472 - training loss (MAE): 0.3381, validation MSE: -0.3067
2025-05-03 06:44:58 [INFO]: Epoch 473 - training loss (MAE): 0.3377, validation MSE: -0.3613
2025-05-03 06:45:13 [INFO]: Epoch 474 - training loss (MAE): 0.3385, validation MSE: -0.3255
2025-05-03 06:45:29 [INFO]: Epoch 475 - training loss (MAE): 0.3386, validation MSE: -0.3693
2025-05-03 06:45:44 [INFO]: Epoch 476 - training loss (MAE): 0.3381, validation MSE: -0.3578
2025-05-03 06:45:59 [INFO]: Epoch 477 - training loss (MAE): 0.3377, validation MSE: -0.3599
2025-05-03 06:46:14 [INFO]: Epoch 478 - training loss (MAE): 0.3375, validation MSE: -0.3296
2025-05-03 06:46:29 [INFO]: Epoch 479 - training loss (MAE): 0.3378, validation MSE: -0.3452
2025-05-03 06:46:44 [INFO]: Epoch 480 - training loss (MAE): 0.3381, validation MSE: -0.3222
2025-05-03 06:47:00 [INFO]: Epoch 481 - training loss (MAE): 0.3384, validation MSE: -0.3411
2025-05-03 06:47:15 [INFO]: Epoch 482 - training loss (MAE): 0.3382, validation MSE: -0.3083
2025-05-03 06:47:30 [INFO]: Epoch 483 - training loss (MAE): 0.3375, validation MSE: -0.3319
2025-05-03 06:47:45 [INFO]: Epoch 484 - training loss (MAE): 0.3384, validation MSE: -0.3151
2025-05-03 06:48:00 [INFO]: Epoch 485 - training loss (MAE): 0.3378, validation MSE: -0.3575
2025-05-03 06:48:15 [INFO]: Epoch 486 - training loss (MAE): 0.3379, validation MSE: -0.3630
2025-05-03 06:48:30 [INFO]: Epoch 487 - training loss (MAE): 0.3376, validation MSE: -0.3730
2025-05-03 06:48:45 [INFO]: Epoch 488 - training loss (MAE): 0.3384, validation MSE: -0.3684
2025-05-03 06:49:00 [INFO]: Epoch 489 - training loss (MAE): 0.3374, validation MSE: -0.3743
2025-05-03 06:49:15 [INFO]: Epoch 490 - training loss (MAE): 0.3385, validation MSE: -0.3114
2025-05-03 06:49:31 [INFO]: Epoch 491 - training loss (MAE): 0.3378, validation MSE: -0.3533
2025-05-03 06:49:46 [INFO]: Epoch 492 - training loss (MAE): 0.3375, validation MSE: -0.3566
2025-05-03 06:50:01 [INFO]: Epoch 493 - training loss (MAE): 0.3379, validation MSE: -0.2992
2025-05-03 06:50:16 [INFO]: Epoch 494 - training loss (MAE): 0.3382, validation MSE: -0.3234
2025-05-03 06:50:31 [INFO]: Epoch 495 - training loss (MAE): 0.3377, validation MSE: -0.3201
2025-05-03 06:50:46 [INFO]: Epoch 496 - training loss (MAE): 0.3376, validation MSE: -0.3641
2025-05-03 06:51:00 [INFO]: Epoch 497 - training loss (MAE): 0.3376, validation MSE: -0.3756
2025-05-03 06:51:16 [INFO]: Epoch 498 - training loss (MAE): 0.3378, validation MSE: -0.3807
2025-05-03 06:51:31 [INFO]: Epoch 499 - training loss (MAE): 0.3372, validation MSE: -0.3968
2025-05-03 06:51:46 [INFO]: Epoch 500 - training loss (MAE): 0.3379, validation MSE: -0.3876
2025-05-03 06:51:46 [INFO]: Finished training. The best model is from epoch#499.
2025-05-03 06:51:46 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_7.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 06:51:46 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_7.pypots
Fold 7 metrics: MAE: 0.303, MSE: 0.332, MRE: 0.434
Fold 7 metrics: MAE: 0.303, MSE: 0.332, MRE: 0.434
Training fold 8/10
2025-05-03 06:51:46 [INFO]: No given device, using default device: cuda
2025-05-03 06:51:46 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-03 06:51:46 [INFO]: Using customized MAE as the training loss function.
2025-05-03 06:51:46 [INFO]: Using customized MSE as the validation metric function.
2025-05-03 06:51:46 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-03 06:52:02 [INFO]: Epoch 001 - training loss (MAE): 1.0103, validation MSE: -0.0447
2025-05-03 06:52:17 [INFO]: Epoch 002 - training loss (MAE): 0.4721, validation MSE: -0.1115
2025-05-03 06:52:32 [INFO]: Epoch 003 - training loss (MAE): 0.4026, validation MSE: -0.1667
2025-05-03 06:52:48 [INFO]: Epoch 004 - training loss (MAE): 0.3638, validation MSE: -0.2083
2025-05-03 06:53:03 [INFO]: Epoch 005 - training loss (MAE): 0.3304, validation MSE: -0.2423
2025-05-03 06:53:18 [INFO]: Epoch 006 - training loss (MAE): 0.3014, validation MSE: -0.2907
2025-05-03 06:53:33 [INFO]: Epoch 007 - training loss (MAE): 0.2738, validation MSE: -0.3356
2025-05-03 06:53:48 [INFO]: Epoch 008 - training loss (MAE): 0.2492, validation MSE: -0.3917
2025-05-03 06:54:03 [INFO]: Epoch 009 - training loss (MAE): 0.2273, validation MSE: -0.4257
2025-05-03 06:54:19 [INFO]: Epoch 010 - training loss (MAE): 0.2106, validation MSE: -0.4288
2025-05-03 06:54:34 [INFO]: Epoch 011 - training loss (MAE): 0.1971, validation MSE: -0.4402
2025-05-03 06:54:49 [INFO]: Epoch 012 - training loss (MAE): 0.1861, validation MSE: -0.4722
2025-05-03 06:55:04 [INFO]: Epoch 013 - training loss (MAE): 0.1777, validation MSE: -0.4442
2025-05-03 06:55:19 [INFO]: Epoch 014 - training loss (MAE): 0.1685, validation MSE: -0.4479
2025-05-03 06:55:34 [INFO]: Epoch 015 - training loss (MAE): 0.1626, validation MSE: -0.4558
2025-05-03 06:55:50 [INFO]: Epoch 016 - training loss (MAE): 0.1556, validation MSE: -0.4715
2025-05-03 06:56:06 [INFO]: Epoch 017 - training loss (MAE): 0.1500, validation MSE: -0.4583
2025-05-03 06:56:21 [INFO]: Epoch 018 - training loss (MAE): 0.1452, validation MSE: -0.4638
2025-05-03 06:56:36 [INFO]: Epoch 019 - training loss (MAE): 0.1401, validation MSE: -0.5014
2025-05-03 06:56:51 [INFO]: Epoch 020 - training loss (MAE): 0.1386, validation MSE: -0.4767
2025-05-03 06:57:07 [INFO]: Epoch 021 - training loss (MAE): 0.1329, validation MSE: -0.4887
2025-05-03 06:57:22 [INFO]: Epoch 022 - training loss (MAE): 0.1293, validation MSE: -0.4813
2025-05-03 06:57:37 [INFO]: Epoch 023 - training loss (MAE): 0.1258, validation MSE: -0.4787
2025-05-03 06:57:53 [INFO]: Epoch 024 - training loss (MAE): 0.1222, validation MSE: -0.4846
2025-05-03 06:58:08 [INFO]: Epoch 025 - training loss (MAE): 0.1201, validation MSE: -0.4829
2025-05-03 06:58:23 [INFO]: Epoch 026 - training loss (MAE): 0.1165, validation MSE: -0.5194
2025-05-03 06:58:38 [INFO]: Epoch 027 - training loss (MAE): 0.1140, validation MSE: -0.4913
2025-05-03 06:58:53 [INFO]: Epoch 028 - training loss (MAE): 0.1122, validation MSE: -0.4863
2025-05-03 06:59:08 [INFO]: Epoch 029 - training loss (MAE): 0.1093, validation MSE: -0.4946
2025-05-03 06:59:24 [INFO]: Epoch 030 - training loss (MAE): 0.1083, validation MSE: -0.5113
2025-05-03 06:59:39 [INFO]: Epoch 031 - training loss (MAE): 0.1046, validation MSE: -0.4930
2025-05-03 06:59:54 [INFO]: Epoch 032 - training loss (MAE): 0.1034, validation MSE: -0.5071
2025-05-03 07:00:10 [INFO]: Epoch 033 - training loss (MAE): 0.1010, validation MSE: -0.5197
2025-05-03 07:00:25 [INFO]: Epoch 034 - training loss (MAE): 0.0990, validation MSE: -0.5082
2025-05-03 07:00:40 [INFO]: Epoch 035 - training loss (MAE): 0.0980, validation MSE: -0.5145
2025-05-03 07:00:55 [INFO]: Epoch 036 - training loss (MAE): 0.0961, validation MSE: -0.5184
2025-05-03 07:01:11 [INFO]: Epoch 037 - training loss (MAE): 0.0958, validation MSE: -0.5158
2025-05-03 07:01:26 [INFO]: Epoch 038 - training loss (MAE): 0.0926, validation MSE: -0.5153
2025-05-03 07:01:41 [INFO]: Epoch 039 - training loss (MAE): 0.0926, validation MSE: -0.5025
2025-05-03 07:01:56 [INFO]: Epoch 040 - training loss (MAE): 0.0911, validation MSE: -0.5132
2025-05-03 07:02:11 [INFO]: Epoch 041 - training loss (MAE): 0.0893, validation MSE: -0.4697
2025-05-03 07:02:27 [INFO]: Epoch 042 - training loss (MAE): 0.0886, validation MSE: -0.5160
2025-05-03 07:02:42 [INFO]: Epoch 043 - training loss (MAE): 0.0888, validation MSE: -0.5298
2025-05-03 07:02:57 [INFO]: Epoch 044 - training loss (MAE): 0.0863, validation MSE: -0.5301
2025-05-03 07:03:12 [INFO]: Epoch 045 - training loss (MAE): 0.0862, validation MSE: -0.5228
2025-05-03 07:03:27 [INFO]: Epoch 046 - training loss (MAE): 0.0854, validation MSE: -0.5074
2025-05-03 07:03:42 [INFO]: Epoch 047 - training loss (MAE): 0.0846, validation MSE: -0.5150
2025-05-03 07:03:58 [INFO]: Epoch 048 - training loss (MAE): 0.0849, validation MSE: -0.5212
2025-05-03 07:04:13 [INFO]: Epoch 049 - training loss (MAE): 0.0826, validation MSE: -0.5185
2025-05-03 07:04:28 [INFO]: Epoch 050 - training loss (MAE): 0.0824, validation MSE: -0.5236
2025-05-03 07:04:43 [INFO]: Epoch 051 - training loss (MAE): 0.0819, validation MSE: -0.5134
2025-05-03 07:04:58 [INFO]: Epoch 052 - training loss (MAE): 0.0819, validation MSE: -0.5052
2025-05-03 07:05:14 [INFO]: Epoch 053 - training loss (MAE): 0.0804, validation MSE: -0.5103
2025-05-03 07:05:29 [INFO]: Epoch 054 - training loss (MAE): 0.0805, validation MSE: -0.5354
2025-05-03 07:05:44 [INFO]: Epoch 055 - training loss (MAE): 0.0798, validation MSE: -0.5046
2025-05-03 07:05:59 [INFO]: Epoch 056 - training loss (MAE): 0.0793, validation MSE: -0.5111
2025-05-03 07:06:14 [INFO]: Epoch 057 - training loss (MAE): 0.0790, validation MSE: -0.5336
2025-05-03 07:06:30 [INFO]: Epoch 058 - training loss (MAE): 0.0783, validation MSE: -0.5285
2025-05-03 07:06:45 [INFO]: Epoch 059 - training loss (MAE): 0.0781, validation MSE: -0.5190
2025-05-03 07:07:00 [INFO]: Epoch 060 - training loss (MAE): 0.0771, validation MSE: -0.5318
2025-05-03 07:07:15 [INFO]: Epoch 061 - training loss (MAE): 0.0768, validation MSE: -0.5427
2025-05-03 07:07:31 [INFO]: Epoch 062 - training loss (MAE): 0.0771, validation MSE: -0.5228
2025-05-03 07:07:46 [INFO]: Epoch 063 - training loss (MAE): 0.0767, validation MSE: -0.5334
2025-05-03 07:08:01 [INFO]: Epoch 064 - training loss (MAE): 0.0764, validation MSE: -0.5306
2025-05-03 07:08:16 [INFO]: Epoch 065 - training loss (MAE): 0.0757, validation MSE: -0.5268
2025-05-03 07:08:31 [INFO]: Epoch 066 - training loss (MAE): 0.0754, validation MSE: -0.5138
2025-05-03 07:08:46 [INFO]: Epoch 067 - training loss (MAE): 0.0741, validation MSE: -0.5134
2025-05-03 07:09:02 [INFO]: Epoch 068 - training loss (MAE): 0.0746, validation MSE: -0.5232
2025-05-03 07:09:17 [INFO]: Epoch 069 - training loss (MAE): 0.0748, validation MSE: -0.5446
2025-05-03 07:09:32 [INFO]: Epoch 070 - training loss (MAE): 0.0736, validation MSE: -0.5283
2025-05-03 07:09:47 [INFO]: Epoch 071 - training loss (MAE): 0.0736, validation MSE: -0.5366
2025-05-03 07:10:02 [INFO]: Epoch 072 - training loss (MAE): 0.0738, validation MSE: -0.5421
2025-05-03 07:10:17 [INFO]: Epoch 073 - training loss (MAE): 0.0731, validation MSE: -0.5331
2025-05-03 07:10:33 [INFO]: Epoch 074 - training loss (MAE): 0.0733, validation MSE: -0.5277
2025-05-03 07:10:48 [INFO]: Epoch 075 - training loss (MAE): 0.0726, validation MSE: -0.5388
2025-05-03 07:11:03 [INFO]: Epoch 076 - training loss (MAE): 0.0721, validation MSE: -0.5241
2025-05-03 07:11:18 [INFO]: Epoch 077 - training loss (MAE): 0.0727, validation MSE: -0.5383
2025-05-03 07:11:33 [INFO]: Epoch 078 - training loss (MAE): 0.0720, validation MSE: -0.5507
2025-05-03 07:11:49 [INFO]: Epoch 079 - training loss (MAE): 0.0720, validation MSE: -0.5511
2025-05-03 07:12:04 [INFO]: Epoch 080 - training loss (MAE): 0.0715, validation MSE: -0.5157
2025-05-03 07:12:19 [INFO]: Epoch 081 - training loss (MAE): 0.0711, validation MSE: -0.5406
2025-05-03 07:12:34 [INFO]: Epoch 082 - training loss (MAE): 0.0709, validation MSE: -0.5202
2025-05-03 07:12:49 [INFO]: Epoch 083 - training loss (MAE): 0.0710, validation MSE: -0.5313
2025-05-03 07:13:04 [INFO]: Epoch 084 - training loss (MAE): 0.0704, validation MSE: -0.5312
2025-05-03 07:13:19 [INFO]: Epoch 085 - training loss (MAE): 0.0710, validation MSE: -0.5184
2025-05-03 07:13:34 [INFO]: Epoch 086 - training loss (MAE): 0.0700, validation MSE: -0.5660
2025-05-03 07:13:50 [INFO]: Epoch 087 - training loss (MAE): 0.0698, validation MSE: -0.5286
2025-05-03 07:14:05 [INFO]: Epoch 088 - training loss (MAE): 0.0702, validation MSE: -0.5380
2025-05-03 07:14:20 [INFO]: Epoch 089 - training loss (MAE): 0.0698, validation MSE: -0.5461
2025-05-03 07:14:36 [INFO]: Epoch 090 - training loss (MAE): 0.0700, validation MSE: -0.5532
2025-05-03 07:14:51 [INFO]: Epoch 091 - training loss (MAE): 0.0693, validation MSE: -0.5394
2025-05-03 07:15:06 [INFO]: Epoch 092 - training loss (MAE): 0.0695, validation MSE: -0.5331
2025-05-03 07:15:21 [INFO]: Epoch 093 - training loss (MAE): 0.0700, validation MSE: -0.5296
2025-05-03 07:15:37 [INFO]: Epoch 094 - training loss (MAE): 0.0689, validation MSE: -0.5431
2025-05-03 07:15:52 [INFO]: Epoch 095 - training loss (MAE): 0.0684, validation MSE: -0.5605
2025-05-03 07:16:07 [INFO]: Epoch 096 - training loss (MAE): 0.0686, validation MSE: -0.5399
2025-05-03 07:16:22 [INFO]: Epoch 097 - training loss (MAE): 0.0681, validation MSE: -0.5551
2025-05-03 07:16:37 [INFO]: Epoch 098 - training loss (MAE): 0.0691, validation MSE: -0.5023
2025-05-03 07:16:52 [INFO]: Epoch 099 - training loss (MAE): 0.0680, validation MSE: -0.5184
2025-05-03 07:17:07 [INFO]: Epoch 100 - training loss (MAE): 0.0680, validation MSE: -0.5151
2025-05-03 07:17:22 [INFO]: Epoch 101 - training loss (MAE): 0.0677, validation MSE: -0.5168
2025-05-03 07:17:37 [INFO]: Epoch 102 - training loss (MAE): 0.0680, validation MSE: -0.5242
2025-05-03 07:17:53 [INFO]: Epoch 103 - training loss (MAE): 0.0683, validation MSE: -0.5351
2025-05-03 07:18:08 [INFO]: Epoch 104 - training loss (MAE): 0.0674, validation MSE: -0.5232
2025-05-03 07:18:23 [INFO]: Epoch 105 - training loss (MAE): 0.0680, validation MSE: -0.5413
2025-05-03 07:18:38 [INFO]: Epoch 106 - training loss (MAE): 0.0665, validation MSE: -0.5280
2025-05-03 07:18:53 [INFO]: Epoch 107 - training loss (MAE): 0.0671, validation MSE: -0.5284
2025-05-03 07:19:09 [INFO]: Epoch 108 - training loss (MAE): 0.0674, validation MSE: -0.5062
2025-05-03 07:19:24 [INFO]: Epoch 109 - training loss (MAE): 0.0670, validation MSE: -0.5483
2025-05-03 07:19:39 [INFO]: Epoch 110 - training loss (MAE): 0.0666, validation MSE: -0.5448
2025-05-03 07:19:54 [INFO]: Epoch 111 - training loss (MAE): 0.0671, validation MSE: -0.5192
2025-05-03 07:20:09 [INFO]: Epoch 112 - training loss (MAE): 0.0666, validation MSE: -0.5255
2025-05-03 07:20:25 [INFO]: Epoch 113 - training loss (MAE): 0.0679, validation MSE: -0.4694
2025-05-03 07:20:40 [INFO]: Epoch 114 - training loss (MAE): 0.0659, validation MSE: -0.5417
2025-05-03 07:20:55 [INFO]: Epoch 115 - training loss (MAE): 0.0664, validation MSE: -0.4517
2025-05-03 07:21:10 [INFO]: Epoch 116 - training loss (MAE): 0.0664, validation MSE: -0.5180
2025-05-03 07:21:25 [INFO]: Epoch 117 - training loss (MAE): 0.0659, validation MSE: -0.5509
2025-05-03 07:21:41 [INFO]: Epoch 118 - training loss (MAE): 0.0661, validation MSE: -0.5262
2025-05-03 07:21:56 [INFO]: Epoch 119 - training loss (MAE): 0.0658, validation MSE: -0.5299
2025-05-03 07:22:11 [INFO]: Epoch 120 - training loss (MAE): 0.0657, validation MSE: -0.5395
2025-05-03 07:22:26 [INFO]: Epoch 121 - training loss (MAE): 0.0659, validation MSE: -0.5416
2025-05-03 07:22:41 [INFO]: Epoch 122 - training loss (MAE): 0.0652, validation MSE: -0.5438
2025-05-03 07:22:57 [INFO]: Epoch 123 - training loss (MAE): 0.0661, validation MSE: -0.5258
2025-05-03 07:23:12 [INFO]: Epoch 124 - training loss (MAE): 0.0655, validation MSE: -0.4992
2025-05-03 07:23:27 [INFO]: Epoch 125 - training loss (MAE): 0.0653, validation MSE: -0.5617
2025-05-03 07:23:42 [INFO]: Epoch 126 - training loss (MAE): 0.0652, validation MSE: -0.5415
2025-05-03 07:23:57 [INFO]: Epoch 127 - training loss (MAE): 0.0650, validation MSE: -0.5535
2025-05-03 07:24:12 [INFO]: Epoch 128 - training loss (MAE): 0.0650, validation MSE: -0.5479
2025-05-03 07:24:27 [INFO]: Epoch 129 - training loss (MAE): 0.0661, validation MSE: -0.5523
2025-05-03 07:24:42 [INFO]: Epoch 130 - training loss (MAE): 0.0645, validation MSE: -0.5220
2025-05-03 07:24:58 [INFO]: Epoch 131 - training loss (MAE): 0.0649, validation MSE: -0.5229
2025-05-03 07:25:13 [INFO]: Epoch 132 - training loss (MAE): 0.0642, validation MSE: -0.5473
2025-05-03 07:25:28 [INFO]: Epoch 133 - training loss (MAE): 0.0641, validation MSE: -0.5419
2025-05-03 07:25:43 [INFO]: Epoch 134 - training loss (MAE): 0.0647, validation MSE: -0.5382
2025-05-03 07:25:58 [INFO]: Epoch 135 - training loss (MAE): 0.0644, validation MSE: -0.5308
2025-05-03 07:26:13 [INFO]: Epoch 136 - training loss (MAE): 0.0641, validation MSE: -0.5469
2025-05-03 07:26:28 [INFO]: Epoch 137 - training loss (MAE): 0.0641, validation MSE: -0.5343
2025-05-03 07:26:44 [INFO]: Epoch 138 - training loss (MAE): 0.0642, validation MSE: -0.5179
2025-05-03 07:26:59 [INFO]: Epoch 139 - training loss (MAE): 0.0640, validation MSE: -0.5376
2025-05-03 07:27:14 [INFO]: Epoch 140 - training loss (MAE): 0.0639, validation MSE: -0.5390
2025-05-03 07:27:29 [INFO]: Epoch 141 - training loss (MAE): 0.0640, validation MSE: -0.5372
2025-05-03 07:27:44 [INFO]: Epoch 142 - training loss (MAE): 0.0655, validation MSE: -0.5185
2025-05-03 07:27:59 [INFO]: Epoch 143 - training loss (MAE): 0.0638, validation MSE: -0.5348
2025-05-03 07:28:14 [INFO]: Epoch 144 - training loss (MAE): 0.0643, validation MSE: -0.5533
2025-05-03 07:28:30 [INFO]: Epoch 145 - training loss (MAE): 0.0634, validation MSE: -0.4913
2025-05-03 07:28:45 [INFO]: Epoch 146 - training loss (MAE): 0.0636, validation MSE: -0.4985
2025-05-03 07:29:00 [INFO]: Epoch 147 - training loss (MAE): 0.0635, validation MSE: -0.4977
2025-05-03 07:29:16 [INFO]: Epoch 148 - training loss (MAE): 0.0632, validation MSE: -0.5195
2025-05-03 07:29:31 [INFO]: Epoch 149 - training loss (MAE): 0.0633, validation MSE: -0.5617
2025-05-03 07:29:46 [INFO]: Epoch 150 - training loss (MAE): 0.0633, validation MSE: -0.5348
2025-05-03 07:30:01 [INFO]: Epoch 151 - training loss (MAE): 0.0633, validation MSE: -0.4965
2025-05-03 07:30:17 [INFO]: Epoch 152 - training loss (MAE): 0.0630, validation MSE: -0.3876
2025-05-03 07:30:32 [INFO]: Epoch 153 - training loss (MAE): 0.0631, validation MSE: -0.5396
2025-05-03 07:30:47 [INFO]: Epoch 154 - training loss (MAE): 0.0630, validation MSE: -0.4794
2025-05-03 07:31:02 [INFO]: Epoch 155 - training loss (MAE): 0.0625, validation MSE: -0.5083
2025-05-03 07:31:17 [INFO]: Epoch 156 - training loss (MAE): 0.0633, validation MSE: -0.5245
2025-05-03 07:31:33 [INFO]: Epoch 157 - training loss (MAE): 0.0629, validation MSE: -0.5399
2025-05-03 07:31:48 [INFO]: Epoch 158 - training loss (MAE): 0.0627, validation MSE: -0.5309
2025-05-03 07:32:03 [INFO]: Epoch 159 - training loss (MAE): 0.0632, validation MSE: -0.5342
2025-05-03 07:32:18 [INFO]: Epoch 160 - training loss (MAE): 0.0627, validation MSE: -0.5322
2025-05-03 07:32:33 [INFO]: Epoch 161 - training loss (MAE): 0.0623, validation MSE: -0.5227
2025-05-03 07:32:49 [INFO]: Epoch 162 - training loss (MAE): 0.0627, validation MSE: -0.5526
2025-05-03 07:33:04 [INFO]: Epoch 163 - training loss (MAE): 0.0622, validation MSE: -0.5295
2025-05-03 07:33:19 [INFO]: Epoch 164 - training loss (MAE): 0.0620, validation MSE: -0.4820
2025-05-03 07:33:34 [INFO]: Epoch 165 - training loss (MAE): 0.0627, validation MSE: -0.5187
2025-05-03 07:33:49 [INFO]: Epoch 166 - training loss (MAE): 0.0622, validation MSE: -0.5196
2025-05-03 07:34:04 [INFO]: Epoch 167 - training loss (MAE): 0.0621, validation MSE: -0.5097
2025-05-03 07:34:19 [INFO]: Epoch 168 - training loss (MAE): 0.0622, validation MSE: -0.5242
2025-05-03 07:34:35 [INFO]: Epoch 169 - training loss (MAE): 0.0623, validation MSE: -0.5146
2025-05-03 07:34:50 [INFO]: Epoch 170 - training loss (MAE): 0.0626, validation MSE: -0.4616
2025-05-03 07:35:05 [INFO]: Epoch 171 - training loss (MAE): 0.0620, validation MSE: -0.5287
2025-05-03 07:35:20 [INFO]: Epoch 172 - training loss (MAE): 0.0619, validation MSE: -0.5520
2025-05-03 07:35:36 [INFO]: Epoch 173 - training loss (MAE): 0.0620, validation MSE: -0.5406
2025-05-03 07:35:51 [INFO]: Epoch 174 - training loss (MAE): 0.0619, validation MSE: -0.4609
2025-05-03 07:36:06 [INFO]: Epoch 175 - training loss (MAE): 0.0621, validation MSE: -0.5348
2025-05-03 07:36:22 [INFO]: Epoch 176 - training loss (MAE): 0.0618, validation MSE: -0.5262
2025-05-03 07:36:37 [INFO]: Epoch 177 - training loss (MAE): 0.0614, validation MSE: -0.5260
2025-05-03 07:36:52 [INFO]: Epoch 178 - training loss (MAE): 0.0616, validation MSE: -0.4318
2025-05-03 07:37:07 [INFO]: Epoch 179 - training loss (MAE): 0.0621, validation MSE: -0.5043
2025-05-03 07:37:22 [INFO]: Epoch 180 - training loss (MAE): 0.0609, validation MSE: -0.5190
2025-05-03 07:37:37 [INFO]: Epoch 181 - training loss (MAE): 0.0607, validation MSE: -0.5083
2025-05-03 07:37:52 [INFO]: Epoch 182 - training loss (MAE): 0.0615, validation MSE: -0.5163
2025-05-03 07:38:08 [INFO]: Epoch 183 - training loss (MAE): 0.0617, validation MSE: -0.5420
2025-05-03 07:38:23 [INFO]: Epoch 184 - training loss (MAE): 0.0609, validation MSE: -0.5406
2025-05-03 07:38:38 [INFO]: Epoch 185 - training loss (MAE): 0.0616, validation MSE: -0.4113
2025-05-03 07:38:53 [INFO]: Epoch 186 - training loss (MAE): 0.0614, validation MSE: -0.5427
2025-05-03 07:39:08 [INFO]: Epoch 187 - training loss (MAE): 0.0612, validation MSE: -0.5360
2025-05-03 07:39:23 [INFO]: Epoch 188 - training loss (MAE): 0.0615, validation MSE: -0.5163
2025-05-03 07:39:38 [INFO]: Epoch 189 - training loss (MAE): 0.0612, validation MSE: -0.5374
2025-05-03 07:39:53 [INFO]: Epoch 190 - training loss (MAE): 0.0609, validation MSE: -0.5075
2025-05-03 07:40:08 [INFO]: Epoch 191 - training loss (MAE): 0.0610, validation MSE: -0.5278
2025-05-03 07:40:23 [INFO]: Epoch 192 - training loss (MAE): 0.0610, validation MSE: -0.5552
2025-05-03 07:40:39 [INFO]: Epoch 193 - training loss (MAE): 0.0611, validation MSE: -0.5422
2025-05-03 07:40:54 [INFO]: Epoch 194 - training loss (MAE): 0.0608, validation MSE: -0.5364
2025-05-03 07:41:09 [INFO]: Epoch 195 - training loss (MAE): 0.0610, validation MSE: -0.5248
2025-05-03 07:41:24 [INFO]: Epoch 196 - training loss (MAE): 0.0607, validation MSE: -0.5320
2025-05-03 07:41:39 [INFO]: Epoch 197 - training loss (MAE): 0.0600, validation MSE: -0.5181
2025-05-03 07:41:54 [INFO]: Epoch 198 - training loss (MAE): 0.0610, validation MSE: -0.4237
2025-05-03 07:42:09 [INFO]: Epoch 199 - training loss (MAE): 0.0610, validation MSE: -0.5380
2025-05-03 07:42:24 [INFO]: Epoch 200 - training loss (MAE): 0.0613, validation MSE: -0.5198
2025-05-03 07:42:39 [INFO]: Epoch 201 - training loss (MAE): 0.0608, validation MSE: -0.4876
2025-05-03 07:42:55 [INFO]: Epoch 202 - training loss (MAE): 0.0602, validation MSE: -0.5191
2025-05-03 07:43:10 [INFO]: Epoch 203 - training loss (MAE): 0.0601, validation MSE: -0.4921
2025-05-03 07:43:25 [INFO]: Epoch 204 - training loss (MAE): 0.0602, validation MSE: -0.5253
2025-05-03 07:43:40 [INFO]: Epoch 205 - training loss (MAE): 0.0599, validation MSE: -0.5009
2025-05-03 07:43:55 [INFO]: Epoch 206 - training loss (MAE): 0.0601, validation MSE: -0.5203
2025-05-03 07:44:10 [INFO]: Epoch 207 - training loss (MAE): 0.0607, validation MSE: -0.5042
2025-05-03 07:44:25 [INFO]: Epoch 208 - training loss (MAE): 0.0605, validation MSE: -0.5209
2025-05-03 07:44:40 [INFO]: Epoch 209 - training loss (MAE): 0.0601, validation MSE: -0.5263
2025-05-03 07:44:55 [INFO]: Epoch 210 - training loss (MAE): 0.0602, validation MSE: -0.5397
2025-05-03 07:45:11 [INFO]: Epoch 211 - training loss (MAE): 0.0598, validation MSE: -0.5414
2025-05-03 07:45:26 [INFO]: Epoch 212 - training loss (MAE): 0.0596, validation MSE: -0.5440
2025-05-03 07:45:41 [INFO]: Epoch 213 - training loss (MAE): 0.0600, validation MSE: -0.5295
2025-05-03 07:45:56 [INFO]: Epoch 214 - training loss (MAE): 0.0598, validation MSE: -0.5033
2025-05-03 07:46:12 [INFO]: Epoch 215 - training loss (MAE): 0.0597, validation MSE: -0.5345
2025-05-03 07:46:27 [INFO]: Epoch 216 - training loss (MAE): 0.0600, validation MSE: -0.4768
2025-05-03 07:46:42 [INFO]: Epoch 217 - training loss (MAE): 0.0593, validation MSE: -0.5284
2025-05-03 07:46:57 [INFO]: Epoch 218 - training loss (MAE): 0.0597, validation MSE: -0.5322
2025-05-03 07:47:12 [INFO]: Epoch 219 - training loss (MAE): 0.0595, validation MSE: -0.5177
2025-05-03 07:47:28 [INFO]: Epoch 220 - training loss (MAE): 0.0594, validation MSE: -0.5163
2025-05-03 07:47:43 [INFO]: Epoch 221 - training loss (MAE): 0.0594, validation MSE: -0.5448
2025-05-03 07:47:58 [INFO]: Epoch 222 - training loss (MAE): 0.0603, validation MSE: -0.4930
2025-05-03 07:48:13 [INFO]: Epoch 223 - training loss (MAE): 0.0596, validation MSE: -0.5008
2025-05-03 07:48:28 [INFO]: Epoch 224 - training loss (MAE): 0.0590, validation MSE: -0.5170
2025-05-03 07:48:43 [INFO]: Epoch 225 - training loss (MAE): 0.0593, validation MSE: -0.5305
2025-05-03 07:48:58 [INFO]: Epoch 226 - training loss (MAE): 0.0598, validation MSE: -0.5144
2025-05-03 07:49:14 [INFO]: Epoch 227 - training loss (MAE): 0.0592, validation MSE: -0.4469
2025-05-03 07:49:29 [INFO]: Epoch 228 - training loss (MAE): 0.0595, validation MSE: -0.4829
2025-05-03 07:49:44 [INFO]: Epoch 229 - training loss (MAE): 0.0596, validation MSE: -0.5462
2025-05-03 07:49:59 [INFO]: Epoch 230 - training loss (MAE): 0.0598, validation MSE: -0.5393
2025-05-03 07:50:14 [INFO]: Epoch 231 - training loss (MAE): 0.0594, validation MSE: -0.5395
2025-05-03 07:50:29 [INFO]: Epoch 232 - training loss (MAE): 0.0589, validation MSE: -0.4936
2025-05-03 07:50:45 [INFO]: Epoch 233 - training loss (MAE): 0.0594, validation MSE: -0.5240
2025-05-03 07:51:00 [INFO]: Epoch 234 - training loss (MAE): 0.0587, validation MSE: -0.5326
2025-05-03 07:51:15 [INFO]: Epoch 235 - training loss (MAE): 0.0590, validation MSE: -0.5233
2025-05-03 07:51:30 [INFO]: Epoch 236 - training loss (MAE): 0.0596, validation MSE: -0.4636
2025-05-03 07:51:45 [INFO]: Epoch 237 - training loss (MAE): 0.0588, validation MSE: -0.4985
2025-05-03 07:52:01 [INFO]: Epoch 238 - training loss (MAE): 0.0587, validation MSE: -0.5080
2025-05-03 07:52:16 [INFO]: Epoch 239 - training loss (MAE): 0.0593, validation MSE: -0.5095
2025-05-03 07:52:31 [INFO]: Epoch 240 - training loss (MAE): 0.0591, validation MSE: -0.5170
2025-05-03 07:52:46 [INFO]: Epoch 241 - training loss (MAE): 0.0585, validation MSE: -0.4849
2025-05-03 07:53:02 [INFO]: Epoch 242 - training loss (MAE): 0.0590, validation MSE: -0.5180
2025-05-03 07:53:17 [INFO]: Epoch 243 - training loss (MAE): 0.0590, validation MSE: -0.5099
2025-05-03 07:53:33 [INFO]: Epoch 244 - training loss (MAE): 0.0597, validation MSE: -0.5332
2025-05-03 07:53:48 [INFO]: Epoch 245 - training loss (MAE): 0.0587, validation MSE: -0.5380
2025-05-03 07:54:03 [INFO]: Epoch 246 - training loss (MAE): 0.0587, validation MSE: -0.4995
2025-05-03 07:54:18 [INFO]: Epoch 247 - training loss (MAE): 0.0585, validation MSE: -0.4787
2025-05-03 07:54:33 [INFO]: Epoch 248 - training loss (MAE): 0.0587, validation MSE: -0.4759
2025-05-03 07:54:48 [INFO]: Epoch 249 - training loss (MAE): 0.0590, validation MSE: -0.4828
2025-05-03 07:55:04 [INFO]: Epoch 250 - training loss (MAE): 0.0581, validation MSE: -0.4743
2025-05-03 07:55:19 [INFO]: Epoch 251 - training loss (MAE): 0.0589, validation MSE: -0.4988
2025-05-03 07:55:34 [INFO]: Epoch 252 - training loss (MAE): 0.0586, validation MSE: -0.5117
2025-05-03 07:55:49 [INFO]: Epoch 253 - training loss (MAE): 0.0594, validation MSE: -0.5224
2025-05-03 07:56:04 [INFO]: Epoch 254 - training loss (MAE): 0.0580, validation MSE: -0.5411
2025-05-03 07:56:20 [INFO]: Epoch 255 - training loss (MAE): 0.0582, validation MSE: -0.5010
2025-05-03 07:56:34 [INFO]: Epoch 256 - training loss (MAE): 0.0587, validation MSE: -0.5033
2025-05-03 07:56:50 [INFO]: Epoch 257 - training loss (MAE): 0.0580, validation MSE: -0.5265
2025-05-03 07:57:05 [INFO]: Epoch 258 - training loss (MAE): 0.0584, validation MSE: -0.5235
2025-05-03 07:57:20 [INFO]: Epoch 259 - training loss (MAE): 0.0587, validation MSE: -0.5181
2025-05-03 07:57:35 [INFO]: Epoch 260 - training loss (MAE): 0.0582, validation MSE: -0.5052
2025-05-03 07:57:51 [INFO]: Epoch 261 - training loss (MAE): 0.0584, validation MSE: -0.5167
2025-05-03 07:58:06 [INFO]: Epoch 262 - training loss (MAE): 0.0579, validation MSE: -0.5310
2025-05-03 07:58:21 [INFO]: Epoch 263 - training loss (MAE): 0.0585, validation MSE: -0.5017
2025-05-03 07:58:36 [INFO]: Epoch 264 - training loss (MAE): 0.0589, validation MSE: -0.4998
2025-05-03 07:58:51 [INFO]: Epoch 265 - training loss (MAE): 0.0577, validation MSE: -0.4817
2025-05-03 07:59:06 [INFO]: Epoch 266 - training loss (MAE): 0.0593, validation MSE: -0.4541
2025-05-03 07:59:22 [INFO]: Epoch 267 - training loss (MAE): 0.0582, validation MSE: -0.5318
2025-05-03 07:59:37 [INFO]: Epoch 268 - training loss (MAE): 0.0586, validation MSE: -0.4933
2025-05-03 07:59:52 [INFO]: Epoch 269 - training loss (MAE): 0.0581, validation MSE: -0.5384
2025-05-03 08:00:07 [INFO]: Epoch 270 - training loss (MAE): 0.0588, validation MSE: -0.4758
2025-05-03 08:00:22 [INFO]: Epoch 271 - training loss (MAE): 0.0579, validation MSE: -0.5379
2025-05-03 08:00:37 [INFO]: Epoch 272 - training loss (MAE): 0.0581, validation MSE: -0.5260
2025-05-03 08:00:53 [INFO]: Epoch 273 - training loss (MAE): 0.0578, validation MSE: -0.5476
2025-05-03 08:01:08 [INFO]: Epoch 274 - training loss (MAE): 0.0574, validation MSE: -0.5072
2025-05-03 08:01:23 [INFO]: Epoch 275 - training loss (MAE): 0.0574, validation MSE: -0.5108
2025-05-03 08:01:38 [INFO]: Epoch 276 - training loss (MAE): 0.0576, validation MSE: -0.5141
2025-05-03 08:01:53 [INFO]: Epoch 277 - training loss (MAE): 0.0584, validation MSE: -0.5104
2025-05-03 08:02:08 [INFO]: Epoch 278 - training loss (MAE): 0.0574, validation MSE: -0.5131
2025-05-03 08:02:24 [INFO]: Epoch 279 - training loss (MAE): 0.0575, validation MSE: -0.4608
2025-05-03 08:02:38 [INFO]: Epoch 280 - training loss (MAE): 0.0581, validation MSE: -0.5180
2025-05-03 08:02:53 [INFO]: Epoch 281 - training loss (MAE): 0.0579, validation MSE: -0.5305
2025-05-03 08:03:09 [INFO]: Epoch 282 - training loss (MAE): 0.0582, validation MSE: -0.4866
2025-05-03 08:03:24 [INFO]: Epoch 283 - training loss (MAE): 0.0577, validation MSE: -0.5217
2025-05-03 08:03:39 [INFO]: Epoch 284 - training loss (MAE): 0.0577, validation MSE: -0.5265
2025-05-03 08:03:54 [INFO]: Epoch 285 - training loss (MAE): 0.0574, validation MSE: -0.5075
2025-05-03 08:04:10 [INFO]: Epoch 286 - training loss (MAE): 0.0576, validation MSE: -0.4455
2025-05-03 08:04:25 [INFO]: Epoch 287 - training loss (MAE): 0.0577, validation MSE: -0.4435
2025-05-03 08:04:40 [INFO]: Epoch 288 - training loss (MAE): 0.0580, validation MSE: -0.5282
2025-05-03 08:04:55 [INFO]: Epoch 289 - training loss (MAE): 0.0576, validation MSE: -0.5353
2025-05-03 08:05:10 [INFO]: Epoch 290 - training loss (MAE): 0.0568, validation MSE: -0.5058
2025-05-03 08:05:26 [INFO]: Epoch 291 - training loss (MAE): 0.0576, validation MSE: -0.5076
2025-05-03 08:05:41 [INFO]: Epoch 292 - training loss (MAE): 0.0576, validation MSE: -0.4997
2025-05-03 08:05:56 [INFO]: Epoch 293 - training loss (MAE): 0.0574, validation MSE: -0.5361
2025-05-03 08:06:11 [INFO]: Epoch 294 - training loss (MAE): 0.0579, validation MSE: -0.4827
2025-05-03 08:06:26 [INFO]: Epoch 295 - training loss (MAE): 0.0574, validation MSE: -0.4933
2025-05-03 08:06:41 [INFO]: Epoch 296 - training loss (MAE): 0.0568, validation MSE: -0.4861
2025-05-03 08:06:56 [INFO]: Epoch 297 - training loss (MAE): 0.0571, validation MSE: -0.4861
2025-05-03 08:07:11 [INFO]: Epoch 298 - training loss (MAE): 0.0574, validation MSE: -0.5230
2025-05-03 08:07:27 [INFO]: Epoch 299 - training loss (MAE): 0.0574, validation MSE: -0.4951
2025-05-03 08:07:42 [INFO]: Epoch 300 - training loss (MAE): 0.0579, validation MSE: -0.5355
2025-05-03 08:07:57 [INFO]: Epoch 301 - training loss (MAE): 0.0576, validation MSE: -0.5070
2025-05-03 08:08:12 [INFO]: Epoch 302 - training loss (MAE): 0.0571, validation MSE: -0.4998
2025-05-03 08:08:27 [INFO]: Epoch 303 - training loss (MAE): 0.0569, validation MSE: -0.4400
2025-05-03 08:08:42 [INFO]: Epoch 304 - training loss (MAE): 0.0567, validation MSE: -0.4474
2025-05-03 08:08:58 [INFO]: Epoch 305 - training loss (MAE): 0.0577, validation MSE: -0.5272
2025-05-03 08:09:13 [INFO]: Epoch 306 - training loss (MAE): 0.0573, validation MSE: -0.4636
2025-05-03 08:09:28 [INFO]: Epoch 307 - training loss (MAE): 0.0581, validation MSE: -0.5156
2025-05-03 08:09:43 [INFO]: Epoch 308 - training loss (MAE): 0.0575, validation MSE: -0.5200
2025-05-03 08:09:58 [INFO]: Epoch 309 - training loss (MAE): 0.0570, validation MSE: -0.5111
2025-05-03 08:10:13 [INFO]: Epoch 310 - training loss (MAE): 0.0573, validation MSE: -0.5087
2025-05-03 08:10:29 [INFO]: Epoch 311 - training loss (MAE): 0.0568, validation MSE: -0.5247
2025-05-03 08:10:43 [INFO]: Epoch 312 - training loss (MAE): 0.0567, validation MSE: -0.4734
2025-05-03 08:10:59 [INFO]: Epoch 313 - training loss (MAE): 0.0569, validation MSE: -0.5191
2025-05-03 08:11:14 [INFO]: Epoch 314 - training loss (MAE): 0.0567, validation MSE: -0.5381
2025-05-03 08:11:29 [INFO]: Epoch 315 - training loss (MAE): 0.0566, validation MSE: -0.5365
2025-05-03 08:11:44 [INFO]: Epoch 316 - training loss (MAE): 0.0567, validation MSE: -0.5308
2025-05-03 08:12:00 [INFO]: Epoch 317 - training loss (MAE): 0.0572, validation MSE: -0.5177
2025-05-03 08:12:15 [INFO]: Epoch 318 - training loss (MAE): 0.0569, validation MSE: -0.5461
2025-05-03 08:12:31 [INFO]: Epoch 319 - training loss (MAE): 0.0568, validation MSE: -0.5293
2025-05-03 08:12:46 [INFO]: Epoch 320 - training loss (MAE): 0.0568, validation MSE: -0.4737
2025-05-03 08:13:01 [INFO]: Epoch 321 - training loss (MAE): 0.0564, validation MSE: -0.5385
2025-05-03 08:13:16 [INFO]: Epoch 322 - training loss (MAE): 0.0565, validation MSE: -0.5195
2025-05-03 08:13:31 [INFO]: Epoch 323 - training loss (MAE): 0.0569, validation MSE: -0.5418
2025-05-03 08:13:46 [INFO]: Epoch 324 - training loss (MAE): 0.0567, validation MSE: -0.5200
2025-05-03 08:14:01 [INFO]: Epoch 325 - training loss (MAE): 0.0563, validation MSE: -0.5246
2025-05-03 08:14:16 [INFO]: Epoch 326 - training loss (MAE): 0.0562, validation MSE: -0.4813
2025-05-03 08:14:32 [INFO]: Epoch 327 - training loss (MAE): 0.0561, validation MSE: -0.5005
2025-05-03 08:14:47 [INFO]: Epoch 328 - training loss (MAE): 0.0564, validation MSE: -0.4962
2025-05-03 08:15:02 [INFO]: Epoch 329 - training loss (MAE): 0.0571, validation MSE: -0.5247
2025-05-03 08:15:17 [INFO]: Epoch 330 - training loss (MAE): 0.0562, validation MSE: -0.5252
2025-05-03 08:15:32 [INFO]: Epoch 331 - training loss (MAE): 0.0566, validation MSE: -0.4891
2025-05-03 08:15:47 [INFO]: Epoch 332 - training loss (MAE): 0.0560, validation MSE: -0.5320
2025-05-03 08:16:03 [INFO]: Epoch 333 - training loss (MAE): 0.0566, validation MSE: -0.5241
2025-05-03 08:16:18 [INFO]: Epoch 334 - training loss (MAE): 0.0560, validation MSE: -0.5218
2025-05-03 08:16:33 [INFO]: Epoch 335 - training loss (MAE): 0.0567, validation MSE: -0.5070
2025-05-03 08:16:48 [INFO]: Epoch 336 - training loss (MAE): 0.0566, validation MSE: -0.5288
2025-05-03 08:17:03 [INFO]: Epoch 337 - training loss (MAE): 0.0556, validation MSE: -0.4622
2025-05-03 08:17:19 [INFO]: Epoch 338 - training loss (MAE): 0.0567, validation MSE: -0.4576
2025-05-03 08:17:34 [INFO]: Epoch 339 - training loss (MAE): 0.0568, validation MSE: -0.5118
2025-05-03 08:17:49 [INFO]: Epoch 340 - training loss (MAE): 0.0561, validation MSE: -0.5122
2025-05-03 08:18:04 [INFO]: Epoch 341 - training loss (MAE): 0.0558, validation MSE: -0.5380
2025-05-03 08:18:19 [INFO]: Epoch 342 - training loss (MAE): 0.0562, validation MSE: -0.5298
2025-05-03 08:18:34 [INFO]: Epoch 343 - training loss (MAE): 0.0558, validation MSE: -0.5086
2025-05-03 08:18:49 [INFO]: Epoch 344 - training loss (MAE): 0.0558, validation MSE: -0.4726
2025-05-03 08:19:05 [INFO]: Epoch 345 - training loss (MAE): 0.0554, validation MSE: -0.5350
2025-05-03 08:19:20 [INFO]: Epoch 346 - training loss (MAE): 0.0555, validation MSE: -0.4821
2025-05-03 08:19:35 [INFO]: Epoch 347 - training loss (MAE): 0.0560, validation MSE: -0.5128
2025-05-03 08:19:50 [INFO]: Epoch 348 - training loss (MAE): 0.0558, validation MSE: -0.4919
2025-05-03 08:20:05 [INFO]: Epoch 349 - training loss (MAE): 0.0561, validation MSE: -0.4928
2025-05-03 08:20:20 [INFO]: Epoch 350 - training loss (MAE): 0.0558, validation MSE: -0.5190
2025-05-03 08:20:36 [INFO]: Epoch 351 - training loss (MAE): 0.0556, validation MSE: -0.4780
2025-05-03 08:20:51 [INFO]: Epoch 352 - training loss (MAE): 0.0558, validation MSE: -0.5057
2025-05-03 08:21:06 [INFO]: Epoch 353 - training loss (MAE): 0.0552, validation MSE: -0.5305
2025-05-03 08:21:21 [INFO]: Epoch 354 - training loss (MAE): 0.0555, validation MSE: -0.5206
2025-05-03 08:21:36 [INFO]: Epoch 355 - training loss (MAE): 0.0553, validation MSE: -0.3818
2025-05-03 08:21:51 [INFO]: Epoch 356 - training loss (MAE): 0.0552, validation MSE: -0.4713
2025-05-03 08:22:06 [INFO]: Epoch 357 - training loss (MAE): 0.0552, validation MSE: -0.4159
2025-05-03 08:22:21 [INFO]: Epoch 358 - training loss (MAE): 0.0549, validation MSE: -0.5169
2025-05-03 08:22:37 [INFO]: Epoch 359 - training loss (MAE): 0.0551, validation MSE: -0.3909
2025-05-03 08:22:52 [INFO]: Epoch 360 - training loss (MAE): 0.0557, validation MSE: -0.4860
2025-05-03 08:23:07 [INFO]: Epoch 361 - training loss (MAE): 0.0551, validation MSE: -0.5379
2025-05-03 08:23:22 [INFO]: Epoch 362 - training loss (MAE): 0.0554, validation MSE: -0.5213
2025-05-03 08:23:37 [INFO]: Epoch 363 - training loss (MAE): 0.0548, validation MSE: -0.5093
2025-05-03 08:23:52 [INFO]: Epoch 364 - training loss (MAE): 0.0556, validation MSE: -0.4526
2025-05-03 08:24:07 [INFO]: Epoch 365 - training loss (MAE): 0.0553, validation MSE: -0.5394
2025-05-03 08:24:22 [INFO]: Epoch 366 - training loss (MAE): 0.0548, validation MSE: -0.4660
2025-05-03 08:24:37 [INFO]: Epoch 367 - training loss (MAE): 0.0554, validation MSE: -0.5309
2025-05-03 08:24:53 [INFO]: Epoch 368 - training loss (MAE): 0.0547, validation MSE: -0.5000
2025-05-03 08:25:08 [INFO]: Epoch 369 - training loss (MAE): 0.0551, validation MSE: -0.4791
2025-05-03 08:25:23 [INFO]: Epoch 370 - training loss (MAE): 0.0549, validation MSE: -0.5132
2025-05-03 08:25:38 [INFO]: Epoch 371 - training loss (MAE): 0.0548, validation MSE: -0.5257
2025-05-03 08:25:53 [INFO]: Epoch 372 - training loss (MAE): 0.0542, validation MSE: -0.4995
2025-05-03 08:26:09 [INFO]: Epoch 373 - training loss (MAE): 0.0548, validation MSE: -0.5231
2025-05-03 08:26:24 [INFO]: Epoch 374 - training loss (MAE): 0.0547, validation MSE: -0.5236
2025-05-03 08:26:39 [INFO]: Epoch 375 - training loss (MAE): 0.0546, validation MSE: -0.5284
2025-05-03 08:26:54 [INFO]: Epoch 376 - training loss (MAE): 0.0549, validation MSE: -0.4278
2025-05-03 08:27:09 [INFO]: Epoch 377 - training loss (MAE): 0.0543, validation MSE: -0.4815
2025-05-03 08:27:24 [INFO]: Epoch 378 - training loss (MAE): 0.0546, validation MSE: -0.5299
2025-05-03 08:27:39 [INFO]: Epoch 379 - training loss (MAE): 0.0560, validation MSE: -0.5251
2025-05-03 08:27:54 [INFO]: Epoch 380 - training loss (MAE): 0.0538, validation MSE: -0.5221
2025-05-03 08:28:09 [INFO]: Epoch 381 - training loss (MAE): 0.0543, validation MSE: -0.5069
2025-05-03 08:28:25 [INFO]: Epoch 382 - training loss (MAE): 0.0545, validation MSE: -0.4233
2025-05-03 08:28:40 [INFO]: Epoch 383 - training loss (MAE): 0.0545, validation MSE: -0.5064
2025-05-03 08:28:55 [INFO]: Epoch 384 - training loss (MAE): 0.0541, validation MSE: -0.5118
2025-05-03 08:29:10 [INFO]: Epoch 385 - training loss (MAE): 0.0539, validation MSE: -0.5208
2025-05-03 08:29:25 [INFO]: Epoch 386 - training loss (MAE): 0.0542, validation MSE: -0.5212
2025-05-03 08:29:40 [INFO]: Epoch 387 - training loss (MAE): 0.0545, validation MSE: -0.4846
2025-05-03 08:29:55 [INFO]: Epoch 388 - training loss (MAE): 0.0539, validation MSE: -0.5137
2025-05-03 08:30:11 [INFO]: Epoch 389 - training loss (MAE): 0.0540, validation MSE: -0.5060
2025-05-03 08:30:26 [INFO]: Epoch 390 - training loss (MAE): 0.0540, validation MSE: -0.4825
2025-05-03 08:30:41 [INFO]: Epoch 391 - training loss (MAE): 0.0543, validation MSE: -0.4913
2025-05-03 08:30:56 [INFO]: Epoch 392 - training loss (MAE): 0.0539, validation MSE: -0.5357
2025-05-03 08:31:11 [INFO]: Epoch 393 - training loss (MAE): 0.0540, validation MSE: -0.5203
2025-05-03 08:31:27 [INFO]: Epoch 394 - training loss (MAE): 0.0536, validation MSE: -0.5368
2025-05-03 08:31:42 [INFO]: Epoch 395 - training loss (MAE): 0.0541, validation MSE: -0.5312
2025-05-03 08:31:57 [INFO]: Epoch 396 - training loss (MAE): 0.0533, validation MSE: -0.5217
2025-05-03 08:32:12 [INFO]: Epoch 397 - training loss (MAE): 0.0539, validation MSE: -0.5158
2025-05-03 08:32:27 [INFO]: Epoch 398 - training loss (MAE): 0.0534, validation MSE: -0.5113
2025-05-03 08:32:42 [INFO]: Epoch 399 - training loss (MAE): 0.0535, validation MSE: -0.5198
2025-05-03 08:32:57 [INFO]: Epoch 400 - training loss (MAE): 0.0537, validation MSE: -0.4867
2025-05-03 08:33:12 [INFO]: Epoch 401 - training loss (MAE): 0.0535, validation MSE: -0.4866
2025-05-03 08:33:28 [INFO]: Epoch 402 - training loss (MAE): 0.0542, validation MSE: -0.5138
2025-05-03 08:33:43 [INFO]: Epoch 403 - training loss (MAE): 0.0539, validation MSE: -0.4772
2025-05-03 08:33:58 [INFO]: Epoch 404 - training loss (MAE): 0.0535, validation MSE: -0.4791
2025-05-03 08:34:13 [INFO]: Epoch 405 - training loss (MAE): 0.0538, validation MSE: -0.4753
2025-05-03 08:34:28 [INFO]: Epoch 406 - training loss (MAE): 0.0532, validation MSE: -0.4980
2025-05-03 08:34:43 [INFO]: Epoch 407 - training loss (MAE): 0.0538, validation MSE: -0.4562
2025-05-03 08:34:59 [INFO]: Epoch 408 - training loss (MAE): 0.0534, validation MSE: -0.5175
2025-05-03 08:35:14 [INFO]: Epoch 409 - training loss (MAE): 0.0533, validation MSE: -0.4847
2025-05-03 08:35:29 [INFO]: Epoch 410 - training loss (MAE): 0.0535, validation MSE: -0.5256
2025-05-03 08:35:44 [INFO]: Epoch 411 - training loss (MAE): 0.0531, validation MSE: -0.5200
2025-05-03 08:36:00 [INFO]: Epoch 412 - training loss (MAE): 0.0532, validation MSE: -0.4876
2025-05-03 08:36:15 [INFO]: Epoch 413 - training loss (MAE): 0.0535, validation MSE: -0.5145
2025-05-03 08:36:30 [INFO]: Epoch 414 - training loss (MAE): 0.0536, validation MSE: -0.5210
2025-05-03 08:36:45 [INFO]: Epoch 415 - training loss (MAE): 0.0530, validation MSE: -0.4617
2025-05-03 08:37:00 [INFO]: Epoch 416 - training loss (MAE): 0.0545, validation MSE: -0.5032
2025-05-03 08:37:16 [INFO]: Epoch 417 - training loss (MAE): 0.0527, validation MSE: -0.5132
2025-05-03 08:37:31 [INFO]: Epoch 418 - training loss (MAE): 0.0529, validation MSE: -0.5054
2025-05-03 08:37:46 [INFO]: Epoch 419 - training loss (MAE): 0.0535, validation MSE: -0.4719
2025-05-03 08:38:01 [INFO]: Epoch 420 - training loss (MAE): 0.0535, validation MSE: -0.4686
2025-05-03 08:38:16 [INFO]: Epoch 421 - training loss (MAE): 0.0533, validation MSE: -0.5020
2025-05-03 08:38:31 [INFO]: Epoch 422 - training loss (MAE): 0.0526, validation MSE: -0.4848
2025-05-03 08:38:46 [INFO]: Epoch 423 - training loss (MAE): 0.0531, validation MSE: -0.5154
2025-05-03 08:39:02 [INFO]: Epoch 424 - training loss (MAE): 0.0528, validation MSE: -0.5040
2025-05-03 08:39:17 [INFO]: Epoch 425 - training loss (MAE): 0.0527, validation MSE: -0.5056
2025-05-03 08:39:32 [INFO]: Epoch 426 - training loss (MAE): 0.0527, validation MSE: -0.5375
2025-05-03 08:39:47 [INFO]: Epoch 427 - training loss (MAE): 0.0527, validation MSE: -0.5045
2025-05-03 08:40:02 [INFO]: Epoch 428 - training loss (MAE): 0.0534, validation MSE: -0.4850
2025-05-03 08:40:18 [INFO]: Epoch 429 - training loss (MAE): 0.0523, validation MSE: -0.5075
2025-05-03 08:40:33 [INFO]: Epoch 430 - training loss (MAE): 0.0522, validation MSE: -0.4837
2025-05-03 08:40:48 [INFO]: Epoch 431 - training loss (MAE): 0.0525, validation MSE: -0.5199
2025-05-03 08:41:03 [INFO]: Epoch 432 - training loss (MAE): 0.0527, validation MSE: -0.5016
2025-05-03 08:41:19 [INFO]: Epoch 433 - training loss (MAE): 0.0523, validation MSE: -0.4816
2025-05-03 08:41:34 [INFO]: Epoch 434 - training loss (MAE): 0.0525, validation MSE: -0.4736
2025-05-03 08:41:49 [INFO]: Epoch 435 - training loss (MAE): 0.0522, validation MSE: -0.4678
2025-05-03 08:42:04 [INFO]: Epoch 436 - training loss (MAE): 0.0522, validation MSE: -0.4916
2025-05-03 08:42:19 [INFO]: Epoch 437 - training loss (MAE): 0.0524, validation MSE: -0.4752
2025-05-03 08:42:35 [INFO]: Epoch 438 - training loss (MAE): 0.0517, validation MSE: -0.4591
2025-05-03 08:42:50 [INFO]: Epoch 439 - training loss (MAE): 0.0519, validation MSE: -0.5090
2025-05-03 08:43:05 [INFO]: Epoch 440 - training loss (MAE): 0.0530, validation MSE: -0.4912
2025-05-03 08:43:20 [INFO]: Epoch 441 - training loss (MAE): 0.0526, validation MSE: -0.4879
2025-05-03 08:43:35 [INFO]: Epoch 442 - training loss (MAE): 0.0525, validation MSE: -0.5013
2025-05-03 08:43:50 [INFO]: Epoch 443 - training loss (MAE): 0.0528, validation MSE: -0.4894
2025-05-03 08:44:05 [INFO]: Epoch 444 - training loss (MAE): 0.0520, validation MSE: -0.4808
2025-05-03 08:44:21 [INFO]: Epoch 445 - training loss (MAE): 0.0521, validation MSE: -0.4892
2025-05-03 08:44:36 [INFO]: Epoch 446 - training loss (MAE): 0.0514, validation MSE: -0.4893
2025-05-03 08:44:51 [INFO]: Epoch 447 - training loss (MAE): 0.0522, validation MSE: -0.4849
2025-05-03 08:45:06 [INFO]: Epoch 448 - training loss (MAE): 0.0524, validation MSE: -0.5071
2025-05-03 08:45:21 [INFO]: Epoch 449 - training loss (MAE): 0.0517, validation MSE: -0.5031
2025-05-03 08:45:37 [INFO]: Epoch 450 - training loss (MAE): 0.0515, validation MSE: -0.4825
2025-05-03 08:45:52 [INFO]: Epoch 451 - training loss (MAE): 0.0520, validation MSE: -0.5106
2025-05-03 08:46:07 [INFO]: Epoch 452 - training loss (MAE): 0.0515, validation MSE: -0.5160
2025-05-03 08:46:22 [INFO]: Epoch 453 - training loss (MAE): 0.0517, validation MSE: -0.4913
2025-05-03 08:46:38 [INFO]: Epoch 454 - training loss (MAE): 0.0517, validation MSE: -0.4866
2025-05-03 08:46:53 [INFO]: Epoch 455 - training loss (MAE): 0.0523, validation MSE: -0.4966
2025-05-03 08:47:08 [INFO]: Epoch 456 - training loss (MAE): 0.0519, validation MSE: -0.5019
2025-05-03 08:47:23 [INFO]: Epoch 457 - training loss (MAE): 0.0515, validation MSE: -0.4495
2025-05-03 08:47:38 [INFO]: Epoch 458 - training loss (MAE): 0.0521, validation MSE: -0.4906
2025-05-03 08:47:53 [INFO]: Epoch 459 - training loss (MAE): 0.0515, validation MSE: -0.4906
2025-05-03 08:48:08 [INFO]: Epoch 460 - training loss (MAE): 0.0518, validation MSE: -0.4835
2025-05-03 08:48:23 [INFO]: Epoch 461 - training loss (MAE): 0.0519, validation MSE: -0.4946
2025-05-03 08:48:39 [INFO]: Epoch 462 - training loss (MAE): 0.0514, validation MSE: -0.4676
2025-05-03 08:48:53 [INFO]: Epoch 463 - training loss (MAE): 0.0512, validation MSE: -0.4979
2025-05-03 08:49:09 [INFO]: Epoch 464 - training loss (MAE): 0.0513, validation MSE: -0.4940
2025-05-03 08:49:24 [INFO]: Epoch 465 - training loss (MAE): 0.0512, validation MSE: -0.4979
2025-05-03 08:49:39 [INFO]: Epoch 466 - training loss (MAE): 0.0520, validation MSE: -0.4811
2025-05-03 08:49:54 [INFO]: Epoch 467 - training loss (MAE): 0.0510, validation MSE: -0.4982
2025-05-03 08:50:09 [INFO]: Epoch 468 - training loss (MAE): 0.0507, validation MSE: -0.4739
2025-05-03 08:50:24 [INFO]: Epoch 469 - training loss (MAE): 0.0512, validation MSE: -0.4825
2025-05-03 08:50:39 [INFO]: Epoch 470 - training loss (MAE): 0.0512, validation MSE: -0.4742
2025-05-03 08:50:55 [INFO]: Epoch 471 - training loss (MAE): 0.0510, validation MSE: -0.5032
2025-05-03 08:51:10 [INFO]: Epoch 472 - training loss (MAE): 0.0511, validation MSE: -0.4850
2025-05-03 08:51:25 [INFO]: Epoch 473 - training loss (MAE): 0.0512, validation MSE: -0.4948
2025-05-03 08:51:40 [INFO]: Epoch 474 - training loss (MAE): 0.0509, validation MSE: -0.4676
2025-05-03 08:51:55 [INFO]: Epoch 475 - training loss (MAE): 0.0509, validation MSE: -0.4589
2025-05-03 08:52:10 [INFO]: Epoch 476 - training loss (MAE): 0.0518, validation MSE: -0.4920
2025-05-03 08:52:26 [INFO]: Epoch 477 - training loss (MAE): 0.0509, validation MSE: -0.4744
2025-05-03 08:52:41 [INFO]: Epoch 478 - training loss (MAE): 0.0511, validation MSE: -0.4878
2025-05-03 08:52:56 [INFO]: Epoch 479 - training loss (MAE): 0.0507, validation MSE: -0.4953
2025-05-03 08:53:11 [INFO]: Epoch 480 - training loss (MAE): 0.0505, validation MSE: -0.4793
2025-05-03 08:53:26 [INFO]: Epoch 481 - training loss (MAE): 0.0505, validation MSE: -0.4884
2025-05-03 08:53:42 [INFO]: Epoch 482 - training loss (MAE): 0.0510, validation MSE: -0.4885
2025-05-03 08:53:57 [INFO]: Epoch 483 - training loss (MAE): 0.0513, validation MSE: -0.4941
2025-05-03 08:54:12 [INFO]: Epoch 484 - training loss (MAE): 0.0503, validation MSE: -0.4976
2025-05-03 08:54:27 [INFO]: Epoch 485 - training loss (MAE): 0.0507, validation MSE: -0.4943
2025-05-03 08:54:43 [INFO]: Epoch 486 - training loss (MAE): 0.0505, validation MSE: -0.5014
2025-05-03 08:54:57 [INFO]: Epoch 487 - training loss (MAE): 0.0508, validation MSE: -0.4927
2025-05-03 08:55:13 [INFO]: Epoch 488 - training loss (MAE): 0.0501, validation MSE: -0.5061
2025-05-03 08:55:28 [INFO]: Epoch 489 - training loss (MAE): 0.0510, validation MSE: -0.4705
2025-05-03 08:55:43 [INFO]: Epoch 490 - training loss (MAE): 0.0508, validation MSE: -0.5105
2025-05-03 08:55:58 [INFO]: Epoch 491 - training loss (MAE): 0.0506, validation MSE: -0.5004
2025-05-03 08:56:13 [INFO]: Epoch 492 - training loss (MAE): 0.0508, validation MSE: -0.4855
2025-05-03 08:56:28 [INFO]: Epoch 493 - training loss (MAE): 0.0504, validation MSE: -0.4778
2025-05-03 08:56:43 [INFO]: Epoch 494 - training loss (MAE): 0.0501, validation MSE: -0.4739
2025-05-03 08:56:58 [INFO]: Epoch 495 - training loss (MAE): 0.0497, validation MSE: -0.4999
2025-05-03 08:57:13 [INFO]: Epoch 496 - training loss (MAE): 0.0503, validation MSE: -0.4566
2025-05-03 08:57:28 [INFO]: Epoch 497 - training loss (MAE): 0.0500, validation MSE: -0.4989
2025-05-03 08:57:44 [INFO]: Epoch 498 - training loss (MAE): 0.0500, validation MSE: -0.4982
2025-05-03 08:57:59 [INFO]: Epoch 499 - training loss (MAE): 0.0505, validation MSE: -0.5055
2025-05-03 08:58:14 [INFO]: Epoch 500 - training loss (MAE): 0.0500, validation MSE: -0.4868
2025-05-03 08:58:14 [INFO]: Finished training. The best model is from epoch#86.
2025-05-03 08:58:15 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_8.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 08:58:15 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_8.pypots
Fold 8 metrics: MAE: 0.309, MSE: 0.351, MRE: 0.429
Fold 8 metrics: MAE: 0.309, MSE: 0.351, MRE: 0.429
Training fold 9/10
2025-05-03 08:58:15 [INFO]: No given device, using default device: cuda
2025-05-03 08:58:15 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-03 08:58:15 [INFO]: Using customized MAE as the training loss function.
2025-05-03 08:58:15 [INFO]: Using customized MSE as the validation metric function.
2025-05-03 08:58:15 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-03 08:58:30 [INFO]: Epoch 001 - training loss (MAE): 0.9359, validation MSE: -0.0252
2025-05-03 08:58:45 [INFO]: Epoch 002 - training loss (MAE): 0.3819, validation MSE: -0.0718
2025-05-03 08:59:00 [INFO]: Epoch 003 - training loss (MAE): 0.3257, validation MSE: -0.1016
2025-05-03 08:59:15 [INFO]: Epoch 004 - training loss (MAE): 0.2953, validation MSE: -0.1035
2025-05-03 08:59:31 [INFO]: Epoch 005 - training loss (MAE): 0.2740, validation MSE: -0.1342
2025-05-03 08:59:46 [INFO]: Epoch 006 - training loss (MAE): 0.2520, validation MSE: -0.1430
2025-05-03 09:00:01 [INFO]: Epoch 007 - training loss (MAE): 0.2329, validation MSE: -0.1623
2025-05-03 09:00:16 [INFO]: Epoch 008 - training loss (MAE): 0.2155, validation MSE: -0.1578
2025-05-03 09:00:31 [INFO]: Epoch 009 - training loss (MAE): 0.1955, validation MSE: -0.1798
2025-05-03 09:00:46 [INFO]: Epoch 010 - training loss (MAE): 0.1793, validation MSE: -0.1734
2025-05-03 09:01:01 [INFO]: Epoch 011 - training loss (MAE): 0.1680, validation MSE: -0.1808
2025-05-03 09:01:17 [INFO]: Epoch 012 - training loss (MAE): 0.1561, validation MSE: -0.1751
2025-05-03 09:01:32 [INFO]: Epoch 013 - training loss (MAE): 0.1468, validation MSE: -0.1769
2025-05-03 09:01:47 [INFO]: Epoch 014 - training loss (MAE): 0.1385, validation MSE: -0.1740
2025-05-03 09:02:02 [INFO]: Epoch 015 - training loss (MAE): 0.1323, validation MSE: -0.1584
2025-05-03 09:02:17 [INFO]: Epoch 016 - training loss (MAE): 0.1267, validation MSE: -0.1653
2025-05-03 09:02:32 [INFO]: Epoch 017 - training loss (MAE): 0.1223, validation MSE: -0.1692
2025-05-03 09:02:47 [INFO]: Epoch 018 - training loss (MAE): 0.1190, validation MSE: -0.1741
2025-05-03 09:03:02 [INFO]: Epoch 019 - training loss (MAE): 0.1154, validation MSE: -0.1754
2025-05-03 09:03:17 [INFO]: Epoch 020 - training loss (MAE): 0.1128, validation MSE: -0.1738
2025-05-03 09:03:32 [INFO]: Epoch 021 - training loss (MAE): 0.1098, validation MSE: -0.1719
2025-05-03 09:03:48 [INFO]: Epoch 022 - training loss (MAE): 0.1078, validation MSE: -0.1956
2025-05-03 09:04:03 [INFO]: Epoch 023 - training loss (MAE): 0.1056, validation MSE: -0.1907
2025-05-03 09:04:18 [INFO]: Epoch 024 - training loss (MAE): 0.1033, validation MSE: -0.1827
2025-05-03 09:04:33 [INFO]: Epoch 025 - training loss (MAE): 0.1016, validation MSE: -0.1972
2025-05-03 09:04:49 [INFO]: Epoch 026 - training loss (MAE): 0.1006, validation MSE: -0.1831
2025-05-03 09:05:04 [INFO]: Epoch 027 - training loss (MAE): 0.0986, validation MSE: -0.1821
2025-05-03 09:05:19 [INFO]: Epoch 028 - training loss (MAE): 0.0978, validation MSE: -0.1728
2025-05-03 09:05:34 [INFO]: Epoch 029 - training loss (MAE): 0.0951, validation MSE: -0.1890
2025-05-03 09:05:50 [INFO]: Epoch 030 - training loss (MAE): 0.0949, validation MSE: -0.1872
2025-05-03 09:06:05 [INFO]: Epoch 031 - training loss (MAE): 0.0934, validation MSE: -0.1783
2025-05-03 09:06:20 [INFO]: Epoch 032 - training loss (MAE): 0.0930, validation MSE: -0.1924
2025-05-03 09:06:35 [INFO]: Epoch 033 - training loss (MAE): 0.0909, validation MSE: -0.1899
2025-05-03 09:06:50 [INFO]: Epoch 034 - training loss (MAE): 0.0914, validation MSE: -0.1802
2025-05-03 09:07:05 [INFO]: Epoch 035 - training loss (MAE): 0.0897, validation MSE: -0.1815
2025-05-03 09:07:20 [INFO]: Epoch 036 - training loss (MAE): 0.0890, validation MSE: -0.1904
2025-05-03 09:07:36 [INFO]: Epoch 037 - training loss (MAE): 0.0884, validation MSE: -0.1753
2025-05-03 09:07:51 [INFO]: Epoch 038 - training loss (MAE): 0.0880, validation MSE: -0.1889
2025-05-03 09:08:06 [INFO]: Epoch 039 - training loss (MAE): 0.0863, validation MSE: -0.1845
2025-05-03 09:08:21 [INFO]: Epoch 040 - training loss (MAE): 0.0858, validation MSE: -0.1971
2025-05-03 09:08:36 [INFO]: Epoch 041 - training loss (MAE): 0.0854, validation MSE: -0.1835
2025-05-03 09:08:52 [INFO]: Epoch 042 - training loss (MAE): 0.0842, validation MSE: -0.1928
2025-05-03 09:09:07 [INFO]: Epoch 043 - training loss (MAE): 0.0846, validation MSE: -0.1900
2025-05-03 09:09:22 [INFO]: Epoch 044 - training loss (MAE): 0.0832, validation MSE: -0.1899
2025-05-03 09:09:37 [INFO]: Epoch 045 - training loss (MAE): 0.0826, validation MSE: -0.2036
2025-05-03 09:09:52 [INFO]: Epoch 046 - training loss (MAE): 0.0821, validation MSE: -0.1929
2025-05-03 09:10:07 [INFO]: Epoch 047 - training loss (MAE): 0.0818, validation MSE: -0.1913
2025-05-03 09:10:23 [INFO]: Epoch 048 - training loss (MAE): 0.0809, validation MSE: -0.1879
2025-05-03 09:10:38 [INFO]: Epoch 049 - training loss (MAE): 0.0803, validation MSE: -0.1777
2025-05-03 09:10:53 [INFO]: Epoch 050 - training loss (MAE): 0.0804, validation MSE: -0.1856
2025-05-03 09:11:08 [INFO]: Epoch 051 - training loss (MAE): 0.0799, validation MSE: -0.1969
2025-05-03 09:11:23 [INFO]: Epoch 052 - training loss (MAE): 0.0790, validation MSE: -0.1980
2025-05-03 09:11:38 [INFO]: Epoch 053 - training loss (MAE): 0.0792, validation MSE: -0.1975
2025-05-03 09:11:53 [INFO]: Epoch 054 - training loss (MAE): 0.0784, validation MSE: -0.2003
2025-05-03 09:12:08 [INFO]: Epoch 055 - training loss (MAE): 0.0780, validation MSE: -0.2028
2025-05-03 09:12:23 [INFO]: Epoch 056 - training loss (MAE): 0.0778, validation MSE: -0.2012
2025-05-03 09:12:39 [INFO]: Epoch 057 - training loss (MAE): 0.0770, validation MSE: -0.1852
2025-05-03 09:12:54 [INFO]: Epoch 058 - training loss (MAE): 0.0772, validation MSE: -0.2018
2025-05-03 09:13:09 [INFO]: Epoch 059 - training loss (MAE): 0.0770, validation MSE: -0.1924
2025-05-03 09:13:24 [INFO]: Epoch 060 - training loss (MAE): 0.0765, validation MSE: -0.1938
2025-05-03 09:13:39 [INFO]: Epoch 061 - training loss (MAE): 0.0757, validation MSE: -0.2036
2025-05-03 09:13:54 [INFO]: Epoch 062 - training loss (MAE): 0.0752, validation MSE: -0.2032
2025-05-03 09:14:09 [INFO]: Epoch 063 - training loss (MAE): 0.0758, validation MSE: -0.1951
2025-05-03 09:14:25 [INFO]: Epoch 064 - training loss (MAE): 0.0752, validation MSE: -0.1989
2025-05-03 09:14:40 [INFO]: Epoch 065 - training loss (MAE): 0.0745, validation MSE: -0.1949
2025-05-03 09:14:55 [INFO]: Epoch 066 - training loss (MAE): 0.0745, validation MSE: -0.2050
2025-05-03 09:15:10 [INFO]: Epoch 067 - training loss (MAE): 0.0741, validation MSE: -0.1920
2025-05-03 09:15:25 [INFO]: Epoch 068 - training loss (MAE): 0.0737, validation MSE: -0.2054
2025-05-03 09:15:40 [INFO]: Epoch 069 - training loss (MAE): 0.0744, validation MSE: -0.2077
2025-05-03 09:15:55 [INFO]: Epoch 070 - training loss (MAE): 0.0730, validation MSE: -0.1740
2025-05-03 09:16:11 [INFO]: Epoch 071 - training loss (MAE): 0.0732, validation MSE: -0.2120
2025-05-03 09:16:26 [INFO]: Epoch 072 - training loss (MAE): 0.0735, validation MSE: -0.1766
2025-05-03 09:16:41 [INFO]: Epoch 073 - training loss (MAE): 0.0734, validation MSE: -0.2020
2025-05-03 09:16:56 [INFO]: Epoch 074 - training loss (MAE): 0.0719, validation MSE: -0.1905
2025-05-03 09:17:12 [INFO]: Epoch 075 - training loss (MAE): 0.0721, validation MSE: -0.2015
2025-05-03 09:17:27 [INFO]: Epoch 076 - training loss (MAE): 0.0716, validation MSE: -0.2030
2025-05-03 09:17:42 [INFO]: Epoch 077 - training loss (MAE): 0.0715, validation MSE: -0.2120
2025-05-03 09:17:57 [INFO]: Epoch 078 - training loss (MAE): 0.0719, validation MSE: -0.1913
2025-05-03 09:18:12 [INFO]: Epoch 079 - training loss (MAE): 0.0714, validation MSE: -0.2142
2025-05-03 09:18:28 [INFO]: Epoch 080 - training loss (MAE): 0.0710, validation MSE: -0.2011
2025-05-03 09:18:43 [INFO]: Epoch 081 - training loss (MAE): 0.0712, validation MSE: -0.2023
2025-05-03 09:18:58 [INFO]: Epoch 082 - training loss (MAE): 0.0709, validation MSE: -0.2152
2025-05-03 09:19:13 [INFO]: Epoch 083 - training loss (MAE): 0.0703, validation MSE: -0.2160
2025-05-03 09:19:28 [INFO]: Epoch 084 - training loss (MAE): 0.0699, validation MSE: -0.2222
2025-05-03 09:19:44 [INFO]: Epoch 085 - training loss (MAE): 0.0702, validation MSE: -0.1892
2025-05-03 09:19:59 [INFO]: Epoch 086 - training loss (MAE): 0.0708, validation MSE: -0.2071
2025-05-03 09:20:14 [INFO]: Epoch 087 - training loss (MAE): 0.0698, validation MSE: -0.2066
2025-05-03 09:20:29 [INFO]: Epoch 088 - training loss (MAE): 0.0696, validation MSE: -0.2093
2025-05-03 09:20:44 [INFO]: Epoch 089 - training loss (MAE): 0.0694, validation MSE: -0.2069
2025-05-03 09:20:59 [INFO]: Epoch 090 - training loss (MAE): 0.0691, validation MSE: -0.2136
2025-05-03 09:21:15 [INFO]: Epoch 091 - training loss (MAE): 0.0690, validation MSE: -0.2026
2025-05-03 09:21:30 [INFO]: Epoch 092 - training loss (MAE): 0.0695, validation MSE: -0.1959
2025-05-03 09:21:45 [INFO]: Epoch 093 - training loss (MAE): 0.0685, validation MSE: -0.2060
2025-05-03 09:22:01 [INFO]: Epoch 094 - training loss (MAE): 0.0693, validation MSE: -0.2069
2025-05-03 09:22:16 [INFO]: Epoch 095 - training loss (MAE): 0.0685, validation MSE: -0.1984
2025-05-03 09:22:31 [INFO]: Epoch 096 - training loss (MAE): 0.0684, validation MSE: -0.2185
2025-05-03 09:22:46 [INFO]: Epoch 097 - training loss (MAE): 0.0679, validation MSE: -0.2127
2025-05-03 09:23:01 [INFO]: Epoch 098 - training loss (MAE): 0.0685, validation MSE: -0.1960
2025-05-03 09:23:17 [INFO]: Epoch 099 - training loss (MAE): 0.0682, validation MSE: -0.2011
2025-05-03 09:23:32 [INFO]: Epoch 100 - training loss (MAE): 0.0677, validation MSE: -0.1830
2025-05-03 09:23:47 [INFO]: Epoch 101 - training loss (MAE): 0.0677, validation MSE: -0.2131
2025-05-03 09:24:02 [INFO]: Epoch 102 - training loss (MAE): 0.0673, validation MSE: -0.2088
2025-05-03 09:24:17 [INFO]: Epoch 103 - training loss (MAE): 0.0677, validation MSE: -0.2053
2025-05-03 09:24:32 [INFO]: Epoch 104 - training loss (MAE): 0.0673, validation MSE: -0.2220
2025-05-03 09:24:47 [INFO]: Epoch 105 - training loss (MAE): 0.0670, validation MSE: -0.2271
2025-05-03 09:25:02 [INFO]: Epoch 106 - training loss (MAE): 0.0672, validation MSE: -0.2239
2025-05-03 09:25:18 [INFO]: Epoch 107 - training loss (MAE): 0.0673, validation MSE: -0.2206
2025-05-03 09:25:33 [INFO]: Epoch 108 - training loss (MAE): 0.0665, validation MSE: -0.2187
2025-05-03 09:25:48 [INFO]: Epoch 109 - training loss (MAE): 0.0671, validation MSE: -0.2169
2025-05-03 09:26:03 [INFO]: Epoch 110 - training loss (MAE): 0.0669, validation MSE: -0.2182
2025-05-03 09:26:18 [INFO]: Epoch 111 - training loss (MAE): 0.0659, validation MSE: -0.2154
2025-05-03 09:26:34 [INFO]: Epoch 112 - training loss (MAE): 0.0665, validation MSE: -0.2164
2025-05-03 09:26:49 [INFO]: Epoch 113 - training loss (MAE): 0.0670, validation MSE: -0.2118
2025-05-03 09:27:04 [INFO]: Epoch 114 - training loss (MAE): 0.0665, validation MSE: -0.2068
2025-05-03 09:27:18 [INFO]: Epoch 115 - training loss (MAE): 0.0666, validation MSE: -0.2187
2025-05-03 09:27:33 [INFO]: Epoch 116 - training loss (MAE): 0.0658, validation MSE: -0.1997
2025-05-03 09:27:48 [INFO]: Epoch 117 - training loss (MAE): 0.0660, validation MSE: -0.2059
2025-05-03 09:28:03 [INFO]: Epoch 118 - training loss (MAE): 0.0656, validation MSE: -0.2215
2025-05-03 09:28:19 [INFO]: Epoch 119 - training loss (MAE): 0.0658, validation MSE: -0.2048
2025-05-03 09:28:34 [INFO]: Epoch 120 - training loss (MAE): 0.0654, validation MSE: -0.2146
2025-05-03 09:28:49 [INFO]: Epoch 121 - training loss (MAE): 0.0654, validation MSE: -0.2158
2025-05-03 09:29:04 [INFO]: Epoch 122 - training loss (MAE): 0.0656, validation MSE: -0.2036
2025-05-03 09:29:19 [INFO]: Epoch 123 - training loss (MAE): 0.0657, validation MSE: -0.2084
2025-05-03 09:29:34 [INFO]: Epoch 124 - training loss (MAE): 0.0654, validation MSE: -0.2287
2025-05-03 09:29:50 [INFO]: Epoch 125 - training loss (MAE): 0.0653, validation MSE: -0.2203
2025-05-03 09:30:05 [INFO]: Epoch 126 - training loss (MAE): 0.0645, validation MSE: -0.2261
2025-05-03 09:30:20 [INFO]: Epoch 127 - training loss (MAE): 0.0650, validation MSE: -0.2099
2025-05-03 09:30:35 [INFO]: Epoch 128 - training loss (MAE): 0.0651, validation MSE: -0.2248
2025-05-03 09:30:50 [INFO]: Epoch 129 - training loss (MAE): 0.0652, validation MSE: -0.2206
2025-05-03 09:31:05 [INFO]: Epoch 130 - training loss (MAE): 0.0656, validation MSE: -0.2042
2025-05-03 09:31:20 [INFO]: Epoch 131 - training loss (MAE): 0.0649, validation MSE: -0.2111
2025-05-03 09:31:36 [INFO]: Epoch 132 - training loss (MAE): 0.0647, validation MSE: -0.2037
2025-05-03 09:31:51 [INFO]: Epoch 133 - training loss (MAE): 0.0642, validation MSE: -0.2134
2025-05-03 09:32:06 [INFO]: Epoch 134 - training loss (MAE): 0.0644, validation MSE: -0.2122
2025-05-03 09:32:21 [INFO]: Epoch 135 - training loss (MAE): 0.0646, validation MSE: -0.2033
2025-05-03 09:32:36 [INFO]: Epoch 136 - training loss (MAE): 0.0646, validation MSE: -0.2034
2025-05-03 09:32:51 [INFO]: Epoch 137 - training loss (MAE): 0.0645, validation MSE: -0.2114
2025-05-03 09:33:06 [INFO]: Epoch 138 - training loss (MAE): 0.0641, validation MSE: -0.2155
2025-05-03 09:33:21 [INFO]: Epoch 139 - training loss (MAE): 0.0636, validation MSE: -0.2138
2025-05-03 09:33:37 [INFO]: Epoch 140 - training loss (MAE): 0.0637, validation MSE: -0.2254
2025-05-03 09:33:52 [INFO]: Epoch 141 - training loss (MAE): 0.0635, validation MSE: -0.2191
2025-05-03 09:34:07 [INFO]: Epoch 142 - training loss (MAE): 0.0637, validation MSE: -0.2075
2025-05-03 09:34:22 [INFO]: Epoch 143 - training loss (MAE): 0.0635, validation MSE: -0.2283
2025-05-03 09:34:37 [INFO]: Epoch 144 - training loss (MAE): 0.0635, validation MSE: -0.2251
2025-05-03 09:34:52 [INFO]: Epoch 145 - training loss (MAE): 0.0636, validation MSE: -0.2174
2025-05-03 09:35:07 [INFO]: Epoch 146 - training loss (MAE): 0.0641, validation MSE: -0.2293
2025-05-03 09:35:22 [INFO]: Epoch 147 - training loss (MAE): 0.0635, validation MSE: -0.2161
2025-05-03 09:35:37 [INFO]: Epoch 148 - training loss (MAE): 0.0635, validation MSE: -0.2018
2025-05-03 09:35:52 [INFO]: Epoch 149 - training loss (MAE): 0.0633, validation MSE: -0.2258
2025-05-03 09:36:07 [INFO]: Epoch 150 - training loss (MAE): 0.0630, validation MSE: -0.2268
2025-05-03 09:36:23 [INFO]: Epoch 151 - training loss (MAE): 0.0634, validation MSE: -0.2150
2025-05-03 09:36:38 [INFO]: Epoch 152 - training loss (MAE): 0.0638, validation MSE: -0.2086
2025-05-03 09:36:53 [INFO]: Epoch 153 - training loss (MAE): 0.0631, validation MSE: -0.2124
2025-05-03 09:37:08 [INFO]: Epoch 154 - training loss (MAE): 0.0629, validation MSE: -0.2232
2025-05-03 09:37:23 [INFO]: Epoch 155 - training loss (MAE): 0.0627, validation MSE: -0.2227
2025-05-03 09:37:38 [INFO]: Epoch 156 - training loss (MAE): 0.0635, validation MSE: -0.1983
2025-05-03 09:37:54 [INFO]: Epoch 157 - training loss (MAE): 0.0623, validation MSE: -0.2213
2025-05-03 09:38:08 [INFO]: Epoch 158 - training loss (MAE): 0.0631, validation MSE: -0.2116
2025-05-03 09:38:23 [INFO]: Epoch 159 - training loss (MAE): 0.0626, validation MSE: -0.2309
2025-05-03 09:38:38 [INFO]: Epoch 160 - training loss (MAE): 0.0630, validation MSE: -0.2314
2025-05-03 09:38:53 [INFO]: Epoch 161 - training loss (MAE): 0.0629, validation MSE: -0.2205
2025-05-03 09:39:09 [INFO]: Epoch 162 - training loss (MAE): 0.0630, validation MSE: -0.2096
2025-05-03 09:39:24 [INFO]: Epoch 163 - training loss (MAE): 0.0627, validation MSE: -0.2232
2025-05-03 09:39:39 [INFO]: Epoch 164 - training loss (MAE): 0.0622, validation MSE: -0.2054
2025-05-03 09:39:54 [INFO]: Epoch 165 - training loss (MAE): 0.0620, validation MSE: -0.2207
2025-05-03 09:40:09 [INFO]: Epoch 166 - training loss (MAE): 0.0632, validation MSE: -0.2204
2025-05-03 09:40:24 [INFO]: Epoch 167 - training loss (MAE): 0.0616, validation MSE: -0.1964
2025-05-03 09:40:39 [INFO]: Epoch 168 - training loss (MAE): 0.0621, validation MSE: -0.2120
2025-05-03 09:40:54 [INFO]: Epoch 169 - training loss (MAE): 0.0614, validation MSE: -0.2296
2025-05-03 09:41:09 [INFO]: Epoch 170 - training loss (MAE): 0.0620, validation MSE: -0.2352
2025-05-03 09:41:25 [INFO]: Epoch 171 - training loss (MAE): 0.0619, validation MSE: -0.2431
2025-05-03 09:41:40 [INFO]: Epoch 172 - training loss (MAE): 0.0615, validation MSE: -0.2235
2025-05-03 09:41:55 [INFO]: Epoch 173 - training loss (MAE): 0.0622, validation MSE: -0.2255
2025-05-03 09:42:10 [INFO]: Epoch 174 - training loss (MAE): 0.0622, validation MSE: -0.2306
2025-05-03 09:42:25 [INFO]: Epoch 175 - training loss (MAE): 0.0615, validation MSE: -0.2268
2025-05-03 09:42:40 [INFO]: Epoch 176 - training loss (MAE): 0.0614, validation MSE: -0.2182
2025-05-03 09:42:55 [INFO]: Epoch 177 - training loss (MAE): 0.0617, validation MSE: -0.1939
2025-05-03 09:43:10 [INFO]: Epoch 178 - training loss (MAE): 0.0615, validation MSE: -0.2201
2025-05-03 09:43:26 [INFO]: Epoch 179 - training loss (MAE): 0.0612, validation MSE: -0.2133
2025-05-03 09:43:41 [INFO]: Epoch 180 - training loss (MAE): 0.0617, validation MSE: -0.2097
2025-05-03 09:43:56 [INFO]: Epoch 181 - training loss (MAE): 0.0613, validation MSE: -0.2322
2025-05-03 09:44:12 [INFO]: Epoch 182 - training loss (MAE): 0.0616, validation MSE: -0.2044
2025-05-03 09:44:27 [INFO]: Epoch 183 - training loss (MAE): 0.0613, validation MSE: -0.2033
2025-05-03 09:44:42 [INFO]: Epoch 184 - training loss (MAE): 0.0614, validation MSE: -0.2189
2025-05-03 09:44:57 [INFO]: Epoch 185 - training loss (MAE): 0.0621, validation MSE: -0.2254
2025-05-03 09:45:12 [INFO]: Epoch 186 - training loss (MAE): 0.0609, validation MSE: -0.2239
2025-05-03 09:45:27 [INFO]: Epoch 187 - training loss (MAE): 0.0609, validation MSE: -0.2199
2025-05-03 09:45:42 [INFO]: Epoch 188 - training loss (MAE): 0.0611, validation MSE: -0.2096
2025-05-03 09:45:57 [INFO]: Epoch 189 - training loss (MAE): 0.0613, validation MSE: -0.2060
2025-05-03 09:46:12 [INFO]: Epoch 190 - training loss (MAE): 0.0615, validation MSE: -0.2237
2025-05-03 09:46:28 [INFO]: Epoch 191 - training loss (MAE): 0.0617, validation MSE: -0.2312
2025-05-03 09:46:43 [INFO]: Epoch 192 - training loss (MAE): 0.0615, validation MSE: -0.2078
2025-05-03 09:46:58 [INFO]: Epoch 193 - training loss (MAE): 0.0604, validation MSE: -0.2019
2025-05-03 09:47:13 [INFO]: Epoch 194 - training loss (MAE): 0.0612, validation MSE: -0.2285
2025-05-03 09:47:28 [INFO]: Epoch 195 - training loss (MAE): 0.0608, validation MSE: -0.2254
2025-05-03 09:47:43 [INFO]: Epoch 196 - training loss (MAE): 0.0607, validation MSE: -0.2177
2025-05-03 09:47:58 [INFO]: Epoch 197 - training loss (MAE): 0.0607, validation MSE: -0.2126
2025-05-03 09:48:13 [INFO]: Epoch 198 - training loss (MAE): 0.0610, validation MSE: -0.2350
2025-05-03 09:48:28 [INFO]: Epoch 199 - training loss (MAE): 0.0603, validation MSE: -0.2266
2025-05-03 09:48:43 [INFO]: Epoch 200 - training loss (MAE): 0.0606, validation MSE: -0.2285
2025-05-03 09:48:59 [INFO]: Epoch 201 - training loss (MAE): 0.0610, validation MSE: -0.2160
2025-05-03 09:49:14 [INFO]: Epoch 202 - training loss (MAE): 0.0608, validation MSE: -0.2237
2025-05-03 09:49:29 [INFO]: Epoch 203 - training loss (MAE): 0.0601, validation MSE: -0.2250
2025-05-03 09:49:44 [INFO]: Epoch 204 - training loss (MAE): 0.0604, validation MSE: -0.2171
2025-05-03 09:49:59 [INFO]: Epoch 205 - training loss (MAE): 0.0599, validation MSE: -0.2235
2025-05-03 09:50:14 [INFO]: Epoch 206 - training loss (MAE): 0.0601, validation MSE: -0.2310
2025-05-03 09:50:29 [INFO]: Epoch 207 - training loss (MAE): 0.0608, validation MSE: -0.2370
2025-05-03 09:50:44 [INFO]: Epoch 208 - training loss (MAE): 0.0605, validation MSE: -0.2309
2025-05-03 09:51:00 [INFO]: Epoch 209 - training loss (MAE): 0.0602, validation MSE: -0.2273
2025-05-03 09:51:15 [INFO]: Epoch 210 - training loss (MAE): 0.0610, validation MSE: -0.2318
2025-05-03 09:51:30 [INFO]: Epoch 211 - training loss (MAE): 0.0609, validation MSE: -0.2197
2025-05-03 09:51:45 [INFO]: Epoch 212 - training loss (MAE): 0.0597, validation MSE: -0.2331
2025-05-03 09:52:00 [INFO]: Epoch 213 - training loss (MAE): 0.0599, validation MSE: -0.2417
2025-05-03 09:52:15 [INFO]: Epoch 214 - training loss (MAE): 0.0600, validation MSE: -0.2341
2025-05-03 09:52:31 [INFO]: Epoch 215 - training loss (MAE): 0.0605, validation MSE: -0.2372
2025-05-03 09:52:46 [INFO]: Epoch 216 - training loss (MAE): 0.0597, validation MSE: -0.2252
2025-05-03 09:53:01 [INFO]: Epoch 217 - training loss (MAE): 0.0597, validation MSE: -0.2322
2025-05-03 09:53:16 [INFO]: Epoch 218 - training loss (MAE): 0.0597, validation MSE: -0.2287
2025-05-03 09:53:31 [INFO]: Epoch 219 - training loss (MAE): 0.0604, validation MSE: -0.2383
2025-05-03 09:53:46 [INFO]: Epoch 220 - training loss (MAE): 0.0598, validation MSE: -0.2212
2025-05-03 09:54:01 [INFO]: Epoch 221 - training loss (MAE): 0.0599, validation MSE: -0.2274
2025-05-03 09:54:17 [INFO]: Epoch 222 - training loss (MAE): 0.0601, validation MSE: -0.2078
2025-05-03 09:54:32 [INFO]: Epoch 223 - training loss (MAE): 0.0593, validation MSE: -0.2178
2025-05-03 09:54:47 [INFO]: Epoch 224 - training loss (MAE): 0.0597, validation MSE: -0.2400
2025-05-03 09:55:02 [INFO]: Epoch 225 - training loss (MAE): 0.0601, validation MSE: -0.2097
2025-05-03 09:55:18 [INFO]: Epoch 226 - training loss (MAE): 0.0594, validation MSE: -0.2424
2025-05-03 09:55:33 [INFO]: Epoch 227 - training loss (MAE): 0.0588, validation MSE: -0.2387
2025-05-03 09:55:48 [INFO]: Epoch 228 - training loss (MAE): 0.0592, validation MSE: -0.1716
2025-05-03 09:56:03 [INFO]: Epoch 229 - training loss (MAE): 0.0596, validation MSE: -0.2342
2025-05-03 09:56:18 [INFO]: Epoch 230 - training loss (MAE): 0.0598, validation MSE: -0.2205
2025-05-03 09:56:34 [INFO]: Epoch 231 - training loss (MAE): 0.0597, validation MSE: -0.2290
2025-05-03 09:56:50 [INFO]: Epoch 232 - training loss (MAE): 0.0595, validation MSE: -0.2267
2025-05-03 09:57:05 [INFO]: Epoch 233 - training loss (MAE): 0.0587, validation MSE: -0.1673
2025-05-03 09:57:20 [INFO]: Epoch 234 - training loss (MAE): 0.0599, validation MSE: -0.2203
2025-05-03 09:57:35 [INFO]: Epoch 235 - training loss (MAE): 0.0615, validation MSE: -0.1544
2025-05-03 09:57:50 [INFO]: Epoch 236 - training loss (MAE): 0.0600, validation MSE: -0.1875
2025-05-03 09:58:05 [INFO]: Epoch 237 - training loss (MAE): 0.0598, validation MSE: -0.1792
2025-05-03 09:58:21 [INFO]: Epoch 238 - training loss (MAE): 0.0589, validation MSE: -0.2145
2025-05-03 09:58:36 [INFO]: Epoch 239 - training loss (MAE): 0.0593, validation MSE: -0.2275
2025-05-03 09:58:51 [INFO]: Epoch 240 - training loss (MAE): 0.0589, validation MSE: -0.2279
2025-05-03 09:59:06 [INFO]: Epoch 241 - training loss (MAE): 0.0592, validation MSE: -0.2140
2025-05-03 09:59:21 [INFO]: Epoch 242 - training loss (MAE): 0.0592, validation MSE: -0.2227
2025-05-03 09:59:36 [INFO]: Epoch 243 - training loss (MAE): 0.0594, validation MSE: -0.2192
2025-05-03 09:59:52 [INFO]: Epoch 244 - training loss (MAE): 0.0596, validation MSE: -0.2298
2025-05-03 10:00:07 [INFO]: Epoch 245 - training loss (MAE): 0.0596, validation MSE: -0.2179
2025-05-03 10:00:22 [INFO]: Epoch 246 - training loss (MAE): 0.0594, validation MSE: -0.2306
2025-05-03 10:00:37 [INFO]: Epoch 247 - training loss (MAE): 0.0587, validation MSE: -0.2259
2025-05-03 10:00:52 [INFO]: Epoch 248 - training loss (MAE): 0.0588, validation MSE: -0.2182
2025-05-03 10:01:07 [INFO]: Epoch 249 - training loss (MAE): 0.0583, validation MSE: -0.2321
2025-05-03 10:01:23 [INFO]: Epoch 250 - training loss (MAE): 0.0591, validation MSE: -0.2463
2025-05-03 10:01:38 [INFO]: Epoch 251 - training loss (MAE): 0.0589, validation MSE: -0.2314
2025-05-03 10:01:53 [INFO]: Epoch 252 - training loss (MAE): 0.0588, validation MSE: -0.2322
2025-05-03 10:02:08 [INFO]: Epoch 253 - training loss (MAE): 0.0583, validation MSE: -0.2245
2025-05-03 10:02:23 [INFO]: Epoch 254 - training loss (MAE): 0.0587, validation MSE: -0.2100
2025-05-03 10:02:38 [INFO]: Epoch 255 - training loss (MAE): 0.0587, validation MSE: -0.2190
2025-05-03 10:02:54 [INFO]: Epoch 256 - training loss (MAE): 0.0581, validation MSE: -0.2245
2025-05-03 10:03:09 [INFO]: Epoch 257 - training loss (MAE): 0.0583, validation MSE: -0.2385
2025-05-03 10:03:24 [INFO]: Epoch 258 - training loss (MAE): 0.0586, validation MSE: -0.2285
2025-05-03 10:03:39 [INFO]: Epoch 259 - training loss (MAE): 0.0588, validation MSE: -0.2331
2025-05-03 10:03:54 [INFO]: Epoch 260 - training loss (MAE): 0.0582, validation MSE: -0.2375
2025-05-03 10:04:10 [INFO]: Epoch 261 - training loss (MAE): 0.0586, validation MSE: -0.2323
2025-05-03 10:04:24 [INFO]: Epoch 262 - training loss (MAE): 0.0587, validation MSE: -0.2411
2025-05-03 10:04:40 [INFO]: Epoch 263 - training loss (MAE): 0.0588, validation MSE: -0.2444
2025-05-03 10:04:55 [INFO]: Epoch 264 - training loss (MAE): 0.0588, validation MSE: -0.2229
2025-05-03 10:05:10 [INFO]: Epoch 265 - training loss (MAE): 0.0583, validation MSE: -0.2306
2025-05-03 10:05:25 [INFO]: Epoch 266 - training loss (MAE): 0.0582, validation MSE: -0.2332
2025-05-03 10:05:40 [INFO]: Epoch 267 - training loss (MAE): 0.0587, validation MSE: -0.2440
2025-05-03 10:05:55 [INFO]: Epoch 268 - training loss (MAE): 0.0579, validation MSE: -0.2496
2025-05-03 10:06:10 [INFO]: Epoch 269 - training loss (MAE): 0.0582, validation MSE: -0.2555
2025-05-03 10:06:25 [INFO]: Epoch 270 - training loss (MAE): 0.0585, validation MSE: -0.2119
2025-05-03 10:06:40 [INFO]: Epoch 271 - training loss (MAE): 0.0584, validation MSE: -0.2414
2025-05-03 10:06:56 [INFO]: Epoch 272 - training loss (MAE): 0.0580, validation MSE: -0.2433
2025-05-03 10:07:11 [INFO]: Epoch 273 - training loss (MAE): 0.0583, validation MSE: -0.2222
2025-05-03 10:07:26 [INFO]: Epoch 274 - training loss (MAE): 0.0586, validation MSE: -0.2473
2025-05-03 10:07:41 [INFO]: Epoch 275 - training loss (MAE): 0.0587, validation MSE: -0.2060
2025-05-03 10:07:56 [INFO]: Epoch 276 - training loss (MAE): 0.0578, validation MSE: -0.2277
2025-05-03 10:08:11 [INFO]: Epoch 277 - training loss (MAE): 0.0579, validation MSE: -0.2178
2025-05-03 10:08:26 [INFO]: Epoch 278 - training loss (MAE): 0.0586, validation MSE: -0.2247
2025-05-03 10:08:42 [INFO]: Epoch 279 - training loss (MAE): 0.0584, validation MSE: -0.1970
2025-05-03 10:08:57 [INFO]: Epoch 280 - training loss (MAE): 0.0582, validation MSE: -0.2359
2025-05-03 10:09:12 [INFO]: Epoch 281 - training loss (MAE): 0.0576, validation MSE: -0.2332
2025-05-03 10:09:27 [INFO]: Epoch 282 - training loss (MAE): 0.0577, validation MSE: -0.2413
2025-05-03 10:09:42 [INFO]: Epoch 283 - training loss (MAE): 0.0581, validation MSE: -0.2255
2025-05-03 10:09:57 [INFO]: Epoch 284 - training loss (MAE): 0.0579, validation MSE: -0.2350
2025-05-03 10:10:12 [INFO]: Epoch 285 - training loss (MAE): 0.0571, validation MSE: -0.2285
2025-05-03 10:10:28 [INFO]: Epoch 286 - training loss (MAE): 0.0574, validation MSE: -0.2415
2025-05-03 10:10:43 [INFO]: Epoch 287 - training loss (MAE): 0.0575, validation MSE: -0.2321
2025-05-03 10:10:58 [INFO]: Epoch 288 - training loss (MAE): 0.0578, validation MSE: -0.2366
2025-05-03 10:11:13 [INFO]: Epoch 289 - training loss (MAE): 0.0579, validation MSE: -0.2140
2025-05-03 10:11:28 [INFO]: Epoch 290 - training loss (MAE): 0.0580, validation MSE: -0.2150
2025-05-03 10:11:43 [INFO]: Epoch 291 - training loss (MAE): 0.0573, validation MSE: -0.2254
2025-05-03 10:11:58 [INFO]: Epoch 292 - training loss (MAE): 0.0582, validation MSE: -0.2403
2025-05-03 10:12:14 [INFO]: Epoch 293 - training loss (MAE): 0.0577, validation MSE: -0.2245
2025-05-03 10:12:29 [INFO]: Epoch 294 - training loss (MAE): 0.0577, validation MSE: -0.2378
2025-05-03 10:12:44 [INFO]: Epoch 295 - training loss (MAE): 0.0578, validation MSE: -0.2165
2025-05-03 10:12:59 [INFO]: Epoch 296 - training loss (MAE): 0.0576, validation MSE: -0.2431
2025-05-03 10:13:14 [INFO]: Epoch 297 - training loss (MAE): 0.0575, validation MSE: -0.2408
2025-05-03 10:13:29 [INFO]: Epoch 298 - training loss (MAE): 0.0576, validation MSE: -0.2218
2025-05-03 10:13:44 [INFO]: Epoch 299 - training loss (MAE): 0.0572, validation MSE: -0.2251
2025-05-03 10:13:59 [INFO]: Epoch 300 - training loss (MAE): 0.0572, validation MSE: -0.2349
2025-05-03 10:14:15 [INFO]: Epoch 301 - training loss (MAE): 0.0575, validation MSE: -0.2328
2025-05-03 10:14:29 [INFO]: Epoch 302 - training loss (MAE): 0.0573, validation MSE: -0.2364
2025-05-03 10:14:45 [INFO]: Epoch 303 - training loss (MAE): 0.0573, validation MSE: -0.2343
2025-05-03 10:15:00 [INFO]: Epoch 304 - training loss (MAE): 0.0575, validation MSE: -0.2381
2025-05-03 10:15:15 [INFO]: Epoch 305 - training loss (MAE): 0.0571, validation MSE: -0.2326
2025-05-03 10:15:30 [INFO]: Epoch 306 - training loss (MAE): 0.0573, validation MSE: -0.2289
2025-05-03 10:15:45 [INFO]: Epoch 307 - training loss (MAE): 0.0575, validation MSE: -0.2343
2025-05-03 10:16:00 [INFO]: Epoch 308 - training loss (MAE): 0.0569, validation MSE: -0.2104
2025-05-03 10:16:16 [INFO]: Epoch 309 - training loss (MAE): 0.0578, validation MSE: -0.2492
2025-05-03 10:16:31 [INFO]: Epoch 310 - training loss (MAE): 0.0574, validation MSE: -0.2214
2025-05-03 10:16:46 [INFO]: Epoch 311 - training loss (MAE): 0.0574, validation MSE: -0.2356
2025-05-03 10:17:01 [INFO]: Epoch 312 - training loss (MAE): 0.0578, validation MSE: -0.2317
2025-05-03 10:17:17 [INFO]: Epoch 313 - training loss (MAE): 0.0571, validation MSE: -0.2306
2025-05-03 10:17:32 [INFO]: Epoch 314 - training loss (MAE): 0.0567, validation MSE: -0.1806
2025-05-03 10:17:47 [INFO]: Epoch 315 - training loss (MAE): 0.0575, validation MSE: -0.2324
2025-05-03 10:18:02 [INFO]: Epoch 316 - training loss (MAE): 0.0572, validation MSE: -0.2348
2025-05-03 10:18:17 [INFO]: Epoch 317 - training loss (MAE): 0.0568, validation MSE: -0.2178
2025-05-03 10:18:32 [INFO]: Epoch 318 - training loss (MAE): 0.0569, validation MSE: -0.2344
2025-05-03 10:18:47 [INFO]: Epoch 319 - training loss (MAE): 0.0576, validation MSE: -0.2094
2025-05-03 10:19:02 [INFO]: Epoch 320 - training loss (MAE): 0.0565, validation MSE: -0.2311
2025-05-03 10:19:17 [INFO]: Epoch 321 - training loss (MAE): 0.0571, validation MSE: -0.2456
2025-05-03 10:19:33 [INFO]: Epoch 322 - training loss (MAE): 0.0568, validation MSE: -0.2329
2025-05-03 10:19:48 [INFO]: Epoch 323 - training loss (MAE): 0.0563, validation MSE: -0.2144
2025-05-03 10:20:03 [INFO]: Epoch 324 - training loss (MAE): 0.0573, validation MSE: -0.2010
2025-05-03 10:20:18 [INFO]: Epoch 325 - training loss (MAE): 0.0565, validation MSE: -0.2425
2025-05-03 10:20:33 [INFO]: Epoch 326 - training loss (MAE): 0.0566, validation MSE: -0.2512
2025-05-03 10:20:49 [INFO]: Epoch 327 - training loss (MAE): 0.0577, validation MSE: -0.1511
2025-05-03 10:21:04 [INFO]: Epoch 328 - training loss (MAE): 0.0572, validation MSE: -0.2373
2025-05-03 10:21:19 [INFO]: Epoch 329 - training loss (MAE): 0.0572, validation MSE: -0.2274
2025-05-03 10:21:34 [INFO]: Epoch 330 - training loss (MAE): 0.0568, validation MSE: -0.2282
2025-05-03 10:21:49 [INFO]: Epoch 331 - training loss (MAE): 0.0568, validation MSE: -0.2372
2025-05-03 10:22:04 [INFO]: Epoch 332 - training loss (MAE): 0.0570, validation MSE: -0.1908
2025-05-03 10:22:19 [INFO]: Epoch 333 - training loss (MAE): 0.0567, validation MSE: -0.2363
2025-05-03 10:22:34 [INFO]: Epoch 334 - training loss (MAE): 0.0569, validation MSE: -0.2288
2025-05-03 10:22:49 [INFO]: Epoch 335 - training loss (MAE): 0.0566, validation MSE: -0.2395
2025-05-03 10:23:05 [INFO]: Epoch 336 - training loss (MAE): 0.0561, validation MSE: -0.1964
2025-05-03 10:23:20 [INFO]: Epoch 337 - training loss (MAE): 0.0573, validation MSE: -0.2300
2025-05-03 10:23:35 [INFO]: Epoch 338 - training loss (MAE): 0.0561, validation MSE: -0.2271
2025-05-03 10:23:50 [INFO]: Epoch 339 - training loss (MAE): 0.0562, validation MSE: -0.2368
2025-05-03 10:24:05 [INFO]: Epoch 340 - training loss (MAE): 0.0565, validation MSE: -0.2039
2025-05-03 10:24:20 [INFO]: Epoch 341 - training loss (MAE): 0.0564, validation MSE: -0.2421
2025-05-03 10:24:35 [INFO]: Epoch 342 - training loss (MAE): 0.0572, validation MSE: -0.2324
2025-05-03 10:24:50 [INFO]: Epoch 343 - training loss (MAE): 0.0565, validation MSE: -0.2190
2025-05-03 10:25:06 [INFO]: Epoch 344 - training loss (MAE): 0.0561, validation MSE: -0.2449
2025-05-03 10:25:21 [INFO]: Epoch 345 - training loss (MAE): 0.0564, validation MSE: -0.2213
2025-05-03 10:25:36 [INFO]: Epoch 346 - training loss (MAE): 0.0566, validation MSE: -0.2342
2025-05-03 10:25:51 [INFO]: Epoch 347 - training loss (MAE): 0.0563, validation MSE: -0.2013
2025-05-03 10:26:06 [INFO]: Epoch 348 - training loss (MAE): 0.0567, validation MSE: -0.2430
2025-05-03 10:26:21 [INFO]: Epoch 349 - training loss (MAE): 0.0565, validation MSE: -0.2353
2025-05-03 10:26:37 [INFO]: Epoch 350 - training loss (MAE): 0.0558, validation MSE: -0.2502
2025-05-03 10:26:52 [INFO]: Epoch 351 - training loss (MAE): 0.0558, validation MSE: -0.2538
2025-05-03 10:27:07 [INFO]: Epoch 352 - training loss (MAE): 0.0561, validation MSE: -0.2238
2025-05-03 10:27:22 [INFO]: Epoch 353 - training loss (MAE): 0.0565, validation MSE: -0.2211
2025-05-03 10:27:37 [INFO]: Epoch 354 - training loss (MAE): 0.0568, validation MSE: -0.2370
2025-05-03 10:27:52 [INFO]: Epoch 355 - training loss (MAE): 0.0563, validation MSE: -0.2399
2025-05-03 10:28:08 [INFO]: Epoch 356 - training loss (MAE): 0.0557, validation MSE: -0.2448
2025-05-03 10:28:23 [INFO]: Epoch 357 - training loss (MAE): 0.0567, validation MSE: -0.2380
2025-05-03 10:28:38 [INFO]: Epoch 358 - training loss (MAE): 0.0563, validation MSE: -0.2238
2025-05-03 10:28:53 [INFO]: Epoch 359 - training loss (MAE): 0.0564, validation MSE: -0.2153
2025-05-03 10:29:08 [INFO]: Epoch 360 - training loss (MAE): 0.0563, validation MSE: -0.2219
2025-05-03 10:29:23 [INFO]: Epoch 361 - training loss (MAE): 0.0558, validation MSE: -0.2022
2025-05-03 10:29:38 [INFO]: Epoch 362 - training loss (MAE): 0.0557, validation MSE: -0.2228
2025-05-03 10:29:53 [INFO]: Epoch 363 - training loss (MAE): 0.0563, validation MSE: -0.2225
2025-05-03 10:30:08 [INFO]: Epoch 364 - training loss (MAE): 0.0562, validation MSE: -0.2149
2025-05-03 10:30:23 [INFO]: Epoch 365 - training loss (MAE): 0.0558, validation MSE: -0.2105
2025-05-03 10:30:38 [INFO]: Epoch 366 - training loss (MAE): 0.0560, validation MSE: -0.2327
2025-05-03 10:30:54 [INFO]: Epoch 367 - training loss (MAE): 0.0557, validation MSE: -0.2285
2025-05-03 10:31:09 [INFO]: Epoch 368 - training loss (MAE): 0.0556, validation MSE: -0.2336
2025-05-03 10:31:24 [INFO]: Epoch 369 - training loss (MAE): 0.0554, validation MSE: -0.2421
2025-05-03 10:31:39 [INFO]: Epoch 370 - training loss (MAE): 0.0562, validation MSE: -0.2170
2025-05-03 10:31:54 [INFO]: Epoch 371 - training loss (MAE): 0.0559, validation MSE: -0.2420
2025-05-03 10:32:09 [INFO]: Epoch 372 - training loss (MAE): 0.0560, validation MSE: -0.2208
2025-05-03 10:32:24 [INFO]: Epoch 373 - training loss (MAE): 0.0557, validation MSE: -0.2528
2025-05-03 10:32:39 [INFO]: Epoch 374 - training loss (MAE): 0.0558, validation MSE: -0.2446
2025-05-03 10:32:55 [INFO]: Epoch 375 - training loss (MAE): 0.0558, validation MSE: -0.2253
2025-05-03 10:33:10 [INFO]: Epoch 376 - training loss (MAE): 0.0555, validation MSE: -0.2374
2025-05-03 10:33:25 [INFO]: Epoch 377 - training loss (MAE): 0.0560, validation MSE: -0.2333
2025-05-03 10:33:40 [INFO]: Epoch 378 - training loss (MAE): 0.0556, validation MSE: -0.2394
2025-05-03 10:33:55 [INFO]: Epoch 379 - training loss (MAE): 0.0552, validation MSE: -0.2439
2025-05-03 10:34:10 [INFO]: Epoch 380 - training loss (MAE): 0.0555, validation MSE: -0.2418
2025-05-03 10:34:25 [INFO]: Epoch 381 - training loss (MAE): 0.0557, validation MSE: -0.2413
2025-05-03 10:34:41 [INFO]: Epoch 382 - training loss (MAE): 0.0556, validation MSE: -0.2429
2025-05-03 10:34:56 [INFO]: Epoch 383 - training loss (MAE): 0.0559, validation MSE: -0.2295
2025-05-03 10:35:11 [INFO]: Epoch 384 - training loss (MAE): 0.0547, validation MSE: -0.2405
2025-05-03 10:35:26 [INFO]: Epoch 385 - training loss (MAE): 0.0549, validation MSE: -0.2445
2025-05-03 10:35:41 [INFO]: Epoch 386 - training loss (MAE): 0.0551, validation MSE: -0.2422
2025-05-03 10:35:56 [INFO]: Epoch 387 - training loss (MAE): 0.0551, validation MSE: -0.2432
2025-05-03 10:36:11 [INFO]: Epoch 388 - training loss (MAE): 0.0550, validation MSE: -0.2378
2025-05-03 10:36:26 [INFO]: Epoch 389 - training loss (MAE): 0.0547, validation MSE: -0.2422
2025-05-03 10:36:41 [INFO]: Epoch 390 - training loss (MAE): 0.0556, validation MSE: -0.2401
2025-05-03 10:36:57 [INFO]: Epoch 391 - training loss (MAE): 0.0562, validation MSE: -0.2212
2025-05-03 10:37:12 [INFO]: Epoch 392 - training loss (MAE): 0.0557, validation MSE: -0.2320
2025-05-03 10:37:27 [INFO]: Epoch 393 - training loss (MAE): 0.0550, validation MSE: -0.2143
2025-05-03 10:37:42 [INFO]: Epoch 394 - training loss (MAE): 0.0547, validation MSE: -0.2359
2025-05-03 10:37:57 [INFO]: Epoch 395 - training loss (MAE): 0.0549, validation MSE: -0.2470
2025-05-03 10:38:12 [INFO]: Epoch 396 - training loss (MAE): 0.0544, validation MSE: -0.2154
2025-05-03 10:38:28 [INFO]: Epoch 397 - training loss (MAE): 0.0548, validation MSE: -0.2323
2025-05-03 10:38:43 [INFO]: Epoch 398 - training loss (MAE): 0.0550, validation MSE: -0.2312
2025-05-03 10:38:58 [INFO]: Epoch 399 - training loss (MAE): 0.0552, validation MSE: -0.2451
2025-05-03 10:39:13 [INFO]: Epoch 400 - training loss (MAE): 0.0546, validation MSE: -0.2360
2025-05-03 10:39:28 [INFO]: Epoch 401 - training loss (MAE): 0.0547, validation MSE: -0.2447
2025-05-03 10:39:43 [INFO]: Epoch 402 - training loss (MAE): 0.0544, validation MSE: -0.2114
2025-05-03 10:39:58 [INFO]: Epoch 403 - training loss (MAE): 0.0543, validation MSE: -0.2462
2025-05-03 10:40:14 [INFO]: Epoch 404 - training loss (MAE): 0.0546, validation MSE: -0.2194
2025-05-03 10:40:29 [INFO]: Epoch 405 - training loss (MAE): 0.0542, validation MSE: -0.2427
2025-05-03 10:40:44 [INFO]: Epoch 406 - training loss (MAE): 0.0545, validation MSE: -0.2400
2025-05-03 10:40:59 [INFO]: Epoch 407 - training loss (MAE): 0.0543, validation MSE: -0.2311
2025-05-03 10:41:14 [INFO]: Epoch 408 - training loss (MAE): 0.0548, validation MSE: -0.2282
2025-05-03 10:41:29 [INFO]: Epoch 409 - training loss (MAE): 0.0541, validation MSE: -0.2366
2025-05-03 10:41:44 [INFO]: Epoch 410 - training loss (MAE): 0.0544, validation MSE: -0.2536
2025-05-03 10:41:59 [INFO]: Epoch 411 - training loss (MAE): 0.0544, validation MSE: -0.2090
2025-05-03 10:42:15 [INFO]: Epoch 412 - training loss (MAE): 0.0541, validation MSE: -0.2345
2025-05-03 10:42:30 [INFO]: Epoch 413 - training loss (MAE): 0.0537, validation MSE: -0.2451
2025-05-03 10:42:45 [INFO]: Epoch 414 - training loss (MAE): 0.0545, validation MSE: -0.2382
2025-05-03 10:43:00 [INFO]: Epoch 415 - training loss (MAE): 0.0536, validation MSE: -0.2361
2025-05-03 10:43:15 [INFO]: Epoch 416 - training loss (MAE): 0.0537, validation MSE: -0.2145
2025-05-03 10:43:30 [INFO]: Epoch 417 - training loss (MAE): 0.0540, validation MSE: -0.2451
2025-05-03 10:43:45 [INFO]: Epoch 418 - training loss (MAE): 0.0543, validation MSE: -0.2362
2025-05-03 10:44:00 [INFO]: Epoch 419 - training loss (MAE): 0.0540, validation MSE: -0.2172
2025-05-03 10:44:15 [INFO]: Epoch 420 - training loss (MAE): 0.0544, validation MSE: -0.2554
2025-05-03 10:44:30 [INFO]: Epoch 421 - training loss (MAE): 0.0534, validation MSE: -0.2331
2025-05-03 10:44:45 [INFO]: Epoch 422 - training loss (MAE): 0.0536, validation MSE: -0.2469
2025-05-03 10:45:01 [INFO]: Epoch 423 - training loss (MAE): 0.0546, validation MSE: -0.2290
2025-05-03 10:45:16 [INFO]: Epoch 424 - training loss (MAE): 0.0535, validation MSE: -0.2386
2025-05-03 10:45:31 [INFO]: Epoch 425 - training loss (MAE): 0.0537, validation MSE: -0.2507
2025-05-03 10:45:46 [INFO]: Epoch 426 - training loss (MAE): 0.0541, validation MSE: -0.2353
2025-05-03 10:46:01 [INFO]: Epoch 427 - training loss (MAE): 0.0536, validation MSE: -0.2358
2025-05-03 10:46:16 [INFO]: Epoch 428 - training loss (MAE): 0.0538, validation MSE: -0.2464
2025-05-03 10:46:32 [INFO]: Epoch 429 - training loss (MAE): 0.0533, validation MSE: -0.2212
2025-05-03 10:46:47 [INFO]: Epoch 430 - training loss (MAE): 0.0540, validation MSE: -0.2393
2025-05-03 10:47:02 [INFO]: Epoch 431 - training loss (MAE): 0.0538, validation MSE: -0.2326
2025-05-03 10:47:17 [INFO]: Epoch 432 - training loss (MAE): 0.0538, validation MSE: -0.2268
2025-05-03 10:47:32 [INFO]: Epoch 433 - training loss (MAE): 0.0532, validation MSE: -0.2374
2025-05-03 10:47:47 [INFO]: Epoch 434 - training loss (MAE): 0.0530, validation MSE: -0.2448
2025-05-03 10:48:02 [INFO]: Epoch 435 - training loss (MAE): 0.0536, validation MSE: -0.2366
2025-05-03 10:48:18 [INFO]: Epoch 436 - training loss (MAE): 0.0530, validation MSE: -0.2334
2025-05-03 10:48:33 [INFO]: Epoch 437 - training loss (MAE): 0.0536, validation MSE: -0.2272
2025-05-03 10:48:48 [INFO]: Epoch 438 - training loss (MAE): 0.0525, validation MSE: -0.2442
2025-05-03 10:49:03 [INFO]: Epoch 439 - training loss (MAE): 0.0530, validation MSE: -0.2195
2025-05-03 10:49:18 [INFO]: Epoch 440 - training loss (MAE): 0.0530, validation MSE: -0.2450
2025-05-03 10:49:34 [INFO]: Epoch 441 - training loss (MAE): 0.0529, validation MSE: -0.2358
2025-05-03 10:49:49 [INFO]: Epoch 442 - training loss (MAE): 0.0529, validation MSE: -0.2467
2025-05-03 10:50:04 [INFO]: Epoch 443 - training loss (MAE): 0.0534, validation MSE: -0.2374
2025-05-03 10:50:19 [INFO]: Epoch 444 - training loss (MAE): 0.0532, validation MSE: -0.2340
2025-05-03 10:50:34 [INFO]: Epoch 445 - training loss (MAE): 0.0523, validation MSE: -0.2427
2025-05-03 10:50:50 [INFO]: Epoch 446 - training loss (MAE): 0.0529, validation MSE: -0.2412
2025-05-03 10:51:05 [INFO]: Epoch 447 - training loss (MAE): 0.0527, validation MSE: -0.2481
2025-05-03 10:51:20 [INFO]: Epoch 448 - training loss (MAE): 0.0529, validation MSE: -0.2471
2025-05-03 10:51:35 [INFO]: Epoch 449 - training loss (MAE): 0.0526, validation MSE: -0.2407
2025-05-03 10:51:50 [INFO]: Epoch 450 - training loss (MAE): 0.0528, validation MSE: -0.2384
2025-05-03 10:52:06 [INFO]: Epoch 451 - training loss (MAE): 0.0534, validation MSE: -0.2498
2025-05-03 10:52:21 [INFO]: Epoch 452 - training loss (MAE): 0.0526, validation MSE: -0.2603
2025-05-03 10:52:36 [INFO]: Epoch 453 - training loss (MAE): 0.0526, validation MSE: -0.2367
2025-05-03 10:52:51 [INFO]: Epoch 454 - training loss (MAE): 0.0532, validation MSE: -0.2476
2025-05-03 10:53:06 [INFO]: Epoch 455 - training loss (MAE): 0.0537, validation MSE: -0.2101
2025-05-03 10:53:21 [INFO]: Epoch 456 - training loss (MAE): 0.0525, validation MSE: -0.2436
2025-05-03 10:53:36 [INFO]: Epoch 457 - training loss (MAE): 0.0519, validation MSE: -0.2530
2025-05-03 10:53:51 [INFO]: Epoch 458 - training loss (MAE): 0.0523, validation MSE: -0.2541
2025-05-03 10:54:06 [INFO]: Epoch 459 - training loss (MAE): 0.0524, validation MSE: -0.2538
2025-05-03 10:54:22 [INFO]: Epoch 460 - training loss (MAE): 0.0526, validation MSE: -0.2488
2025-05-03 10:54:37 [INFO]: Epoch 461 - training loss (MAE): 0.0526, validation MSE: -0.2476
2025-05-03 10:54:52 [INFO]: Epoch 462 - training loss (MAE): 0.0523, validation MSE: -0.2116
2025-05-03 10:55:07 [INFO]: Epoch 463 - training loss (MAE): 0.0523, validation MSE: -0.2138
2025-05-03 10:55:23 [INFO]: Epoch 464 - training loss (MAE): 0.0524, validation MSE: -0.2478
2025-05-03 10:55:38 [INFO]: Epoch 465 - training loss (MAE): 0.0525, validation MSE: -0.2282
2025-05-03 10:55:53 [INFO]: Epoch 466 - training loss (MAE): 0.0524, validation MSE: -0.2480
2025-05-03 10:56:08 [INFO]: Epoch 467 - training loss (MAE): 0.0517, validation MSE: -0.2337
2025-05-03 10:56:23 [INFO]: Epoch 468 - training loss (MAE): 0.0527, validation MSE: -0.2571
2025-05-03 10:56:38 [INFO]: Epoch 469 - training loss (MAE): 0.0523, validation MSE: -0.2253
2025-05-03 10:56:54 [INFO]: Epoch 470 - training loss (MAE): 0.0518, validation MSE: -0.1930
2025-05-03 10:57:09 [INFO]: Epoch 471 - training loss (MAE): 0.0522, validation MSE: -0.2274
2025-05-03 10:57:24 [INFO]: Epoch 472 - training loss (MAE): 0.0519, validation MSE: -0.2237
2025-05-03 10:57:40 [INFO]: Epoch 473 - training loss (MAE): 0.0515, validation MSE: -0.2338
2025-05-03 10:57:55 [INFO]: Epoch 474 - training loss (MAE): 0.0525, validation MSE: -0.2414
2025-05-03 10:58:10 [INFO]: Epoch 475 - training loss (MAE): 0.0520, validation MSE: -0.2212
2025-05-03 10:58:25 [INFO]: Epoch 476 - training loss (MAE): 0.0520, validation MSE: -0.2492
2025-05-03 10:58:40 [INFO]: Epoch 477 - training loss (MAE): 0.0519, validation MSE: -0.2384
2025-05-03 10:58:56 [INFO]: Epoch 478 - training loss (MAE): 0.0516, validation MSE: -0.2400
2025-05-03 10:59:11 [INFO]: Epoch 479 - training loss (MAE): 0.0527, validation MSE: -0.2136
2025-05-03 10:59:26 [INFO]: Epoch 480 - training loss (MAE): 0.0516, validation MSE: -0.2483
2025-05-03 10:59:41 [INFO]: Epoch 481 - training loss (MAE): 0.0518, validation MSE: -0.2414
2025-05-03 10:59:56 [INFO]: Epoch 482 - training loss (MAE): 0.0519, validation MSE: -0.2226
2025-05-03 11:00:11 [INFO]: Epoch 483 - training loss (MAE): 0.0520, validation MSE: -0.2337
2025-05-03 11:00:27 [INFO]: Epoch 484 - training loss (MAE): 0.0512, validation MSE: -0.2295
2025-05-03 11:00:42 [INFO]: Epoch 485 - training loss (MAE): 0.0528, validation MSE: -0.2352
2025-05-03 11:00:58 [INFO]: Epoch 486 - training loss (MAE): 0.0514, validation MSE: -0.2518
2025-05-03 11:01:13 [INFO]: Epoch 487 - training loss (MAE): 0.0518, validation MSE: -0.2289
2025-05-03 11:01:28 [INFO]: Epoch 488 - training loss (MAE): 0.0509, validation MSE: -0.2357
2025-05-03 11:01:43 [INFO]: Epoch 489 - training loss (MAE): 0.0514, validation MSE: -0.2384
2025-05-03 11:01:58 [INFO]: Epoch 490 - training loss (MAE): 0.0513, validation MSE: -0.2372
2025-05-03 11:02:13 [INFO]: Epoch 491 - training loss (MAE): 0.0512, validation MSE: -0.2566
2025-05-03 11:02:29 [INFO]: Epoch 492 - training loss (MAE): 0.0521, validation MSE: -0.2350
2025-05-03 11:02:44 [INFO]: Epoch 493 - training loss (MAE): 0.0511, validation MSE: -0.2463
2025-05-03 11:02:59 [INFO]: Epoch 494 - training loss (MAE): 0.0516, validation MSE: -0.2285
2025-05-03 11:03:14 [INFO]: Epoch 495 - training loss (MAE): 0.0516, validation MSE: -0.2454
2025-05-03 11:03:29 [INFO]: Epoch 496 - training loss (MAE): 0.0512, validation MSE: -0.2370
2025-05-03 11:03:44 [INFO]: Epoch 497 - training loss (MAE): 0.0513, validation MSE: -0.2392
2025-05-03 11:03:59 [INFO]: Epoch 498 - training loss (MAE): 0.0512, validation MSE: -0.2437
2025-05-03 11:04:14 [INFO]: Epoch 499 - training loss (MAE): 0.0515, validation MSE: -0.2462
2025-05-03 11:04:30 [INFO]: Epoch 500 - training loss (MAE): 0.0511, validation MSE: -0.2344
2025-05-03 11:04:30 [INFO]: Finished training. The best model is from epoch#452.
2025-05-03 11:04:30 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_9.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 11:04:30 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_9.pypots
Fold 9 metrics: MAE: 0.258, MSE: 0.270, MRE: 0.350
Fold 9 metrics: MAE: 0.258, MSE: 0.270, MRE: 0.350
Training fold 10/10
2025-05-03 11:04:30 [INFO]: No given device, using default device: cuda
2025-05-03 11:04:30 [WARNING]: ‚ÄºÔ∏è saving_path not given. Model files and tensorboard file will not be saved.
2025-05-03 11:04:30 [INFO]: Using customized MAE as the training loss function.
2025-05-03 11:04:30 [INFO]: Using customized MSE as the validation metric function.
2025-05-03 11:04:30 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,318,906
2025-05-03 11:04:45 [INFO]: Epoch 001 - training loss (MAE): 1.2394, validation MSE: 5.0667
2025-05-03 11:05:00 [INFO]: Epoch 002 - training loss (MAE): 0.7035, validation MSE: 6.0492
2025-05-03 11:05:15 [INFO]: Epoch 003 - training loss (MAE): 0.6304, validation MSE: 7.3587
2025-05-03 11:05:30 [INFO]: Epoch 004 - training loss (MAE): 0.5897, validation MSE: 8.9919
2025-05-03 11:05:45 [INFO]: Epoch 005 - training loss (MAE): 0.5560, validation MSE: 10.4201
2025-05-03 11:06:01 [INFO]: Epoch 006 - training loss (MAE): 0.5266, validation MSE: 11.5647
2025-05-03 11:06:16 [INFO]: Epoch 007 - training loss (MAE): 0.5052, validation MSE: 12.5752
2025-05-03 11:06:31 [INFO]: Epoch 008 - training loss (MAE): 0.4866, validation MSE: 13.4596
2025-05-03 11:06:46 [INFO]: Epoch 009 - training loss (MAE): 0.4709, validation MSE: 13.5083
2025-05-03 11:07:01 [INFO]: Epoch 010 - training loss (MAE): 0.4597, validation MSE: 14.5836
2025-05-03 11:07:16 [INFO]: Epoch 011 - training loss (MAE): 0.4497, validation MSE: 14.9946
2025-05-03 11:07:31 [INFO]: Epoch 012 - training loss (MAE): 0.4405, validation MSE: 14.8531
2025-05-03 11:07:46 [INFO]: Epoch 013 - training loss (MAE): 0.4311, validation MSE: 14.1684
2025-05-03 11:08:02 [INFO]: Epoch 014 - training loss (MAE): 0.4235, validation MSE: 14.4418
2025-05-03 11:08:17 [INFO]: Epoch 015 - training loss (MAE): 0.4165, validation MSE: 14.3209
2025-05-03 11:08:32 [INFO]: Epoch 016 - training loss (MAE): 0.4109, validation MSE: 14.0493
2025-05-03 11:08:47 [INFO]: Epoch 017 - training loss (MAE): 0.4068, validation MSE: 13.9560
2025-05-03 11:09:02 [INFO]: Epoch 018 - training loss (MAE): 0.4029, validation MSE: 14.0679
2025-05-03 11:09:17 [INFO]: Epoch 019 - training loss (MAE): 0.3993, validation MSE: 14.1082
2025-05-03 11:09:33 [INFO]: Epoch 020 - training loss (MAE): 0.3952, validation MSE: 14.6794
2025-05-03 11:09:48 [INFO]: Epoch 021 - training loss (MAE): 0.3934, validation MSE: 14.2168
2025-05-03 11:10:03 [INFO]: Epoch 022 - training loss (MAE): 0.3898, validation MSE: 14.3264
2025-05-03 11:10:18 [INFO]: Epoch 023 - training loss (MAE): 0.3881, validation MSE: 14.3046
2025-05-03 11:10:33 [INFO]: Epoch 024 - training loss (MAE): 0.3856, validation MSE: 14.5460
2025-05-03 11:10:48 [INFO]: Epoch 025 - training loss (MAE): 0.3839, validation MSE: 14.3431
2025-05-03 11:11:03 [INFO]: Epoch 026 - training loss (MAE): 0.3823, validation MSE: 14.3933
2025-05-03 11:11:18 [INFO]: Epoch 027 - training loss (MAE): 0.3803, validation MSE: 13.9048
2025-05-03 11:11:33 [INFO]: Epoch 028 - training loss (MAE): 0.3785, validation MSE: 14.7008
2025-05-03 11:11:49 [INFO]: Epoch 029 - training loss (MAE): 0.3774, validation MSE: 14.3645
2025-05-03 11:12:04 [INFO]: Epoch 030 - training loss (MAE): 0.3765, validation MSE: 14.4503
2025-05-03 11:12:19 [INFO]: Epoch 031 - training loss (MAE): 0.3748, validation MSE: 14.2462
2025-05-03 11:12:35 [INFO]: Epoch 032 - training loss (MAE): 0.3735, validation MSE: 14.1791
2025-05-03 11:12:49 [INFO]: Epoch 033 - training loss (MAE): 0.3728, validation MSE: 14.5240
2025-05-03 11:13:04 [INFO]: Epoch 034 - training loss (MAE): 0.3713, validation MSE: 14.1993
2025-05-03 11:13:20 [INFO]: Epoch 035 - training loss (MAE): 0.3710, validation MSE: 14.4922
2025-05-03 11:13:35 [INFO]: Epoch 036 - training loss (MAE): 0.3697, validation MSE: 14.8943
2025-05-03 11:13:50 [INFO]: Epoch 037 - training loss (MAE): 0.3688, validation MSE: 14.7601
2025-05-03 11:14:05 [INFO]: Epoch 038 - training loss (MAE): 0.3690, validation MSE: 15.1130
2025-05-03 11:14:20 [INFO]: Epoch 039 - training loss (MAE): 0.3676, validation MSE: 14.6260
2025-05-03 11:14:35 [INFO]: Epoch 040 - training loss (MAE): 0.3664, validation MSE: 15.1766
2025-05-03 11:14:50 [INFO]: Epoch 041 - training loss (MAE): 0.3662, validation MSE: 14.6987
2025-05-03 11:15:05 [INFO]: Epoch 042 - training loss (MAE): 0.3663, validation MSE: 14.8508
2025-05-03 11:15:20 [INFO]: Epoch 043 - training loss (MAE): 0.3653, validation MSE: 14.4985
2025-05-03 11:15:35 [INFO]: Epoch 044 - training loss (MAE): 0.3648, validation MSE: 14.7930
2025-05-03 11:15:50 [INFO]: Epoch 045 - training loss (MAE): 0.3640, validation MSE: 14.9083
2025-05-03 11:16:05 [INFO]: Epoch 046 - training loss (MAE): 0.3642, validation MSE: 14.5892
2025-05-03 11:16:20 [INFO]: Epoch 047 - training loss (MAE): 0.3636, validation MSE: 15.5358
2025-05-03 11:16:35 [INFO]: Epoch 048 - training loss (MAE): 0.3620, validation MSE: 14.8441
2025-05-03 11:16:50 [INFO]: Epoch 049 - training loss (MAE): 0.3626, validation MSE: 15.0113
2025-05-03 11:17:05 [INFO]: Epoch 050 - training loss (MAE): 0.3628, validation MSE: 15.0262
2025-05-03 11:17:21 [INFO]: Epoch 051 - training loss (MAE): 0.3608, validation MSE: 14.9793
2025-05-03 11:17:36 [INFO]: Epoch 052 - training loss (MAE): 0.3603, validation MSE: 15.3597
2025-05-03 11:17:51 [INFO]: Epoch 053 - training loss (MAE): 0.3612, validation MSE: 15.2250
2025-05-03 11:18:06 [INFO]: Epoch 054 - training loss (MAE): 0.3606, validation MSE: 14.9104
2025-05-03 11:18:21 [INFO]: Epoch 055 - training loss (MAE): 0.3596, validation MSE: 15.2007
2025-05-03 11:18:36 [INFO]: Epoch 056 - training loss (MAE): 0.3604, validation MSE: 15.6048
2025-05-03 11:18:51 [INFO]: Epoch 057 - training loss (MAE): 0.3596, validation MSE: 15.1661
2025-05-03 11:19:06 [INFO]: Epoch 058 - training loss (MAE): 0.3597, validation MSE: 15.1778
2025-05-03 11:19:21 [INFO]: Epoch 059 - training loss (MAE): 0.3583, validation MSE: 15.1004
2025-05-03 11:19:37 [INFO]: Epoch 060 - training loss (MAE): 0.3584, validation MSE: 15.3804
2025-05-03 11:19:52 [INFO]: Epoch 061 - training loss (MAE): 0.3587, validation MSE: 15.1740
2025-05-03 11:20:07 [INFO]: Epoch 062 - training loss (MAE): 0.3588, validation MSE: 15.5008
2025-05-03 11:20:22 [INFO]: Epoch 063 - training loss (MAE): 0.3584, validation MSE: 15.1740
2025-05-03 11:20:38 [INFO]: Epoch 064 - training loss (MAE): 0.3578, validation MSE: 15.4824
2025-05-03 11:20:53 [INFO]: Epoch 065 - training loss (MAE): 0.3573, validation MSE: 15.1439
2025-05-03 11:21:08 [INFO]: Epoch 066 - training loss (MAE): 0.3569, validation MSE: 15.2751
2025-05-03 11:21:23 [INFO]: Epoch 067 - training loss (MAE): 0.3562, validation MSE: 15.4630
2025-05-03 11:21:38 [INFO]: Epoch 068 - training loss (MAE): 0.3562, validation MSE: 15.2420
2025-05-03 11:21:52 [INFO]: Epoch 069 - training loss (MAE): 0.3559, validation MSE: 15.1974
2025-05-03 11:22:08 [INFO]: Epoch 070 - training loss (MAE): 0.3555, validation MSE: 15.4057
2025-05-03 11:22:23 [INFO]: Epoch 071 - training loss (MAE): 0.3556, validation MSE: 14.9237
2025-05-03 11:22:38 [INFO]: Epoch 072 - training loss (MAE): 0.3555, validation MSE: 15.3979
2025-05-03 11:22:53 [INFO]: Epoch 073 - training loss (MAE): 0.3558, validation MSE: 15.0643
2025-05-03 11:23:08 [INFO]: Epoch 074 - training loss (MAE): 0.3561, validation MSE: 15.0096
2025-05-03 11:23:23 [INFO]: Epoch 075 - training loss (MAE): 0.3556, validation MSE: 15.1544
2025-05-03 11:23:38 [INFO]: Epoch 076 - training loss (MAE): 0.3543, validation MSE: 15.3169
2025-05-03 11:23:53 [INFO]: Epoch 077 - training loss (MAE): 0.3546, validation MSE: 15.4929
2025-05-03 11:24:08 [INFO]: Epoch 078 - training loss (MAE): 0.3545, validation MSE: 15.5069
2025-05-03 11:24:23 [INFO]: Epoch 079 - training loss (MAE): 0.3545, validation MSE: 15.2753
2025-05-03 11:24:39 [INFO]: Epoch 080 - training loss (MAE): 0.3550, validation MSE: 15.2635
2025-05-03 11:24:54 [INFO]: Epoch 081 - training loss (MAE): 0.3545, validation MSE: 15.8740
2025-05-03 11:25:09 [INFO]: Epoch 082 - training loss (MAE): 0.3540, validation MSE: 15.4246
2025-05-03 11:25:24 [INFO]: Epoch 083 - training loss (MAE): 0.3538, validation MSE: 15.3074
2025-05-03 11:25:39 [INFO]: Epoch 084 - training loss (MAE): 0.3533, validation MSE: 15.3300
2025-05-03 11:25:54 [INFO]: Epoch 085 - training loss (MAE): 0.3530, validation MSE: 15.3023
2025-05-03 11:26:09 [INFO]: Epoch 086 - training loss (MAE): 0.3532, validation MSE: 15.5061
2025-05-03 11:26:24 [INFO]: Epoch 087 - training loss (MAE): 0.3529, validation MSE: 15.6800
2025-05-03 11:26:40 [INFO]: Epoch 088 - training loss (MAE): 0.3531, validation MSE: 16.0058
2025-05-03 11:26:55 [INFO]: Epoch 089 - training loss (MAE): 0.3532, validation MSE: 15.3868
2025-05-03 11:27:10 [INFO]: Epoch 090 - training loss (MAE): 0.3529, validation MSE: 15.1761
2025-05-03 11:27:25 [INFO]: Epoch 091 - training loss (MAE): 0.3524, validation MSE: 15.6123
2025-05-03 11:27:40 [INFO]: Epoch 092 - training loss (MAE): 0.3522, validation MSE: 15.8798
2025-05-03 11:27:55 [INFO]: Epoch 093 - training loss (MAE): 0.3517, validation MSE: 15.3996
2025-05-03 11:28:10 [INFO]: Epoch 094 - training loss (MAE): 0.3518, validation MSE: 15.1968
2025-05-03 11:28:25 [INFO]: Epoch 095 - training loss (MAE): 0.3524, validation MSE: 15.8062
2025-05-03 11:28:40 [INFO]: Epoch 096 - training loss (MAE): 0.3521, validation MSE: 15.6907
2025-05-03 11:28:55 [INFO]: Epoch 097 - training loss (MAE): 0.3519, validation MSE: 15.1592
2025-05-03 11:29:10 [INFO]: Epoch 098 - training loss (MAE): 0.3513, validation MSE: 15.5789
2025-05-03 11:29:25 [INFO]: Epoch 099 - training loss (MAE): 0.3508, validation MSE: 15.5813
2025-05-03 11:29:41 [INFO]: Epoch 100 - training loss (MAE): 0.3503, validation MSE: 15.5224
2025-05-03 11:29:56 [INFO]: Epoch 101 - training loss (MAE): 0.3506, validation MSE: 15.3654
2025-05-03 11:30:11 [INFO]: Epoch 102 - training loss (MAE): 0.3508, validation MSE: 15.3782
2025-05-03 11:30:26 [INFO]: Epoch 103 - training loss (MAE): 0.3507, validation MSE: 15.5324
2025-05-03 11:30:41 [INFO]: Epoch 104 - training loss (MAE): 0.3499, validation MSE: 15.5462
2025-05-03 11:30:57 [INFO]: Epoch 105 - training loss (MAE): 0.3504, validation MSE: 15.3248
2025-05-03 11:31:12 [INFO]: Epoch 106 - training loss (MAE): 0.3505, validation MSE: 15.3644
2025-05-03 11:31:27 [INFO]: Epoch 107 - training loss (MAE): 0.3498, validation MSE: 15.8914
2025-05-03 11:31:42 [INFO]: Epoch 108 - training loss (MAE): 0.3497, validation MSE: 15.4702
2025-05-03 11:31:56 [INFO]: Epoch 109 - training loss (MAE): 0.3497, validation MSE: 15.0915
2025-05-03 11:32:12 [INFO]: Epoch 110 - training loss (MAE): 0.3499, validation MSE: 15.5294
2025-05-03 11:32:27 [INFO]: Epoch 111 - training loss (MAE): 0.3492, validation MSE: 15.0503
2025-05-03 11:32:42 [INFO]: Epoch 112 - training loss (MAE): 0.3492, validation MSE: 15.7105
2025-05-03 11:32:57 [INFO]: Epoch 113 - training loss (MAE): 0.3491, validation MSE: 15.3366
2025-05-03 11:33:12 [INFO]: Epoch 114 - training loss (MAE): 0.3489, validation MSE: 15.3088
2025-05-03 11:33:27 [INFO]: Epoch 115 - training loss (MAE): 0.3484, validation MSE: 15.5460
2025-05-03 11:33:42 [INFO]: Epoch 116 - training loss (MAE): 0.3489, validation MSE: 15.5622
2025-05-03 11:33:57 [INFO]: Epoch 117 - training loss (MAE): 0.3493, validation MSE: 15.6517
2025-05-03 11:34:12 [INFO]: Epoch 118 - training loss (MAE): 0.3483, validation MSE: 15.5118
2025-05-03 11:34:27 [INFO]: Epoch 119 - training loss (MAE): 0.3484, validation MSE: 15.8270
2025-05-03 11:34:42 [INFO]: Epoch 120 - training loss (MAE): 0.3489, validation MSE: 15.7872
2025-05-03 11:34:57 [INFO]: Epoch 121 - training loss (MAE): 0.3486, validation MSE: 15.5067
2025-05-03 11:35:12 [INFO]: Epoch 122 - training loss (MAE): 0.3481, validation MSE: 15.6118
2025-05-03 11:35:27 [INFO]: Epoch 123 - training loss (MAE): 0.3496, validation MSE: 15.3069
2025-05-03 11:35:42 [INFO]: Epoch 124 - training loss (MAE): 0.3473, validation MSE: 15.5244
2025-05-03 11:35:57 [INFO]: Epoch 125 - training loss (MAE): 0.3474, validation MSE: 15.4325
2025-05-03 11:36:12 [INFO]: Epoch 126 - training loss (MAE): 0.3477, validation MSE: 15.3606
2025-05-03 11:36:27 [INFO]: Epoch 127 - training loss (MAE): 0.3479, validation MSE: 15.6340
2025-05-03 11:36:43 [INFO]: Epoch 128 - training loss (MAE): 0.3480, validation MSE: 15.5686
2025-05-03 11:36:57 [INFO]: Epoch 129 - training loss (MAE): 0.3481, validation MSE: 15.4254
2025-05-03 11:37:13 [INFO]: Epoch 130 - training loss (MAE): 0.3474, validation MSE: 15.6406
2025-05-03 11:37:28 [INFO]: Epoch 131 - training loss (MAE): 0.3473, validation MSE: 15.4577
2025-05-03 11:37:43 [INFO]: Epoch 132 - training loss (MAE): 0.3469, validation MSE: 15.1961
2025-05-03 11:37:58 [INFO]: Epoch 133 - training loss (MAE): 0.3473, validation MSE: 15.7638
2025-05-03 11:38:13 [INFO]: Epoch 134 - training loss (MAE): 0.3474, validation MSE: 15.5844
2025-05-03 11:38:28 [INFO]: Epoch 135 - training loss (MAE): 0.3470, validation MSE: 16.0033
2025-05-03 11:38:43 [INFO]: Epoch 136 - training loss (MAE): 0.3474, validation MSE: 15.6497
2025-05-03 11:38:58 [INFO]: Epoch 137 - training loss (MAE): 0.3475, validation MSE: 15.3694
2025-05-03 11:39:13 [INFO]: Epoch 138 - training loss (MAE): 0.3474, validation MSE: 15.7826
2025-05-03 11:39:28 [INFO]: Epoch 139 - training loss (MAE): 0.3465, validation MSE: 15.6681
2025-05-03 11:39:43 [INFO]: Epoch 140 - training loss (MAE): 0.3467, validation MSE: 15.5385
2025-05-03 11:39:58 [INFO]: Epoch 141 - training loss (MAE): 0.3465, validation MSE: 15.2985
2025-05-03 11:40:13 [INFO]: Epoch 142 - training loss (MAE): 0.3466, validation MSE: 15.7587
2025-05-03 11:40:28 [INFO]: Epoch 143 - training loss (MAE): 0.3466, validation MSE: 15.5817
2025-05-03 11:40:43 [INFO]: Epoch 144 - training loss (MAE): 0.3466, validation MSE: 15.5658
2025-05-03 11:40:59 [INFO]: Epoch 145 - training loss (MAE): 0.3464, validation MSE: 15.6519
2025-05-03 11:41:14 [INFO]: Epoch 146 - training loss (MAE): 0.3467, validation MSE: 15.7985
2025-05-03 11:41:29 [INFO]: Epoch 147 - training loss (MAE): 0.3463, validation MSE: 15.6233
2025-05-03 11:41:44 [INFO]: Epoch 148 - training loss (MAE): 0.3463, validation MSE: 15.3327
2025-05-03 11:41:59 [INFO]: Epoch 149 - training loss (MAE): 0.3466, validation MSE: 15.5830
2025-05-03 11:42:14 [INFO]: Epoch 150 - training loss (MAE): 0.3456, validation MSE: 15.7805
2025-05-03 11:42:29 [INFO]: Epoch 151 - training loss (MAE): 0.3458, validation MSE: 15.4840
2025-05-03 11:42:44 [INFO]: Epoch 152 - training loss (MAE): 0.3459, validation MSE: 15.5629
2025-05-03 11:42:59 [INFO]: Epoch 153 - training loss (MAE): 0.3459, validation MSE: 15.6199
2025-05-03 11:43:14 [INFO]: Epoch 154 - training loss (MAE): 0.3467, validation MSE: 15.6431
2025-05-03 11:43:29 [INFO]: Epoch 155 - training loss (MAE): 0.3460, validation MSE: 15.6165
2025-05-03 11:43:44 [INFO]: Epoch 156 - training loss (MAE): 0.3456, validation MSE: 15.7240
2025-05-03 11:43:59 [INFO]: Epoch 157 - training loss (MAE): 0.3456, validation MSE: 15.8266
2025-05-03 11:44:14 [INFO]: Epoch 158 - training loss (MAE): 0.3463, validation MSE: 15.2697
2025-05-03 11:44:29 [INFO]: Epoch 159 - training loss (MAE): 0.3455, validation MSE: 15.8334
2025-05-03 11:44:44 [INFO]: Epoch 160 - training loss (MAE): 0.3453, validation MSE: 15.6715
2025-05-03 11:44:59 [INFO]: Epoch 161 - training loss (MAE): 0.3452, validation MSE: 15.7954
2025-05-03 11:45:14 [INFO]: Epoch 162 - training loss (MAE): 0.3459, validation MSE: 15.4696
2025-05-03 11:45:29 [INFO]: Epoch 163 - training loss (MAE): 0.3454, validation MSE: 15.5771
2025-05-03 11:45:44 [INFO]: Epoch 164 - training loss (MAE): 0.3449, validation MSE: 15.3877
2025-05-03 11:45:59 [INFO]: Epoch 165 - training loss (MAE): 0.3448, validation MSE: 15.7895
2025-05-03 11:46:14 [INFO]: Epoch 166 - training loss (MAE): 0.3450, validation MSE: 15.6648
2025-05-03 11:46:29 [INFO]: Epoch 167 - training loss (MAE): 0.3447, validation MSE: 15.7545
2025-05-03 11:46:44 [INFO]: Epoch 168 - training loss (MAE): 0.3454, validation MSE: 15.5170
2025-05-03 11:46:59 [INFO]: Epoch 169 - training loss (MAE): 0.3458, validation MSE: 15.9678
2025-05-03 11:47:14 [INFO]: Epoch 170 - training loss (MAE): 0.3450, validation MSE: 15.5456
2025-05-03 11:47:29 [INFO]: Epoch 171 - training loss (MAE): 0.3451, validation MSE: 15.7652
2025-05-03 11:47:44 [INFO]: Epoch 172 - training loss (MAE): 0.3446, validation MSE: 16.0748
2025-05-03 11:47:59 [INFO]: Epoch 173 - training loss (MAE): 0.3450, validation MSE: 15.5915
2025-05-03 11:48:14 [INFO]: Epoch 174 - training loss (MAE): 0.3452, validation MSE: 15.4471
2025-05-03 11:48:29 [INFO]: Epoch 175 - training loss (MAE): 0.3447, validation MSE: 15.3775
2025-05-03 11:48:45 [INFO]: Epoch 176 - training loss (MAE): 0.3445, validation MSE: 15.6328
2025-05-03 11:49:00 [INFO]: Epoch 177 - training loss (MAE): 0.3445, validation MSE: 15.8650
2025-05-03 11:49:15 [INFO]: Epoch 178 - training loss (MAE): 0.3442, validation MSE: 15.7801
2025-05-03 11:49:30 [INFO]: Epoch 179 - training loss (MAE): 0.3446, validation MSE: 15.4018
2025-05-03 11:49:45 [INFO]: Epoch 180 - training loss (MAE): 0.3448, validation MSE: 15.8017
2025-05-03 11:50:00 [INFO]: Epoch 181 - training loss (MAE): 0.3443, validation MSE: 15.6312
2025-05-03 11:50:15 [INFO]: Epoch 182 - training loss (MAE): 0.3454, validation MSE: 16.1389
2025-05-03 11:50:30 [INFO]: Epoch 183 - training loss (MAE): 0.3443, validation MSE: 15.6330
2025-05-03 11:50:45 [INFO]: Epoch 184 - training loss (MAE): 0.3440, validation MSE: 15.5035
2025-05-03 11:51:00 [INFO]: Epoch 185 - training loss (MAE): 0.3449, validation MSE: 15.6137
2025-05-03 11:51:15 [INFO]: Epoch 186 - training loss (MAE): 0.3442, validation MSE: 15.8131
2025-05-03 11:51:31 [INFO]: Epoch 187 - training loss (MAE): 0.3443, validation MSE: 15.6971
2025-05-03 11:51:46 [INFO]: Epoch 188 - training loss (MAE): 0.3445, validation MSE: 15.7602
2025-05-03 11:52:01 [INFO]: Epoch 189 - training loss (MAE): 0.3439, validation MSE: 15.5992
2025-05-03 11:52:16 [INFO]: Epoch 190 - training loss (MAE): 0.3438, validation MSE: 15.6419
2025-05-03 11:52:31 [INFO]: Epoch 191 - training loss (MAE): 0.3438, validation MSE: 15.3492
2025-05-03 11:52:46 [INFO]: Epoch 192 - training loss (MAE): 0.3446, validation MSE: 15.4474
2025-05-03 11:53:01 [INFO]: Epoch 193 - training loss (MAE): 0.3439, validation MSE: 15.9362
2025-05-03 11:53:16 [INFO]: Epoch 194 - training loss (MAE): 0.3443, validation MSE: 15.7608
2025-05-03 11:53:31 [INFO]: Epoch 195 - training loss (MAE): 0.3437, validation MSE: 15.4582
2025-05-03 11:53:46 [INFO]: Epoch 196 - training loss (MAE): 0.3438, validation MSE: 15.1527
2025-05-03 11:54:01 [INFO]: Epoch 197 - training loss (MAE): 0.3437, validation MSE: 15.5571
2025-05-03 11:54:16 [INFO]: Epoch 198 - training loss (MAE): 0.3441, validation MSE: 15.5787
2025-05-03 11:54:31 [INFO]: Epoch 199 - training loss (MAE): 0.3441, validation MSE: 15.9057
2025-05-03 11:54:46 [INFO]: Epoch 200 - training loss (MAE): 0.3433, validation MSE: 15.6654
2025-05-03 11:55:01 [INFO]: Epoch 201 - training loss (MAE): 0.3437, validation MSE: 15.5506
2025-05-03 11:55:16 [INFO]: Epoch 202 - training loss (MAE): 0.3444, validation MSE: 15.5491
2025-05-03 11:55:31 [INFO]: Epoch 203 - training loss (MAE): 0.3432, validation MSE: 15.5548
2025-05-03 11:55:47 [INFO]: Epoch 204 - training loss (MAE): 0.3443, validation MSE: 15.3841
2025-05-03 11:56:02 [INFO]: Epoch 205 - training loss (MAE): 0.3434, validation MSE: 15.2560
2025-05-03 11:56:17 [INFO]: Epoch 206 - training loss (MAE): 0.3432, validation MSE: 15.6247
2025-05-03 11:56:31 [INFO]: Epoch 207 - training loss (MAE): 0.3431, validation MSE: 15.5238
2025-05-03 11:56:46 [INFO]: Epoch 208 - training loss (MAE): 0.3449, validation MSE: 15.8301
2025-05-03 11:57:01 [INFO]: Epoch 209 - training loss (MAE): 0.3431, validation MSE: 15.8727
2025-05-03 11:57:16 [INFO]: Epoch 210 - training loss (MAE): 0.3431, validation MSE: 15.8483
2025-05-03 11:57:31 [INFO]: Epoch 211 - training loss (MAE): 0.3429, validation MSE: 15.6499
2025-05-03 11:57:47 [INFO]: Epoch 212 - training loss (MAE): 0.3433, validation MSE: 15.3478
2025-05-03 11:58:01 [INFO]: Epoch 213 - training loss (MAE): 0.3429, validation MSE: 15.5264
2025-05-03 11:58:17 [INFO]: Epoch 214 - training loss (MAE): 0.3437, validation MSE: 15.6044
2025-05-03 11:58:32 [INFO]: Epoch 215 - training loss (MAE): 0.3432, validation MSE: 15.5474
2025-05-03 11:58:47 [INFO]: Epoch 216 - training loss (MAE): 0.3426, validation MSE: 15.5906
2025-05-03 11:59:02 [INFO]: Epoch 217 - training loss (MAE): 0.3430, validation MSE: 15.6976
2025-05-03 11:59:17 [INFO]: Epoch 218 - training loss (MAE): 0.3425, validation MSE: 15.6793
2025-05-03 11:59:32 [INFO]: Epoch 219 - training loss (MAE): 0.3436, validation MSE: 15.4625
2025-05-03 11:59:47 [INFO]: Epoch 220 - training loss (MAE): 0.3432, validation MSE: 15.5264
2025-05-03 12:00:02 [INFO]: Epoch 221 - training loss (MAE): 0.3428, validation MSE: 15.3146
2025-05-03 12:00:17 [INFO]: Epoch 222 - training loss (MAE): 0.3431, validation MSE: 15.6952
2025-05-03 12:00:32 [INFO]: Epoch 223 - training loss (MAE): 0.3431, validation MSE: 15.2337
2025-05-03 12:00:47 [INFO]: Epoch 224 - training loss (MAE): 0.3422, validation MSE: 15.5023
2025-05-03 12:01:02 [INFO]: Epoch 225 - training loss (MAE): 0.3426, validation MSE: 15.6186
2025-05-03 12:01:17 [INFO]: Epoch 226 - training loss (MAE): 0.3429, validation MSE: 15.6529
2025-05-03 12:01:32 [INFO]: Epoch 227 - training loss (MAE): 0.3426, validation MSE: 15.7464
2025-05-03 12:01:47 [INFO]: Epoch 228 - training loss (MAE): 0.3424, validation MSE: 15.5268
2025-05-03 12:02:02 [INFO]: Epoch 229 - training loss (MAE): 0.3423, validation MSE: 15.4750
2025-05-03 12:02:17 [INFO]: Epoch 230 - training loss (MAE): 0.3423, validation MSE: 15.5807
2025-05-03 12:02:32 [INFO]: Epoch 231 - training loss (MAE): 0.3428, validation MSE: 15.2008
2025-05-03 12:02:47 [INFO]: Epoch 232 - training loss (MAE): 0.3425, validation MSE: 15.4556
2025-05-03 12:03:02 [INFO]: Epoch 233 - training loss (MAE): 0.3422, validation MSE: 15.3562
2025-05-03 12:03:17 [INFO]: Epoch 234 - training loss (MAE): 0.3436, validation MSE: 15.2434
2025-05-03 12:03:32 [INFO]: Epoch 235 - training loss (MAE): 0.3426, validation MSE: 15.3404
2025-05-03 12:03:47 [INFO]: Epoch 236 - training loss (MAE): 0.3421, validation MSE: 15.5213
2025-05-03 12:04:02 [INFO]: Epoch 237 - training loss (MAE): 0.3418, validation MSE: 15.3158
2025-05-03 12:04:17 [INFO]: Epoch 238 - training loss (MAE): 0.3421, validation MSE: 15.5163
2025-05-03 12:04:33 [INFO]: Epoch 239 - training loss (MAE): 0.3426, validation MSE: 15.4244
2025-05-03 12:04:48 [INFO]: Epoch 240 - training loss (MAE): 0.3419, validation MSE: 15.3331
2025-05-03 12:05:02 [INFO]: Epoch 241 - training loss (MAE): 0.3428, validation MSE: 15.6626
2025-05-03 12:05:18 [INFO]: Epoch 242 - training loss (MAE): 0.3418, validation MSE: 15.4001
2025-05-03 12:05:33 [INFO]: Epoch 243 - training loss (MAE): 0.3425, validation MSE: 15.6287
2025-05-03 12:05:48 [INFO]: Epoch 244 - training loss (MAE): 0.3432, validation MSE: 15.5477
2025-05-03 12:06:03 [INFO]: Epoch 245 - training loss (MAE): 0.3415, validation MSE: 15.3181
2025-05-03 12:06:18 [INFO]: Epoch 246 - training loss (MAE): 0.3421, validation MSE: 15.4843
2025-05-03 12:06:33 [INFO]: Epoch 247 - training loss (MAE): 0.3419, validation MSE: 15.5300
2025-05-03 12:06:48 [INFO]: Epoch 248 - training loss (MAE): 0.3417, validation MSE: 15.4266
2025-05-03 12:07:03 [INFO]: Epoch 249 - training loss (MAE): 0.3424, validation MSE: 15.3991
2025-05-03 12:07:18 [INFO]: Epoch 250 - training loss (MAE): 0.3419, validation MSE: 15.2161
2025-05-03 12:07:33 [INFO]: Epoch 251 - training loss (MAE): 0.3425, validation MSE: 15.1965
2025-05-03 12:07:49 [INFO]: Epoch 252 - training loss (MAE): 0.3425, validation MSE: 15.6966
2025-05-03 12:08:03 [INFO]: Epoch 253 - training loss (MAE): 0.3419, validation MSE: 15.1754
2025-05-03 12:08:18 [INFO]: Epoch 254 - training loss (MAE): 0.3419, validation MSE: 15.4765
2025-05-03 12:08:33 [INFO]: Epoch 255 - training loss (MAE): 0.3423, validation MSE: 15.5388
2025-05-03 12:08:48 [INFO]: Epoch 256 - training loss (MAE): 0.3418, validation MSE: 15.5003
2025-05-03 12:09:03 [INFO]: Epoch 257 - training loss (MAE): 0.3427, validation MSE: 15.3649
2025-05-03 12:09:18 [INFO]: Epoch 258 - training loss (MAE): 0.3416, validation MSE: 15.6219
2025-05-03 12:09:33 [INFO]: Epoch 259 - training loss (MAE): 0.3412, validation MSE: 15.4036
2025-05-03 12:09:49 [INFO]: Epoch 260 - training loss (MAE): 0.3418, validation MSE: 15.7402
2025-05-03 12:10:04 [INFO]: Epoch 261 - training loss (MAE): 0.3419, validation MSE: 15.4316
2025-05-03 12:10:19 [INFO]: Epoch 262 - training loss (MAE): 0.3425, validation MSE: 15.3764
2025-05-03 12:10:34 [INFO]: Epoch 263 - training loss (MAE): 0.3418, validation MSE: 15.6944
2025-05-03 12:10:49 [INFO]: Epoch 264 - training loss (MAE): 0.3423, validation MSE: 14.7654
2025-05-03 12:11:04 [INFO]: Epoch 265 - training loss (MAE): 0.3417, validation MSE: 15.3703
2025-05-03 12:11:19 [INFO]: Epoch 266 - training loss (MAE): 0.3416, validation MSE: 15.5837
2025-05-03 12:11:34 [INFO]: Epoch 267 - training loss (MAE): 0.3414, validation MSE: 15.2922
2025-05-03 12:11:49 [INFO]: Epoch 268 - training loss (MAE): 0.3409, validation MSE: 16.0125
2025-05-03 12:12:04 [INFO]: Epoch 269 - training loss (MAE): 0.3417, validation MSE: 15.4898
2025-05-03 12:12:19 [INFO]: Epoch 270 - training loss (MAE): 0.3417, validation MSE: 15.7487
2025-05-03 12:12:34 [INFO]: Epoch 271 - training loss (MAE): 0.3419, validation MSE: 15.4466
2025-05-03 12:12:49 [INFO]: Epoch 272 - training loss (MAE): 0.3415, validation MSE: 15.5951
2025-05-03 12:13:04 [INFO]: Epoch 273 - training loss (MAE): 0.3417, validation MSE: 15.2453
2025-05-03 12:13:19 [INFO]: Epoch 274 - training loss (MAE): 0.3410, validation MSE: 15.3230
2025-05-03 12:13:34 [INFO]: Epoch 275 - training loss (MAE): 0.3411, validation MSE: 15.4295
2025-05-03 12:13:49 [INFO]: Epoch 276 - training loss (MAE): 0.3414, validation MSE: 15.0841
2025-05-03 12:14:05 [INFO]: Epoch 277 - training loss (MAE): 0.3416, validation MSE: 15.7734
2025-05-03 12:14:20 [INFO]: Epoch 278 - training loss (MAE): 0.3414, validation MSE: 15.6968
2025-05-03 12:14:35 [INFO]: Epoch 279 - training loss (MAE): 0.3412, validation MSE: 15.0940
2025-05-03 12:14:50 [INFO]: Epoch 280 - training loss (MAE): 0.3421, validation MSE: 15.1569
2025-05-03 12:15:04 [INFO]: Epoch 281 - training loss (MAE): 0.3411, validation MSE: 15.2797
2025-05-03 12:15:20 [INFO]: Epoch 282 - training loss (MAE): 0.3416, validation MSE: 15.2329
2025-05-03 12:15:35 [INFO]: Epoch 283 - training loss (MAE): 0.3426, validation MSE: 15.4365
2025-05-03 12:15:50 [INFO]: Epoch 284 - training loss (MAE): 0.3413, validation MSE: 15.1570
2025-05-03 12:16:05 [INFO]: Epoch 285 - training loss (MAE): 0.3406, validation MSE: 15.1908
2025-05-03 12:16:21 [INFO]: Epoch 286 - training loss (MAE): 0.3406, validation MSE: 14.9165
2025-05-03 12:16:36 [INFO]: Epoch 287 - training loss (MAE): 0.3408, validation MSE: 15.3013
2025-05-03 12:16:51 [INFO]: Epoch 288 - training loss (MAE): 0.3407, validation MSE: 15.2189
2025-05-03 12:17:06 [INFO]: Epoch 289 - training loss (MAE): 0.3413, validation MSE: 15.3301
2025-05-03 12:17:21 [INFO]: Epoch 290 - training loss (MAE): 0.3411, validation MSE: 15.2927
2025-05-03 12:17:36 [INFO]: Epoch 291 - training loss (MAE): 0.3400, validation MSE: 15.1786
2025-05-03 12:17:51 [INFO]: Epoch 292 - training loss (MAE): 0.3412, validation MSE: 15.4320
2025-05-03 12:18:06 [INFO]: Epoch 293 - training loss (MAE): 0.3419, validation MSE: 15.3427
2025-05-03 12:18:21 [INFO]: Epoch 294 - training loss (MAE): 0.3413, validation MSE: 15.1941
2025-05-03 12:18:36 [INFO]: Epoch 295 - training loss (MAE): 0.3412, validation MSE: 15.1849
2025-05-03 12:18:51 [INFO]: Epoch 296 - training loss (MAE): 0.3410, validation MSE: 15.2253
2025-05-03 12:19:06 [INFO]: Epoch 297 - training loss (MAE): 0.3409, validation MSE: 15.1744
2025-05-03 12:19:22 [INFO]: Epoch 298 - training loss (MAE): 0.3411, validation MSE: 15.1388
2025-05-03 12:19:37 [INFO]: Epoch 299 - training loss (MAE): 0.3412, validation MSE: 15.3529
2025-05-03 12:19:52 [INFO]: Epoch 300 - training loss (MAE): 0.3406, validation MSE: 15.5491
2025-05-03 12:20:07 [INFO]: Epoch 301 - training loss (MAE): 0.3406, validation MSE: 15.1499
2025-05-03 12:20:22 [INFO]: Epoch 302 - training loss (MAE): 0.3413, validation MSE: 15.1517
2025-05-03 12:20:37 [INFO]: Epoch 303 - training loss (MAE): 0.3407, validation MSE: 14.8445
2025-05-03 12:20:52 [INFO]: Epoch 304 - training loss (MAE): 0.3404, validation MSE: 15.5967
2025-05-03 12:21:07 [INFO]: Epoch 305 - training loss (MAE): 0.3404, validation MSE: 15.4501
2025-05-03 12:21:22 [INFO]: Epoch 306 - training loss (MAE): 0.3408, validation MSE: 15.4797
2025-05-03 12:21:37 [INFO]: Epoch 307 - training loss (MAE): 0.3411, validation MSE: 15.0558
2025-05-03 12:21:52 [INFO]: Epoch 308 - training loss (MAE): 0.3402, validation MSE: 15.1374
2025-05-03 12:22:07 [INFO]: Epoch 309 - training loss (MAE): 0.3411, validation MSE: 14.9669
2025-05-03 12:22:22 [INFO]: Epoch 310 - training loss (MAE): 0.3405, validation MSE: 15.4936
2025-05-03 12:22:37 [INFO]: Epoch 311 - training loss (MAE): 0.3405, validation MSE: 15.2082
2025-05-03 12:22:52 [INFO]: Epoch 312 - training loss (MAE): 0.3403, validation MSE: 15.2170
2025-05-03 12:23:07 [INFO]: Epoch 313 - training loss (MAE): 0.3411, validation MSE: 15.5142
2025-05-03 12:23:22 [INFO]: Epoch 314 - training loss (MAE): 0.3406, validation MSE: 15.3449
2025-05-03 12:23:37 [INFO]: Epoch 315 - training loss (MAE): 0.3408, validation MSE: 15.3127
2025-05-03 12:23:52 [INFO]: Epoch 316 - training loss (MAE): 0.3402, validation MSE: 15.4381
2025-05-03 12:24:07 [INFO]: Epoch 317 - training loss (MAE): 0.3406, validation MSE: 15.3469
2025-05-03 12:24:22 [INFO]: Epoch 318 - training loss (MAE): 0.3404, validation MSE: 15.3136
2025-05-03 12:24:37 [INFO]: Epoch 319 - training loss (MAE): 0.3404, validation MSE: 15.1747
2025-05-03 12:24:53 [INFO]: Epoch 320 - training loss (MAE): 0.3406, validation MSE: 15.0754
2025-05-03 12:25:08 [INFO]: Epoch 321 - training loss (MAE): 0.3404, validation MSE: 15.2268
2025-05-03 12:25:23 [INFO]: Epoch 322 - training loss (MAE): 0.3401, validation MSE: 15.3495
2025-05-03 12:25:38 [INFO]: Epoch 323 - training loss (MAE): 0.3407, validation MSE: 15.3576
2025-05-03 12:25:54 [INFO]: Epoch 324 - training loss (MAE): 0.3401, validation MSE: 15.2488
2025-05-03 12:26:08 [INFO]: Epoch 325 - training loss (MAE): 0.3404, validation MSE: 15.3673
2025-05-03 12:26:23 [INFO]: Epoch 326 - training loss (MAE): 0.3400, validation MSE: 15.4489
2025-05-03 12:26:39 [INFO]: Epoch 327 - training loss (MAE): 0.3405, validation MSE: 14.9765
2025-05-03 12:26:54 [INFO]: Epoch 328 - training loss (MAE): 0.3402, validation MSE: 15.2180
2025-05-03 12:27:08 [INFO]: Epoch 329 - training loss (MAE): 0.3404, validation MSE: 15.2068
2025-05-03 12:27:24 [INFO]: Epoch 330 - training loss (MAE): 0.3392, validation MSE: 15.1656
2025-05-03 12:27:39 [INFO]: Epoch 331 - training loss (MAE): 0.3405, validation MSE: 15.0557
2025-05-03 12:27:54 [INFO]: Epoch 332 - training loss (MAE): 0.3402, validation MSE: 15.0736
2025-05-03 12:28:09 [INFO]: Epoch 333 - training loss (MAE): 0.3401, validation MSE: 15.2705
2025-05-03 12:28:25 [INFO]: Epoch 334 - training loss (MAE): 0.3402, validation MSE: 15.0754
2025-05-03 12:28:40 [INFO]: Epoch 335 - training loss (MAE): 0.3406, validation MSE: 15.1184
2025-05-03 12:28:55 [INFO]: Epoch 336 - training loss (MAE): 0.3402, validation MSE: 14.9494
2025-05-03 12:29:10 [INFO]: Epoch 337 - training loss (MAE): 0.3398, validation MSE: 15.0393
2025-05-03 12:29:25 [INFO]: Epoch 338 - training loss (MAE): 0.3401, validation MSE: 15.2609
2025-05-03 12:29:40 [INFO]: Epoch 339 - training loss (MAE): 0.3400, validation MSE: 15.2178
2025-05-03 12:29:55 [INFO]: Epoch 340 - training loss (MAE): 0.3398, validation MSE: 15.1460
2025-05-03 12:30:10 [INFO]: Epoch 341 - training loss (MAE): 0.3399, validation MSE: 14.9493
2025-05-03 12:30:25 [INFO]: Epoch 342 - training loss (MAE): 0.3398, validation MSE: 15.1972
2025-05-03 12:30:40 [INFO]: Epoch 343 - training loss (MAE): 0.3399, validation MSE: 15.3669
2025-05-03 12:30:55 [INFO]: Epoch 344 - training loss (MAE): 0.3404, validation MSE: 15.1329
2025-05-03 12:31:10 [INFO]: Epoch 345 - training loss (MAE): 0.3405, validation MSE: 15.0537
2025-05-03 12:31:25 [INFO]: Epoch 346 - training loss (MAE): 0.3402, validation MSE: 15.0397
2025-05-03 12:31:40 [INFO]: Epoch 347 - training loss (MAE): 0.3400, validation MSE: 15.2818
2025-05-03 12:31:55 [INFO]: Epoch 348 - training loss (MAE): 0.3398, validation MSE: 15.4000
2025-05-03 12:32:10 [INFO]: Epoch 349 - training loss (MAE): 0.3401, validation MSE: 15.1726
2025-05-03 12:32:25 [INFO]: Epoch 350 - training loss (MAE): 0.3397, validation MSE: 14.9180
2025-05-03 12:32:40 [INFO]: Epoch 351 - training loss (MAE): 0.3398, validation MSE: 15.0643
2025-05-03 12:32:56 [INFO]: Epoch 352 - training loss (MAE): 0.3407, validation MSE: 15.2523
2025-05-03 12:33:10 [INFO]: Epoch 353 - training loss (MAE): 0.3397, validation MSE: 15.1607
2025-05-03 12:33:25 [INFO]: Epoch 354 - training loss (MAE): 0.3390, validation MSE: 15.1562
2025-05-03 12:33:40 [INFO]: Epoch 355 - training loss (MAE): 0.3398, validation MSE: 15.2560
2025-05-03 12:33:56 [INFO]: Epoch 356 - training loss (MAE): 0.3392, validation MSE: 14.8928
2025-05-03 12:34:10 [INFO]: Epoch 357 - training loss (MAE): 0.3395, validation MSE: 15.2646
2025-05-03 12:34:26 [INFO]: Epoch 358 - training loss (MAE): 0.3394, validation MSE: 14.9945
2025-05-03 12:34:41 [INFO]: Epoch 359 - training loss (MAE): 0.3397, validation MSE: 15.0858
2025-05-03 12:34:56 [INFO]: Epoch 360 - training loss (MAE): 0.3394, validation MSE: 15.1663
2025-05-03 12:35:11 [INFO]: Epoch 361 - training loss (MAE): 0.3395, validation MSE: 15.1158
2025-05-03 12:35:26 [INFO]: Epoch 362 - training loss (MAE): 0.3391, validation MSE: 14.8965
2025-05-03 12:35:41 [INFO]: Epoch 363 - training loss (MAE): 0.3402, validation MSE: 14.9675
2025-05-03 12:35:56 [INFO]: Epoch 364 - training loss (MAE): 0.3398, validation MSE: 15.0668
2025-05-03 12:36:11 [INFO]: Epoch 365 - training loss (MAE): 0.3404, validation MSE: 15.0243
2025-05-03 12:36:26 [INFO]: Epoch 366 - training loss (MAE): 0.3395, validation MSE: 15.1468
2025-05-03 12:36:41 [INFO]: Epoch 367 - training loss (MAE): 0.3398, validation MSE: 14.6033
2025-05-03 12:36:56 [INFO]: Epoch 368 - training loss (MAE): 0.3392, validation MSE: 15.0344
2025-05-03 12:37:11 [INFO]: Epoch 369 - training loss (MAE): 0.3391, validation MSE: 14.9763
2025-05-03 12:37:26 [INFO]: Epoch 370 - training loss (MAE): 0.3388, validation MSE: 14.9051
2025-05-03 12:37:41 [INFO]: Epoch 371 - training loss (MAE): 0.3393, validation MSE: 14.7192
2025-05-03 12:37:56 [INFO]: Epoch 372 - training loss (MAE): 0.3394, validation MSE: 15.0240
2025-05-03 12:38:12 [INFO]: Epoch 373 - training loss (MAE): 0.3391, validation MSE: 14.7565
2025-05-03 12:38:27 [INFO]: Epoch 374 - training loss (MAE): 0.3397, validation MSE: 14.8285
2025-05-03 12:38:42 [INFO]: Epoch 375 - training loss (MAE): 0.3391, validation MSE: 15.0765
2025-05-03 12:38:57 [INFO]: Epoch 376 - training loss (MAE): 0.3393, validation MSE: 14.8227
2025-05-03 12:39:11 [INFO]: Epoch 377 - training loss (MAE): 0.3395, validation MSE: 14.3871
2025-05-03 12:39:27 [INFO]: Epoch 378 - training loss (MAE): 0.3388, validation MSE: 14.8944
2025-05-03 12:39:42 [INFO]: Epoch 379 - training loss (MAE): 0.3391, validation MSE: 14.7258
2025-05-03 12:39:57 [INFO]: Epoch 380 - training loss (MAE): 0.3393, validation MSE: 14.9249
2025-05-03 12:40:11 [INFO]: Epoch 381 - training loss (MAE): 0.3391, validation MSE: 14.9240
2025-05-03 12:40:27 [INFO]: Epoch 382 - training loss (MAE): 0.3392, validation MSE: 14.9715
2025-05-03 12:40:42 [INFO]: Epoch 383 - training loss (MAE): 0.3388, validation MSE: 14.9964
2025-05-03 12:40:57 [INFO]: Epoch 384 - training loss (MAE): 0.3391, validation MSE: 14.9858
2025-05-03 12:41:12 [INFO]: Epoch 385 - training loss (MAE): 0.3389, validation MSE: 14.8995
2025-05-03 12:41:27 [INFO]: Epoch 386 - training loss (MAE): 0.3384, validation MSE: 14.7104
2025-05-03 12:41:42 [INFO]: Epoch 387 - training loss (MAE): 0.3384, validation MSE: 14.9300
2025-05-03 12:41:57 [INFO]: Epoch 388 - training loss (MAE): 0.3384, validation MSE: 14.7468
2025-05-03 12:42:12 [INFO]: Epoch 389 - training loss (MAE): 0.3392, validation MSE: 14.8456
2025-05-03 12:42:27 [INFO]: Epoch 390 - training loss (MAE): 0.3390, validation MSE: 14.7344
2025-05-03 12:42:42 [INFO]: Epoch 391 - training loss (MAE): 0.3392, validation MSE: 14.5897
2025-05-03 12:42:57 [INFO]: Epoch 392 - training loss (MAE): 0.3385, validation MSE: 14.9996
2025-05-03 12:43:13 [INFO]: Epoch 393 - training loss (MAE): 0.3384, validation MSE: 14.7004
2025-05-03 12:43:28 [INFO]: Epoch 394 - training loss (MAE): 0.3384, validation MSE: 14.7874
2025-05-03 12:43:43 [INFO]: Epoch 395 - training loss (MAE): 0.3397, validation MSE: 14.5166
2025-05-03 12:43:58 [INFO]: Epoch 396 - training loss (MAE): 0.3419, validation MSE: 14.6113
2025-05-03 12:44:13 [INFO]: Epoch 397 - training loss (MAE): 0.3398, validation MSE: 14.7973
2025-05-03 12:44:28 [INFO]: Epoch 398 - training loss (MAE): 0.3388, validation MSE: 14.7061
2025-05-03 12:44:43 [INFO]: Epoch 399 - training loss (MAE): 0.3382, validation MSE: 14.6341
2025-05-03 12:44:58 [INFO]: Epoch 400 - training loss (MAE): 0.3381, validation MSE: 14.8542
2025-05-03 12:45:13 [INFO]: Epoch 401 - training loss (MAE): 0.3381, validation MSE: 14.8516
2025-05-03 12:45:28 [INFO]: Epoch 402 - training loss (MAE): 0.3383, validation MSE: 14.4020
2025-05-03 12:45:43 [INFO]: Epoch 403 - training loss (MAE): 0.3390, validation MSE: 14.7875
2025-05-03 12:45:59 [INFO]: Epoch 404 - training loss (MAE): 0.3386, validation MSE: 14.5386
2025-05-03 12:46:14 [INFO]: Epoch 405 - training loss (MAE): 0.3380, validation MSE: 14.4438
2025-05-03 12:46:29 [INFO]: Epoch 406 - training loss (MAE): 0.3385, validation MSE: 14.4189
2025-05-03 12:46:44 [INFO]: Epoch 407 - training loss (MAE): 0.3388, validation MSE: 14.4680
2025-05-03 12:46:59 [INFO]: Epoch 408 - training loss (MAE): 0.3384, validation MSE: 14.5174
2025-05-03 12:47:14 [INFO]: Epoch 409 - training loss (MAE): 0.3382, validation MSE: 14.9170
2025-05-03 12:47:29 [INFO]: Epoch 410 - training loss (MAE): 0.3382, validation MSE: 14.5560
2025-05-03 12:47:44 [INFO]: Epoch 411 - training loss (MAE): 0.3384, validation MSE: 14.9247
2025-05-03 12:47:59 [INFO]: Epoch 412 - training loss (MAE): 0.3386, validation MSE: 14.8813
2025-05-03 12:48:14 [INFO]: Epoch 413 - training loss (MAE): 0.3375, validation MSE: 14.5071
2025-05-03 12:48:29 [INFO]: Epoch 414 - training loss (MAE): 0.3378, validation MSE: 14.5325
2025-05-03 12:48:44 [INFO]: Epoch 415 - training loss (MAE): 0.3380, validation MSE: 14.6526
2025-05-03 12:48:59 [INFO]: Epoch 416 - training loss (MAE): 0.3374, validation MSE: 14.7152
2025-05-03 12:49:14 [INFO]: Epoch 417 - training loss (MAE): 0.3383, validation MSE: 14.6586
2025-05-03 12:49:29 [INFO]: Epoch 418 - training loss (MAE): 0.3374, validation MSE: 14.4655
2025-05-03 12:49:44 [INFO]: Epoch 419 - training loss (MAE): 0.3384, validation MSE: 14.8749
2025-05-03 12:49:59 [INFO]: Epoch 420 - training loss (MAE): 0.3379, validation MSE: 14.4774
2025-05-03 12:50:14 [INFO]: Epoch 421 - training loss (MAE): 0.3392, validation MSE: 14.6385
2025-05-03 12:50:29 [INFO]: Epoch 422 - training loss (MAE): 0.3382, validation MSE: 14.3522
2025-05-03 12:50:45 [INFO]: Epoch 423 - training loss (MAE): 0.3379, validation MSE: 14.5984
2025-05-03 12:51:00 [INFO]: Epoch 424 - training loss (MAE): 0.3377, validation MSE: 14.8814
2025-05-03 12:51:15 [INFO]: Epoch 425 - training loss (MAE): 0.3379, validation MSE: 14.5113
2025-05-03 12:51:30 [INFO]: Epoch 426 - training loss (MAE): 0.3385, validation MSE: 14.8364
2025-05-03 12:51:45 [INFO]: Epoch 427 - training loss (MAE): 0.3383, validation MSE: 14.5890
2025-05-03 12:52:00 [INFO]: Epoch 428 - training loss (MAE): 0.3376, validation MSE: 14.5493
2025-05-03 12:52:15 [INFO]: Epoch 429 - training loss (MAE): 0.3380, validation MSE: 14.6322
2025-05-03 12:52:30 [INFO]: Epoch 430 - training loss (MAE): 0.3379, validation MSE: 14.4627
2025-05-03 12:52:45 [INFO]: Epoch 431 - training loss (MAE): 0.3384, validation MSE: 14.2992
2025-05-03 12:53:00 [INFO]: Epoch 432 - training loss (MAE): 0.3377, validation MSE: 14.4308
2025-05-03 12:53:15 [INFO]: Epoch 433 - training loss (MAE): 0.3378, validation MSE: 14.3399
2025-05-03 12:53:31 [INFO]: Epoch 434 - training loss (MAE): 0.3383, validation MSE: 14.8835
2025-05-03 12:53:46 [INFO]: Epoch 435 - training loss (MAE): 0.3376, validation MSE: 14.7105
2025-05-03 12:54:01 [INFO]: Epoch 436 - training loss (MAE): 0.3380, validation MSE: 14.1822
2025-05-03 12:54:15 [INFO]: Epoch 437 - training loss (MAE): 0.3377, validation MSE: 14.4521
2025-05-03 12:54:30 [INFO]: Epoch 438 - training loss (MAE): 0.3378, validation MSE: 14.3066
2025-05-03 12:54:45 [INFO]: Epoch 439 - training loss (MAE): 0.3375, validation MSE: 14.3925
2025-05-03 12:55:01 [INFO]: Epoch 440 - training loss (MAE): 0.3375, validation MSE: 14.6199
2025-05-03 12:55:16 [INFO]: Epoch 441 - training loss (MAE): 0.3376, validation MSE: 14.6059
2025-05-03 12:55:31 [INFO]: Epoch 442 - training loss (MAE): 0.3374, validation MSE: 14.2951
2025-05-03 12:55:46 [INFO]: Epoch 443 - training loss (MAE): 0.3375, validation MSE: 14.4346
2025-05-03 12:56:01 [INFO]: Epoch 444 - training loss (MAE): 0.3376, validation MSE: 14.5431
2025-05-03 12:56:16 [INFO]: Epoch 445 - training loss (MAE): 0.3375, validation MSE: 14.2431
2025-05-03 12:56:31 [INFO]: Epoch 446 - training loss (MAE): 0.3381, validation MSE: 14.0739
2025-05-03 12:56:46 [INFO]: Epoch 447 - training loss (MAE): 0.3374, validation MSE: 14.2407
2025-05-03 12:57:01 [INFO]: Epoch 448 - training loss (MAE): 0.3375, validation MSE: 14.5498
2025-05-03 12:57:16 [INFO]: Epoch 449 - training loss (MAE): 0.3368, validation MSE: 14.5971
2025-05-03 12:57:31 [INFO]: Epoch 450 - training loss (MAE): 0.3370, validation MSE: 14.0398
2025-05-03 12:57:46 [INFO]: Epoch 451 - training loss (MAE): 0.3373, validation MSE: 14.6336
2025-05-03 12:58:01 [INFO]: Epoch 452 - training loss (MAE): 0.3372, validation MSE: 14.4642
2025-05-03 12:58:16 [INFO]: Epoch 453 - training loss (MAE): 0.3375, validation MSE: 14.3935
2025-05-03 12:58:31 [INFO]: Epoch 454 - training loss (MAE): 0.3372, validation MSE: 14.1504
2025-05-03 12:58:46 [INFO]: Epoch 455 - training loss (MAE): 0.3367, validation MSE: 14.2151
2025-05-03 12:59:01 [INFO]: Epoch 456 - training loss (MAE): 0.3375, validation MSE: 14.5762
2025-05-03 12:59:16 [INFO]: Epoch 457 - training loss (MAE): 0.3371, validation MSE: 14.3178
2025-05-03 12:59:31 [INFO]: Epoch 458 - training loss (MAE): 0.3373, validation MSE: 13.9429
2025-05-03 12:59:46 [INFO]: Epoch 459 - training loss (MAE): 0.3375, validation MSE: 14.2644
2025-05-03 13:00:01 [INFO]: Epoch 460 - training loss (MAE): 0.3372, validation MSE: 13.9704
2025-05-03 13:00:16 [INFO]: Epoch 461 - training loss (MAE): 0.3368, validation MSE: 14.5716
2025-05-03 13:00:31 [INFO]: Epoch 462 - training loss (MAE): 0.3370, validation MSE: 14.4006
2025-05-03 13:00:46 [INFO]: Epoch 463 - training loss (MAE): 0.3369, validation MSE: 14.1658
2025-05-03 13:01:01 [INFO]: Epoch 464 - training loss (MAE): 0.3365, validation MSE: 14.0590
2025-05-03 13:01:16 [INFO]: Epoch 465 - training loss (MAE): 0.3368, validation MSE: 14.0420
2025-05-03 13:01:31 [INFO]: Epoch 466 - training loss (MAE): 0.3369, validation MSE: 14.0526
2025-05-03 13:01:46 [INFO]: Epoch 467 - training loss (MAE): 0.3375, validation MSE: 14.1062
2025-05-03 13:02:01 [INFO]: Epoch 468 - training loss (MAE): 0.3368, validation MSE: 14.0848
2025-05-03 13:02:16 [INFO]: Epoch 469 - training loss (MAE): 0.3365, validation MSE: 14.2739
2025-05-03 13:02:32 [INFO]: Epoch 470 - training loss (MAE): 0.3377, validation MSE: 13.9744
2025-05-03 13:02:47 [INFO]: Epoch 471 - training loss (MAE): 0.3373, validation MSE: 13.9675
2025-05-03 13:03:02 [INFO]: Epoch 472 - training loss (MAE): 0.3365, validation MSE: 14.1480
2025-05-03 13:03:17 [INFO]: Epoch 473 - training loss (MAE): 0.3373, validation MSE: 14.3569
2025-05-03 13:03:32 [INFO]: Epoch 474 - training loss (MAE): 0.3372, validation MSE: 14.0654
2025-05-03 13:03:47 [INFO]: Epoch 475 - training loss (MAE): 0.3372, validation MSE: 14.3119
2025-05-03 13:04:02 [INFO]: Epoch 476 - training loss (MAE): 0.3370, validation MSE: 14.1895
2025-05-03 13:04:17 [INFO]: Epoch 477 - training loss (MAE): 0.3364, validation MSE: 14.1909
2025-05-03 13:04:32 [INFO]: Epoch 478 - training loss (MAE): 0.3361, validation MSE: 14.2653
2025-05-03 13:04:47 [INFO]: Epoch 479 - training loss (MAE): 0.3365, validation MSE: 14.2004
2025-05-03 13:05:02 [INFO]: Epoch 480 - training loss (MAE): 0.3362, validation MSE: 14.1991
2025-05-03 13:05:17 [INFO]: Epoch 481 - training loss (MAE): 0.3368, validation MSE: 14.2782
2025-05-03 13:05:32 [INFO]: Epoch 482 - training loss (MAE): 0.3371, validation MSE: 13.9649
2025-05-03 13:05:47 [INFO]: Epoch 483 - training loss (MAE): 0.3362, validation MSE: 14.2941
2025-05-03 13:06:02 [INFO]: Epoch 484 - training loss (MAE): 0.3366, validation MSE: 14.5432
2025-05-03 13:06:17 [INFO]: Epoch 485 - training loss (MAE): 0.3366, validation MSE: 13.9356
2025-05-03 13:06:32 [INFO]: Epoch 486 - training loss (MAE): 0.3365, validation MSE: 14.1014
2025-05-03 13:06:47 [INFO]: Epoch 487 - training loss (MAE): 0.3366, validation MSE: 14.4541
2025-05-03 13:07:02 [INFO]: Epoch 488 - training loss (MAE): 0.3371, validation MSE: 13.9790
2025-05-03 13:07:17 [INFO]: Epoch 489 - training loss (MAE): 0.3371, validation MSE: 14.3326
2025-05-03 13:07:32 [INFO]: Epoch 490 - training loss (MAE): 0.3364, validation MSE: 14.0289
2025-05-03 13:07:47 [INFO]: Epoch 491 - training loss (MAE): 0.3363, validation MSE: 13.8990
2025-05-03 13:08:02 [INFO]: Epoch 492 - training loss (MAE): 0.3364, validation MSE: 14.1038
2025-05-03 13:08:17 [INFO]: Epoch 493 - training loss (MAE): 0.3359, validation MSE: 13.9030
2025-05-03 13:08:32 [INFO]: Epoch 494 - training loss (MAE): 0.3358, validation MSE: 13.9909
2025-05-03 13:08:47 [INFO]: Epoch 495 - training loss (MAE): 0.3372, validation MSE: 14.5613
2025-05-03 13:09:02 [INFO]: Epoch 496 - training loss (MAE): 0.3369, validation MSE: 14.4105
2025-05-03 13:09:17 [INFO]: Epoch 497 - training loss (MAE): 0.3366, validation MSE: 14.1485
2025-05-03 13:09:32 [INFO]: Epoch 498 - training loss (MAE): 0.3364, validation MSE: 14.0065
2025-05-03 13:09:47 [INFO]: Epoch 499 - training loss (MAE): 0.3362, validation MSE: 14.2052
2025-05-03 13:10:02 [INFO]: Epoch 500 - training loss (MAE): 0.3363, validation MSE: 14.2083
2025-05-03 13:10:02 [INFO]: Finished training. The best model is from epoch#1.
2025-05-03 13:10:02 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_4_kfolds_10.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 13:10:02 [INFO]: Saved the model to saits_weights/saits_all_artificial_4_kfolds_10.pypots
Fold 10 metrics: MAE: 0.322, MSE: 0.391, MRE: 0.451
Fold 10 metrics: MAE: 0.322, MSE: 0.391, MRE: 0.451

K-Fold Cross Validation Results
Average MAE: 0.317, Average MSE: 0.382, Average MRE: 0.447
2025-05-03 13:10:02 [WARNING]: ‚ÄºÔ∏è File saits_weights/saits_all_artificial_kfolds.pypots exists. Argument `overwrite` is True. Overwriting now...
2025-05-03 13:10:02 [INFO]: Saved the model to saits_weights/saits_all_artificial_kfolds.pypots
2025-05-03 13:10:03 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_kfolds.pypots
Testing Results:
MAE: 0.255, MSE: 0.268, MRE: 0.347
Processing fold 1...
2025-05-03 13:10:04 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_1.pypots
Fold 1 - MAE: 0.285, MSE: 0.296, MRE: 0.389
Processing fold 2...
2025-05-03 13:10:06 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_2.pypots
Fold 2 - MAE: 0.240, MSE: 0.262, MRE: 0.327
Processing fold 3...
2025-05-03 13:10:07 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_3.pypots
Fold 3 - MAE: 0.277, MSE: 0.286, MRE: 0.379
Processing fold 4...
2025-05-03 13:10:08 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_4.pypots
Fold 4 - MAE: 0.275, MSE: 0.292, MRE: 0.376
Processing fold 5...
2025-05-03 13:10:09 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_5.pypots
Fold 5 - MAE: 0.408, MSE: 0.515, MRE: 0.557
Processing fold 6...
2025-05-03 13:10:10 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_6.pypots
Fold 6 - MAE: 0.269, MSE: 0.284, MRE: 0.367
Processing fold 7...
2025-05-03 13:10:12 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_7.pypots
Fold 7 - MAE: 0.277, MSE: 0.289, MRE: 0.378
Processing fold 8...
2025-05-03 13:10:13 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_8.pypots
Fold 8 - MAE: 0.279, MSE: 0.296, MRE: 0.380
Processing fold 9...
2025-05-03 13:10:14 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_9.pypots
Fold 9 - MAE: 0.276, MSE: 0.290, MRE: 0.376
Processing fold 10...
2025-05-03 13:10:15 [INFO]: Model loaded successfully from saits_weights/saits_all_artificial_4_kfolds_10.pypots
Fold 10 - MAE: 0.268, MSE: 0.283, MRE: 0.365

Average Metrics over 10 folds:
Avg MAE: 0.285, Avg MSE: 0.309, Avg MRE: 0.389
